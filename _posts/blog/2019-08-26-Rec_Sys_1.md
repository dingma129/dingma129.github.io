---
title: Recommender Systems (part 1)
subtitle: Neighborhood-Based Collaborative Filtering
author: Ding Ma
layout: post
categories: [blog]
---

<span style="font-weight:bold;font-size:32px">0. Introduction</span>

The Web provides an easy way to enable users to provide feedback about their likes or dislikes. There are two methodology to provide feedback:
1. explicitly: such as ratings
2. implicitly: such as the simple acts of buying or browsing an item
The basic idea of recommender systems is to utilize these various sources of data to infer customer interests. 

The recommendation analysis is based on the interaction between users and items, because past interests are often good indicators of future choices. In recommendation analysis, 
* users are the entities to which the recommendations are provided,
* items are the products being recommended.

I will focus on the prediction version of the recommendation system. There is an incomplete m-by-n rating matrix corresponding to m users and n items, where the specified values are used for training. It can also be viewed as a matrix completion problem. 

<center><img src="https://dingma129.github.io/assets/figures/rec/rec_1_matrix_completion.png" width="1000" ></center>

In this series of blogs, I will introduce the following family of algorithms of recommendation system along with some examples.
1. Collaborative Filtering Models: use ratings from multiple users/items in a collaborative way  (inter-item correlations or inter-user correlations) to predict missing ratings
	* neighborhood-based methods: simple to implement, easy to explain, might lack full coverage of rating predictions
	* model-based methods: have a high level of coverage even for sparse rating matrices
2. Content-Based Recommender Systems: where content plays a primary role in the recommendation process. Good for making recommendations for new items, but not for new users.

---
<span style="font-weight:bold;font-size:32px">1. Introduction</span>

The neighborhood-based collaborative filtering algorithms are based on the fact that similar users display similar patterns of rating behavior and similar items receive similar ratings. 

There are two primary types of neighborhood-based algorithms:
1. user-based: the ratings provided by similar users are used to make predictions
2. item-based: the ratings received by similar items are used to make predictions

---
<span style="font-weight:bold;font-size:32px">2. User-Based and Item-Based</span>

In user-based approach, user-based neighborhoods are defined in order to identify similar users to the target user. Therefore, a similarity function needs to be defined between the ratings specified by users.

The following Pearson correlation coefficient is the most common choice of similarity function. Cosine similarity is another common choice of similarity function.

<center><img src="https://dingma129.github.io/assets/figures/rec/rec_1_pearson.png" width="1000" ></center>

In all user-based collaborative filtering algorithms, k most similar users (who have rated the target item j) to target user u are chosen, their various weighted averages give different versions of the algorithm.
1. KNN Basic: a vanilla weighted average of target item j from k most similar users of target user u
2. KNN with Means: a weighted average of k nearest mean-centered ratings, shifting back by user u's mean rating
3. KNN with Z-scores: a weighted average of k nearest Z-scores, shifting and rescaling back by user u's mean and standard deviation of ratings

The following figure lists the explicit formula for those three algorithms.

<center><img src="https://dingma129.github.io/assets/figures/rec/rec_1_user.png" width="1000" ></center>

Item-based approach is quite similar to user-based approach. Instead of using similar users, it uses similar items to make the prediction. 

---
<span style="font-weight:bold;font-size:32px">3. Comparison</span>

<span style="font-weight:bold;font-size:28px">3.1 User-Based vs Item-Based</span>

* Item-based methods often provide more relevant recommendations because of the fact that a userâ€™s own ratings are used to perform the recommendation. 
* Item-based methods are more stable with changes to the ratings. Two users may have a very small number of mutually rated items, but two items are more likely to have a larger number of users who have co-rated them. Also new users are likely to be added more frequently than new items. 

<span style="font-weight:bold;font-size:28px">3.2 Advantage and Disadvantage</span>

* Advantage: easy to implement, good interpretability
* Disadvantage: slow, space-intensive, limited coverage because of sparsity, difficult to get robust similarity computation when the number of mutually rated items between two users
is small

---
<span style="font-weight:bold;font-size:32px">4. Examples</span>

<span style="font-weight:bold;font-size:28px">4.1 By Hand</span>

The following three figures show the detailed steps of how to use user-based KNN Basic, user-based KNN with Means and item-based KNN with Z-score.

<center><img src="https://dingma129.github.io/assets/figures/rec/rec_1_knnbasic_user.png" width="800" ></center>
<center><img src="https://dingma129.github.io/assets/figures/rec/rec_1_knnmean_user.png" width="800" ></center>
<center><img src="https://dingma129.github.io/assets/figures/rec/rec_1_knnz_item.png" width="800" ></center>

<span style="font-weight:bold;font-size:28px">4.2 Using `Surprise`</span>

`Surprise` is a Python package for recommender systems. Its documentation can be found [<span style="color: blue">here</span>](https://surprise.readthedocs.io/en/stable/index.html).

It's very simple to use its built-in `KNNBasic`, `KNNWithMeans`, `KNNWithZScore` models.
```python
import pandas as pd
from surprise import KNNBasic,KNNWithMeans,KNNWithZScore,Dataset
from surprise.reader import Reader
# data in pandas
dfSurprise = pd.DataFrame(data={(1,1,5),(1,2,6),(1,3,7),(1,4,4),(1,5,3),
                                (2,1,4),(2,3,3),(2,5,5),(2,6,4),
                                (3,2,3),(3,3,4),(3,4,1),(3,5,1),
                                (4,1,7),(4,2,4),(4,3,3),(4,4,6),(4,6,4),
                                (5,1,1),(5,3,3),(5,4,2),(5,5,2),(5,6,5)})
# load as surprise.Dataset and set the rating scale to be 1-7
data = Dataset.load_from_df(dfSurprise,Reader(rating_scale=(1,7)))
# need to build a trainset before training a model
traindata = data.build_full_trainset()

# 1. KNN Basic model
algo1 = KNNBasic(k=2,sim_options = {'name': 'pearson','user_based': True})
algo1.fit(traindata)
# prediction
algo1.predict(3,1).est  # 2.97337931499421
algo1.predict(3,6).est  # 5.0

# 2. KNN with Means model
algo2 = KNNWithMeans(k=2,sim_options = {'name': 'pearson','user_based': True})
algo2.fit(traindata)
# prediction
algo2.predict(2,2).est  # 3.2
algo2.predict(2,4).est  # 5.2

# 3. KNN with Z-score model
algo3 = KNNWithZScore(k=2,sim_options = {'name': 'pearson','user_based': False})
# prediction
algo3.fit(traindata)
algo3.predict(4,5).est  # 3.5999683317752713
```
The results are exactly the same as the one in Section 4.1. 

Some quick notes here:
* `Surprise.knns` only use neighbors with postive similarities. 
* In `Surprise`, when computing Z-score, the population standard deviation is used instead of the sample standard deviation.