---
title: "Julia: Notes-07"
layout: splash
excerpt: "RNN"
categories: [Julia]
tags: [Regression, Time Series, Flux, RNN, LSTM]
---

# 0. Introduction

In this blog, I will introduce how to construct an RNN model using `Flux.jl`. I will use it to study some simple time series data.

First I will load some necessary packages.
```julia
using Flux, Statistics, Plots
gr()
```

# 1. Data

I will create some fake time series data as below. It's the composition of two sine functions with some normal distributed error term.

```julia
X = (sin.(range(0.03,30,length=1000)) + 0.3 * sin.(range(0.03,30,length=1000)*4) + randn(1000)/30)
# test data are future data of the training data
# they are different from training data because Sin has period 2*pi which is not rational
X_test = (sin.(range(30.03,60,length=1000)) + 0.3 * sin.(range(30.03,60,length=1000)*4) + randn(1000)/30)
plot(X,label="");
```
<center><img src="/assets/figures/julia_notes/07_data.png" width="600" ></center>

# 2. Problem description

The data `X` is a time series data of length `1000`. The problem I will study in this blog can be described as
* For every continuous part of the data of length 50, I want to predict the values in the next 50 timestamps.
* But if we build a model that uses `[t,t+49]` to predict `[t+50,t+99]` directly, this model will not behave well.
* Instead, I will build a model that uses `[t,t+49]` to predict `[t+1,t+50]`. Next, I will use `[t+1,t+49]` along with the predicted value at `t+50` (49 given and 1 prediction) to predict `[t+2,t+51]`. At each step, only the last value will be added into the prediction set. Iteratively, we can get the prediction at `[t+50,t+99]`.

# 3. Data preprocessing 

Since I will build a model that uses `[t,t+49]` to predict `[t+1,t+50]` for any timestamp `t`, I will preprocess the data as follows.

```julia
# seqlen: length of seq, in our case 50
# step_size: used to sample the data, I will use 3 later
# batch_size: batch size, I will use 100
function create_batch(X,seqlen,step_size,batch_size)
    # determine the number of sequences and batchs
    n_seq = div((length(X)-1-seqlen),step_size) 
    n_batchs = div(n_seq,batch_size)       
    # an array storing all batchs
    data = []
    # create two 2D empty arrays of shape (seqlen,batch_size)
    # one for X_batch and the other for y_batch
    for nb in 0:(n_batchs-1) 
        X_batch = fill(Float32(0.0),seqlen,batch_size)  
        y_batch = fill(Float32(0.0),seqlen,batch_size)
        for s in 0:(batch_size-1)  
            loc = 1+s*step_size+nb*batch_size*step_size 
            for i in 0:(seqlen-1)
                X_batch[i+1,s+1] = X[loc+i]
                # y_batch will just be the next of the values in X_batch
                y_batch[i+1,s+1] = X[loc+i+1]
            end
        end
        push!(data,(X_batch,y_batch))
    end
    data
end

data = create_batch(X,50,3,100);
@show length(data)
# length(data) = 3
@show size(data[1][1])
# size((data[1])[1]) = (50, 100)
```

Now I have splitted the trainning data into 3  mini-batchs. In each mini-batch, X_batch and y_batch both have size being `(50,100)`, where `50` is the length of the sequence and `100` is the batch_size I set.

# 4. LSTM model and training

I will build an LSTM model with 4 layers using `Flux`.

```julia
# LSTM model
model = Chain(
    LSTM(50,64),         #(50,100) => (64,100)
    LSTM(64,64),         #(64,100) => (64,100)
    Dense(64,32,relu),   #(64,100) => (32,100)
    Dense(32,50))        #(32,100) => (50,100)
# loss function
function loss(xs,ys)
    l = Flux.mse(model(xs),ys)
    Flux.reset!(model)  # Reset the hidden state of a recurrent layer back to its original value.
    return l
end
# Adam optimizer
opt = ADAM(0.001)
# train for 100 epochs
epochs = 200
for epoch in 1:epochs
    for minibatch in data
        Flux.train!(loss, params(model),[minibatch], opt)
    end
    # print loss for all three mini-batchs every 10 epochs
    if epoch % 20 ==0
        println("epoch $epoch:")
        println("$(Tracker.data(loss(data[1]...))),\t\t$(Tracker.data(loss(data[2]...))),\t\t$(Tracker.data(loss(data[3]...)))")
    end
end
```

```
epoch 20:
0.17965314,		0.19062711,		0.17659807
epoch 40:
0.028906763,		0.02891924,		0.02945946
epoch 60:
0.013280542,		0.01394347,		0.013914293
epoch 80:
0.005228083,		0.005093274,		0.005429643
epoch 100:
0.003956349,		0.0037793624,		0.0042144856
epoch 120:
0.0019410179,		0.0018023388,		0.002094826
epoch 140:
0.0013118078,		0.0011917885,		0.0014822737
epoch 160:
0.0012372733,		0.0011272279,		0.001435466
epoch 180:
0.0011470213,		0.0010351199,		0.0013277353
epoch 200:
0.001106722,		0.0010026643,		0.001290367
```

We can see the loss function decreases pretty well. 


# 5. Visualization

The following figure contains 5 prediction plots
1. use [1,50] to predict [51,250]
2. use [251,300] to predict [301,500]
3. use [501,550] to predict [551,750]
4. use [751,800] to predict [801,1000]
5. use [1,50] to predict [51,1000]
6. use [951,1000] to predict [1001,2000]    <---- compare with test data

```julia
X_pred_1 = X[1:50]
X_pred_2 = X[251:300]
X_pred_3 = X[501:550]
X_pred_4 = X[751:800]
X_pred_test = X[951:1000]

# remember to call Flux.reset! after each step
for i in 1:200
    Flux.reset!(model)
    push!(X_pred_2,Tracker.data(model(X_pred_2[i:49+i])[end]))
    Flux.reset!(model)
    push!(X_pred_3,Tracker.data(model(X_pred_3[i:49+i])[end]))
    Flux.reset!(model)
    push!(X_pred_4,Tracker.data(model(X_pred_4[i:49+i])[end]))
end
for i in 1:950
    Flux.reset!(model)
    push!(X_pred_1,Tracker.data(model(X_pred_1[i:49+i])[end]))
end
for i in 1:1000
    Flux.reset!(model)
    push!(X_pred_test,Tracker.data(model(X_pred_test[i:49+i])[end]))
end
# set a layout, 4 subplots in row 1 and 1 subplot in row 2
l = @layout [a b c d; f;g]
plot(1:250,X[1:250],label="")
p1 = plot!(51:250,X_pred_1[51:250],lw=5,linealpha=0.4,c=:red,label="")
plot(251:500,X[251:500],label="")
p2 = plot!(301:500,X_pred_2[51:250],lw=5,linealpha=0.4,c=:red,label="")
plot(501:750,X[501:750],label="")
p3 = plot!(551:750,X_pred_3[51:250],lw=5,linealpha=0.4,c=:red,label="")
plot(751:1000,X[751:1000],label="")
p4 = plot!(801:1000,X_pred_4[51:250],lw=5,linealpha=0.4,c=:red,label="")
plot(X,label="real")
p5 = plot!(51:1000,X_pred_1[51:1000],lw=5,linealpha=0.4,c=:red,label="prediction")
plot(951:1000,X[951:1000],label="train")
plot!(1001:2000,X_test,label="test")
p6 = plot!(1001:2000,X_pred_test[51:1050],lw=5,linealpha=0.4,c=:green,label="prediction")
plot(p1,p2,p3,p4,p5,p6,layout=l,size=(900,400))
```

<center><img src="/assets/figures/julia_notes/07_prediction.png" width="1000" ></center>

It can be seen from the above figure that our predictions of the next 200 timestamps using 50 timestamps are pretty well. Even for the prediction of 950 future timestamps, the shape is correct with only some minor delays of the prediction. The model behaves quite well on the test data. So it means that the model did not simply memorize the training set to make good predictions.

# 6. Further

Even for a dataset with an extra exponential decay factor, the LSTM model still performs pretty good. It can capture the decay of data.

<center><img src="/assets/figures/julia_notes/07_decay.png" width="1000" ></center>
