---
title: "Probabilistic Modeling - 1"
layout: splash
excerpt: "Overview"
categories: [Python]
tags: [Edward, Bayesian Inference, Variational Inference]
---

# 0. Introduction


# 1. What is a probabilistic model?

A probabilistic model is a joint distribution
<center><img src="https://latex.codecogs.com/png.latex?p(\mathbf{x},\mathbf{z})"/> </center>
of oberved variables <img src="https://latex.codecogs.com/png.latex?\mathbf{x}"/> and latent variables <img src="https://latex.codecogs.com/png.latex?\mathbf{z}"/> that provide the hidden structure. This joint distribution can be factorized into two components.
<center><img src="https://latex.codecogs.com/png.latex?p(\mathbf{x},\mathbf{z})=p(\mathbf{x}\mid\mathbf{z})p(\mathbf{z})"/> </center>
1. The likelihood
<center><img src="https://latex.codecogs.com/png.latex?p(\mathbf{x}\mid\mathbf{z})"/> </center>
describes how any data <img src="https://latex.codecogs.com/png.latex?\mathbf{x}"/> depend on the latent variables <img src="https://latex.codecogs.com/png.latex?\mathbf{z}"/>.
2. The prior
<center><img src="https://latex.codecogs.com/png.latex?p(\mathbf{z})"/> </center>
is a generating process of the hidden structure.

# 2. Inference of probabilistic models

How can we use a model <img src="https://latex.codecogs.com/png.latex?p(\mathbf{x},\mathbf{z})"/> to analyze some data <img src="https://latex.codecogs.com/png.latex?\textbf{x}"/>? We seek to infer this hidden structure using the probabilistic model.

The Bayesian inference is to use Bayes' rule to define the posterior
<center><img src="https://latex.codecogs.com/png.latex?p(\mathbf{z}\mid\mathbf{x})=\frac{p(\mathbf{x},\mathbf{z})}{p(\mathbf{x})}=\frac{p(\mathbf{x},\mathbf{z})}{\int p(\mathbf{x},\mathbf{z})d\mathbf{z}}"/> </center>
The posterior is the distribution of the latent variables <img src="https://latex.codecogs.com/png.latex?\textbf{z}"/> conditioned on some observed data <img src="https://latex.codecogs.com/png.latex?\textbf{x}"/>. The  posterior is just the updated hypothesis about the latent variables.

The posterior is difficult to compute because of its denominator. which is a high-dimensional integral. In practice, we usually approximate the posterior instead of calculating the exact posterior.

# 3. Classes of Inference

## 3.1 Variational Inference

Variational inference transfers posterior inference as an optimization problem. It involves the following two steps:
1. assume a family of distributions <img src="https://latex.codecogs.com/png.latex?q(\textbf{z};\lambda)"/> over latent variables;
2. match <img src="https://latex.codecogs.com/png.latex?q(\textbf{z};\lambda)"/> to the posterior by optimizing over its parameters <img src="https://latex.codecogs.com/png.latex?\lambda"/>. 

Now the solution <img src="https://latex.codecogs.com/png.latex?\lambda^*"/> of the optimization problem that minimizing a divergence measure
<center><img src="https://latex.codecogs.com/png.latex?\lambda^*=\mathrm{argmin}_{\lambda}\mathrm{divergence}\bigg(p(\mathbf{z}\mid \mathbf{x}),q(\mathbf{z};\lambda)}\bigg)"/></center>
can be used to obtain a optimized distribution <img src="https://latex.codecogs.com/png.latex?q(\textbf{z};\lambda^*)"/> that is used as an approximation of the posterior <img src="https://latex.codecogs.com/png.latex?p(\textbf{z}\mid\textbf{x})"/>

The 4 main forms of variational inference are

1. KLpq: Kullback-Leibler divergence from p to q
2. KLqp: Kullback-Leibler divergence from q to p 
3. MAP: maximum a posteriori
4. Laplace approximation: an improvement of MAP

We will discuss them in the next few sections.

## 3.1.1 KLpq vs KLqp

The Kullback-Leibler (KL) divergence has many useful properties.
* It's 0 if and only if p and q are the same distribution in the discreate case, or equal "almost everywhere" in the continuous case.
* It's non-negative and asymmetric.

So How do we choose between KLpq and KLqp?
1. KLpq: leads to an approximation that usually places high probability anywhere that the true distribution places high probability;
2. KLqp: leads to an approximation that rarely places highprobability anywhere that the true distribution places low probability.

The following figure is adopted from **Deep Learning** *by Ian Goodfellow and Yoshua Bengio and Aaron Courville*).

<center><img src="/assets/figures/edward/1_pqvsqp.png" width="800" ></center>

On the left panel, KLpq tries to place high probability on both two peaks, which leads to blurring two peaks together. On the right panel, KLqp tries to avoid placing high probability on the low-probability area of p, which leads to emphasizing only the left peak.

## 3.1.2 KLpq

There is only one strategy to use importance sampling to both estimate the objective and calculate stochastic gradients. 

Adaptive importance sampling follows this gradient to a local optimum using stochastic optimization. It is adaptive because the variational distribution <img src="https://latex.codecogs.com/png.latex?q(\textbf{z};\lambda)"/> iteratively gets closer to the posterior <img src="https://latex.codecogs.com/png.latex?p(\textbf{z}\mid\textbf{x})"/>.

In `Edward`, there is only one class related to KLpq.
1. `ed.KLpq`: uses a technique from adaptive importance sampling to perform the optimization.

## 3.1.3 KLqp

There are two main strategies to obtain the gradient (used for gradient descent in the optimization) of the KLqp (indeed the ELBO (Evidence Lower BOund)).
1. Score function gradient: an unbiased estimate of the actual gradient of ELBO
2. Reparametrization gradient: an unbiased estimate of the actual gradient of ELBO. In general, it has lower variance than the score function gradient, leading to a faster convergence in most cases.

In `Edward`, there are a lot of classes related to KLqp
1. `ed.KLqp`: minimizes the objective by automatically selecting from a variety of black box inference techniques;
2. `ed.ScoreKLqp`: minimizes the objective using the score function gradient;
3. `ed.ReparameterizationKLqp`: minimizes the objective using the reparameterization gradient.


## 3.1.4 MAP and Laplace

Maximum a posteriori (MAP) estimation approximates the posterior <img src="https://latex.codecogs.com/png.latex?p(\textbf{z}\mid\textbf{x})"/> with a point mass (delta function) by simply capturing its mode. The Laplace approximation is one way of improving a MAP estimate. The idea is to approximate the posterior with a normal distribution centered at the MAP estimate.

1. MAP is fast and efficient.
2. Laplace provides a normal distribution leading to a better approximation.

In `Edward`, 
1. `ed.MAP` implements gradient-based optimization of MAP;
2. `ed.Laplace` approximates the posterior distribution using a multivariate normal distribution centered at the mode of the posterior.





<center><img src="https://latex.codecogs.com/png.latex?"/> </center>
<center><img src="https://latex.codecogs.com/png.latex?"/> </center>
<center><img src="https://latex.codecogs.com/png.latex?"/> </center>
<center><img src="https://latex.codecogs.com/png.latex?"/> </center>
<center><img src="https://latex.codecogs.com/png.latex?"/> </center>
<center><img src="https://latex.codecogs.com/png.latex?"/> </center>
<center><img src="https://latex.codecogs.com/png.latex?"/> </center>
