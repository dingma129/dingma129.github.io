---
title: PyTorch (part 1)
author: Ding Ma
layout: post
categories: [blog]
---
<span style="font-weight:bold;font-size:36px">0. Introduction</span>

In this blog, I will clarify the following interesting topics of `PyTorch`.

[<span style="color:blue">1.</span>](#1) `grad` attribute is accumulative

[<span style="color:blue">2.</span>](#2) difference between `retain_graph` and `create_graph`

[<span style="color:blue">3.</span>](#3) difference between `backward` and `autograd.grad`

[<span style="color:blue">4.</span>](#4) higher order derivative/gradient using `autograd.grad`

---
<a name="1"></a><span style="font-weight:bold;font-size:36px">1. `grad` attribute is accumulative</span>

Everytime you run `backward` or `autograd.grad`, the obtained gradient will accumulate in all leaves. Sometimes, you need to manually reset a leaf's `.grad` attribute to be 0 for each epoch of training.
```python
# without manually resetting, .grad is accumulative
>>> x = torch.ones(1, requires_grad=True)
>>> y = x**7
>>> y.backward(retain_graph=True)
>>> print(x.grad)
tensor([7.])
>>> y.backward()
>>> print(x.grad)
tensor([14.])   # accumulated, 7+7=14

# manually resetting to 0
>>> x = torch.ones(1, requires_grad=True)
>>> y = x**7
>>> y.backward(retain_graph=True)
>>> print(x.grad)
tensor([7.])
>>> x.grad.zero_() # reset x.grad to 0, zero_() is an inplace method
>>> y.backward()
>>> print(x.grad)
tensor([7.])
```
---
<a name="2"></a><span style="font-weight:bold;font-size:36px">2. difference between `retain_graph` and `create_graph`</span>

The `backward` method of a tensor computes the gradient of current tensor w.r.t. graph leaves. There are two important parameters of `backward`, which are `retain_graph` and `create_graph`. According to the documentation, `retain_graph` defaults to the value of `create_graph`.

What are the differences between `retain_graph` and `create_graph`? 

<u><b>In short</b></u>,
* `retain_graph` allows you to re-compute gradient multiple times;
* `create_graph` allows you to compute higher order gredient.

Let us look at one example.

```python
# retain_graph=False
>>> x = torch.ones(1, requires_grad=True)
>>> y = x**7
>>> y.backward()
>>> print(x.grad)
tensor([7.])
>>> y.backward() # error, graph has been freed, cannot do .backward()
RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.

# retain_graph=True
>>> x = torch.ones(1, requires_grad=True)
>>> y = x**7
>>> y.backward(retain_graph=True)
>>> print(x.grad)
tensor([7.])
>>> y.backward()
>>> print(x.grad)
tensor([14.]) # x.grad is 7+7 (accumulated), this is not 2nd derivative

# create_graph=True  =>  retain_graph=True
>>> x = torch.ones(1, requires_grad=True)
>>> y = x**7
>>> y.backward(create_graph=True) # by default retain_graph=create_graph
>>> print(x.grad)
tensor([7.], grad_fn=<CloneBackward>)   # this grad_fn will be used when taking higher order derivatives
>>> y.backward()
>>> print(x.grad)
tensor([14.], grad_fn=<CloneBackward>)
# what happens if we set (retain_graph=False,create_graph=True)?
# will get RuntimeError
```
In conclusion, if you need to compute first order gradient several times for some reason, set `retain_graph=True`. Set `create_graph=True` only when you really need to do, since it will cost more memory.

---
<a name="3"></a><span style="font-weight:bold;font-size:36px">3. difference between `backward` and `autograd.grad`</span>

<u><b>In short</b></u>,
* `backward` will accumulate gradients in the leaves;
* `autograd.grad` won't. (`only_inputs` argument is deprecated and is ignored now (defaults to True))

`autograd.grad` also has parameters `retain_graph` and `create_graph` as `backward`.

Let us look at one example.
```python
>>> from torch.autograd import grad
>>> x = torch.ones(1, requires_grad=True)
>>> y = x**7
>>> print(grad(y,x,retain_graph=True))   # returns gradient of x
(tensor([7.]),)
>>> print(x.grad)  # does not accumulate in x.grad, still None
None
>>> y.backward(retain_graph=True)  # gradient of x accumulates in x.grad
>>> print(x.grad)    # None => 7
tensor([7.])
>>> print(grad(y,x,retain_graph=True))
(tensor([7.]),)
>>> print(x.grad)    # 7 => 7, nothing accumulates in x.grad
tensor([7.])
```
---
<a name="4"></a><span style="font-weight:bold;font-size:36px">4. higher order derivative/gradient using `autograd.grad`</span>

In order to compute higher order derivative, we need to set `create_graph=True`.

Let us look at one example.
```python
>>> from torch.autograd import grad
>>> x = torch.ones(1, requires_grad=True)
>>> y = x**7
>>> d1x = grad(y,x,create_graph=True)
>>> print(d1x)  # 1st derivative of x**7 is 7x**6, which evaluates to 7 when x=1
(tensor([7.], grad_fn=<MulBackward0>),)
>>> d2x = grad(d1x,x,create_graph=True)
>>> print(d2x)  # 2nd derivative of x**7 is 42x**5, which evaluates to 42 when x=1
(tensor([42.], grad_fn=<MulBackward0>),)

# we can also define a reusable function for this
>>> def nth_derivative(y, x, n):
>>>     for _ in range(n):
>>>         # taking derivative on y
>>>         d = grad(y, x, create_graph=True)[0]
>>>         # replace y by its derivative d
>>>         y = d
>>>     return d
>>> x = torch.ones(1, requires_grad=True)
>>> y = x**7
>>> print(nth_derivative(y,x,1))
tensor([7.], grad_fn=<MulBackward0>)
>>> print(nth_derivative(y,x,2))
tensor([42.], grad_fn=<MulBackward0>)
>>> print(nth_derivative(y,x,3))
tensor([210.], grad_fn=<MulBackward0>)
>>> print(x.grad)   # since we only used autograd.grad, no gradient accumulates
None
```