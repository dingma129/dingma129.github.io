---
title: "Probabilistic Modeling - 3"
layout: splash
excerpt: "Bayesian Linear Regression using Pyro"
categories: [Python]
tags: [Pyro, PyTorch, Bayesian Inference, Variational Inference, Regression]
---

# 0. Introduction

In this blog, I will introduce how to do Bayesian linear regression in [Pyro](http://pyro.ai/). Pyro is a universal probabilistic programming language (PPL) written in Python and supported by PyTorch on the backend. Pyro enables flexible and expressive deep probabilistic modeling, unifying the best of modern deep learning and Bayesian modeling.

# 1. Basics of `Pyro`

Primitive stochastic functions, or distributions, are an important class of stochastic functions for which we can explicitly compute the probability of the outputs given the inputs.

It is easy to use primitive stochastic functions in `Pyro`.
```python
import pyro
normal = pyro.distributions.Normal(0.0,1.0)  # defines a standard normal distribution
```

There are three different ways to sample from primitive stochastic functions.
```python
# sample()
normal.sample(size)
# rsample() is sample() wrapped in torch.no_grad
normal.rsample(size)
# the following is named sample
pyro.sample("my_sample",normal,size)
```

# 2. Variational inference in `Pyro`

In order to do variational inference, there are several key components:
1. model (p): the probabilistic model, which is a stochatics function containing `pyro.sample()` with or without `obs` argument and  `pyro.param()`.
2. guide (q): the variational distribution in most of the literature, which is a stochastic function containing `pyro.sample()` and `pyro.param()`. It does not contain observed data.

Both model and guide should take the same arguments. Whenever the model contains a random variable defined by `pyro.sample()`, the guide needs to have a matching `pyro.sample()` statement. The optional `obs` keyword argument of `pyro.sample` is used to pass observations directly.

In `Pyro` the machinery for doing variational inference is encapsulated in the `SVI` class (stands for stochastic variational inference). At present, `SVI` only provides support for the [ELBO objective](/python/Edward_1/), which is nothing but KLqp (up to an evidence term) in the previous blog. The user needs to provide three things: the model, the guide, and an optimizer, like the following
```python
import pyro
from pyro.infer import SVI, Trace_ELBO
svi = SVI(model, guide, optimizer, loss=Trace_ELBO())
```
The `SVI` object provides two methods, `step()` and `evaluate_loss()`.  As in `PyTorch`, the `step()` method takes a single gradient step and returns an estimate of the loss.


# 3. Toy example

In this section, I will introduce two ways of doing Bayesian linear regression in `Pyro`. They are almost identical, except that in the first method I defined the guide manually, while in the second method I used `autoguide` from `Pyro`.


## 3.1 Data

The dataset I will use for this section is the following.

```python
import numpy as np
import torch
import pyro
import matplotlib.pyplot as plt
# set random seed
pyro.set_rng_seed(42)
# 50 samples and 1 feature
N = 50
D = 1
# w,b
w_true = Normal(torch.ones(D)*6.0, torch.ones(D)*2.0).sample()
b_true = Normal(torch.ones(1)*10.0, torch.ones(1)*5.0).sample()
# std for error
sigma_true = torch.tensor(0.5)
# generate data
X_train = torch.randn(N, D)
y_train = X_train @ w_true + b_true + Normal(0.0, sigma_true).sample([N])
# plot
plt.scatter(X_train,y_train,alpha=0.6);
```
<center><img src="/assets/figures/pyro/1_data1.png" width="500" ></center>


## 3.2 Define guide manually

We can define the guide manually. Keep in mind that whenever the model contains a random variable defined by `pyro.sample()`, the guide needs to have a matching `pyro.sample()` statement. (e.g. `w`,`b` and `sigma` in the following code)
```python
import pyro.infer
from pyro.optim import Adam
from pyro.distributions import Normal, Uniform, Delta
from pyro.infer import SVI, Trace_ELBO, TracePredictive
from torch.distributions import constraints
# training hyperparameter
LEARNING_RATE = 0.05
NUM_STEPS = 4000
NUM_SAMPLES = 20

# model with prior for w and b
def model(x, y):
    w = pyro.sample("w", Normal(torch.zeros(D), torch.ones(D)))
    b = pyro.sample("b", Normal(torch.zeros(1), torch.ones(1)))
    sigma = pyro.sample("sigma", Uniform(0.0,1.0))
    mu = x @ w + b
    # obs = y => used to pass the observations
    return pyro.sample("y", Normal(mu, sigma), obs=y)

# use manual guide
def guide(x,y):
    # define pyro.params
    qw_loc = pyro.param("qw_loc", torch.tensor([0.0]))
    qw_scale = pyro.param("qw_scale", torch.tensor([1.0]), constraint=constraints.positive)
    qb_loc = pyro.param("qb_loc", torch.tensor([0.0]))
    qb_scale = pyro.param("qb_scale", torch.tensor([1.0]), constraint=constraints.positive)
    qsigma_loc = pyro.param("qsigma_loc", torch.tensor([0.5]), constraint=constraints.interval(0.0,1.0))
    return pyro.sample("w", Normal(qw_loc, qw_scale)),pyro.sample("b", Normal(qb_loc, qb_scale)),pyro.sample("sigma", Delta(qsigma_loc))

# using Adam optimizer
optimizer = pyro.optim.Adam({"lr": LEARNING_RATE})
# loss function
loss = pyro.infer.Trace_ELBO()
# clear all pyro.param
pyro.clear_param_store()
# stochastic variational inference
svi = pyro.infer.SVI(model, guide, optimizer, loss, num_samples=NUM_SAMPLES)
# record losses
losses = np.empty(NUM_STEPS)
# variational inference for 4000 steps
for step in range(NUM_STEPS):
    losses[step] = svi.step(X_train, y_train)
    if step % 500 == 0:
        print(f"step: {step:>5}, ELBO loss: {losses[step]:.2f}")
```
```
step:     0, ELBO loss: 16186.63
step:   500, ELBO loss: 134.39
step:  1000, ELBO loss: 130.11
step:  1500, ELBO loss: 130.27
step:  2000, ELBO loss: 129.98
step:  2500, ELBO loss: 129.57
step:  3000, ELBO loss: 128.67
step:  3500, ELBO loss: 126.92
```
In order to get all inferred parameters, call `pyro.get_param_store()`
```python
list(pyro.get_param_store().items())
#[('qw_loc', tensor([6.6178], requires_grad=True)),
# ('qw_scale', tensor([0.1221], grad_fn=<AddBackward0>)),
# ('qb_loc', tensor([10.6407], requires_grad=True)),
# ('qb_scale', tensor([0.1208], grad_fn=<AddBackward0>)),
# ('qsigma_loc', tensor([0.7165], grad_fn=<ClampBackward>))]
```

We can sample from the posterior distributions of `w` and `b` to get some samples from the guide space.
```python
# record of w and b
w_list1 = []
b_list1 = []
# draw 100 samples
for _ in range(100):
    w_list1.append(Normal(pyro.get_param_store()["qw_loc"],pyro.get_param_store()["qw_scale"]).rsample().item())
    b_list1.append(Normal(pyro.get_param_store()["qb_loc"],pyro.get_param_store()["qb_scale"]).rsample().item())
#plot
X_grid = np.linspace(-4,4,num=5)
plt.figure(figsize=(10,6))
plt.scatter(X_train,y_train,alpha=1.0);
plt.title("manual guide, 100 sampled lines")
for i in range(100):
    plt.plot(X_grid,X_grid*w_list1[i]+b_list1[i],color="green",alpha=0.05)
```
<center><img src="/assets/figures/pyro/1_manual.png" width="600" ></center>
All sampled lines seem to be nice regression lines.

## 3.3 Define guide manually

The `pyro.infer.autoguide` module provides several automatic guide generation methods. In the following, I will use `AutoDiagonalNormal`.

```python
LEARNING_RATE = 0.05
NUM_STEPS = 4000
NUM_SAMPLES = 20

# model with prior for w and b
def model(x, y):
    w = pyro.sample("w", Normal(torch.zeros(D), torch.ones(D)))
    b = pyro.sample("b", Normal(torch.zeros(1), torch.ones(1)))
    sigma = pyro.sample("sigma", Uniform(0.0,1.0))
    mu = x @ w + b
    return pyro.sample("y", Normal(mu, sigma), obs=y)

# use auto guide
guide = pyro.infer.autoguide.AutoDiagonalNormal(model)

# using Adam optimizer
optimizer = pyro.optim.Adam({"lr": LEARNING_RATE})
loss = pyro.infer.Trace_ELBO()
svi = pyro.infer.SVI(model, guide, optimizer, loss, num_samples=NUM_SAMPLES)
losses = np.empty(NUM_STEPS)
pyro.clear_param_store()
for step in range(NUM_STEPS):
    losses[step] = svi.step(X_train, y_train)
    if step % 500 == 0:
        print(f"step: {step:>5}, ELBO loss: {losses[step]:.2f}")
```
```
step:     0, ELBO loss: 7563.55
step:   500, ELBO loss: 2503.28
step:  1000, ELBO loss: 1416.43
step:  1500, ELBO loss: 622.32
step:  2000, ELBO loss: 313.79
step:  2500, ELBO loss: 259.29
step:  3000, ELBO loss: 136.35
step:  3500, ELBO loss: 125.01
```
In order to get all inferred parameters, also call `pyro.get_param_store()`. But this time, the names of the parameters are given by `auto_loc` and `auto_scale`
```python
list(pyro.get_param_store().items())
#[('auto_loc', tensor([ 6.5493, 10.5660,  1.4341], requires_grad=True)),
# ('auto_scale', tensor([0.1291, 0.1202, 0.5113], grad_fn=<AddBackward0>))]
```
Sampling from posterior distribution is also easier in this case.  We can simply call `guide.sample_latent()`.
```python
w_list2 = []
b_list2 = []
for _ in range(100):
    # using guide.sample_latent() to sample
    w_tmp,b_tmp,sigma_tmp = guide.sample_latent().detach().numpy()
    w_list2.append(w_tmp)
    b_list2.append(b_tmp)
X_grid = np.linspace(-4,4,num=5)
plt.figure(figsize=(10,6))
plt.title("auto guide, 100 sampled lines")
plt.scatter(X_train,y_train,alpha=1.0);
for i in range(100):
    plt.plot(X_grid,X_grid*w_list2[i]+b_list2[i],color="blue",alpha=0.05)
```
<center><img src="/assets/figures/pyro/1_auto.png" width="600" ></center>
All sampled lines seem to be nice regression lines.