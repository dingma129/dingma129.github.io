---
title: "Probabilistic Modeling - 5"
layout: splash
excerpt: "Gaussian Mixture Model using Pyro"
categories: [Python]
tags: [Pyro, PyTorch, Variational Inference, MCMC]
---

# 0. Introduction

In this blog, I will introduce Gaussian Mixture Model (GMM). I will show two methods using Pyro: variational inference and MCMC. The dataset I will use for this blog is the South African Heart Disease dataset from [here](https://web.stanford.edu/~hastie/ElemStatLearn/data.html). I will reproduce the following results (Figure 6.17 in the famous book [The Elements of Statistical Learning (ESL)](http://web.stanford.edu/~hastie/ElemStatLearn/)).

<center><img src="/assets/figures/pyro/3_esl.png" width="600" ></center>
Let's first look at the dataset. I will use Gaussian Mixture Model to do the density estimation for the `age` variable. The range of `age` for this dataset is `[15,64]`.
<center><img src="/assets/figures/pyro/3_data.png" width="600" ></center>
Let's first import some necessary libraries.

```python
import numpy as np
import pandas as pd
import scipy.stats
import torch
from torch.distributions import constraints
import matplotlib.pyplot as plt
import pyro
from pyro.distributions import Normal, Uniform, Delta,Dirichlet,LogNormal,Categorical
from pyro import poutine
from pyro.contrib.autoguide import AutoDelta
from pyro.optim import Adam
from pyro.infer import SVI, TraceEnum_ELBO, config_enumerate, infer_discrete

plt.style.use('ggplot')
pyro.set_rng_seed(42)
%matplotlib inline
```

# 1. GMM using variational inference

To use variational inference in `Pyro`, we first need to define a model and a guide. Here I will do MAP estimation of the model, so I choose `AutoDelta` for the auto-generated guide.

Notice that in the definition of the model, I set the prior distribution of `locs` to be `Normal(40,15)`, which uses the extra knowledge about the age from the dataset.
```python
df = pd.read_csv("data/SAheart.data",index_col=0)
X_train = torch.tensor(df["age"].values,dtype=torch.float32)
# set number of components = 2
K = 2  
# clear all pyro parameters
pyro.clear_param_store()
# create a probabilistic model
@config_enumerate
def model(data):
    # Global variables.
    weights = pyro.sample('weights', Dirichlet(torch.ones(K)/K))   # weights: (2,)
    with pyro.plate('components', K):
        scales = pyro.sample('scales', LogNormal(0.0, 10.0))       # scale: (2,)
        locs = pyro.sample('locs', Normal(40., 15.))               # loc: (2,)
    with pyro.plate('data', len(data)):
        # with values 0/1
        assignment = pyro.sample('assignment', Categorical(weights))   # assignment: (N,)
        pyro.sample('obs', Normal(locs[assignment], scales[assignment]), obs=data)
# create a guide using AutoDelta => MAP estimation so using Delta
# only expose 'weights', 'locs', 'scale', hide 'assignment'
global_guide = AutoDelta(poutine.block(model, expose=['weights', 'locs', 'scales']))
```

For discreate samples, we should use `TraceEnum_ELBO`. GMM is very sensitive to the intialization. So instead of doing random initialization only once, I will set `auto_weights` uniformly, `auto_scales` to be half of the data standard deviation, and `auto_locs` to be 2 random samples from the data with 100 different random seeds. I will choose the seed with the smallest ELBO loss to be the initialization of the guide.

```python
def initialize(seed):
    pyro.set_rng_seed(seed)
    pyro.clear_param_store()
    # initial weights to be 1/K (equal weight)
    pyro.param('auto_weights', torch.ones(K)/K, constraint=constraints.simplex)
    # initial var to be half of the data var
    pyro.param('auto_scales', torch.ones(K)*X_train.std()/2, constraint=constraints.positive)
    # initialize means using K samples from the data
    pyro.param('auto_locs', X_train[torch.multinomial(torch.ones(len(X_train)) / len(X_train), K)]);
    # compute ELBO loss
    loss = svi.loss(model, global_guide, X_train)
    return loss

# choose the best initialization from seed = 0 - 99
loss, seed = min((initialize(seed), seed) for seed in range(100))
# set parameters of guide using the best seed
initialize(seed)
print('seed = {}, initial_loss = {}'.format(seed, loss))
# seed = 31, initial_loss = 1890.586181640625
```
Now we are ready for the variational inference. Here I will set the number of steps to be 500.
```python
NUM_STEPS = 500
losses = np.empty(NUM_STEPS)
for step in range(NUM_STEPS):
    losses[step] = svi.step(X_train)
    if step % 100 == 0:
        print(f"step: {step:>5}, ELBO loss: {losses[step]:.2f}")
# step:     0, ELBO loss: 1890.59
# step:   100, ELBO loss: 1865.34
# step:   200, ELBO loss: 1865.34
# step:   300, ELBO loss: 1865.34
# step:   400, ELBO loss: 1865.34
plt.figure(figsize=(12,4))
plt.ylim(1850,2000)
plt.plot(losses);
```
The ELBO loss function becomes very stable after only 50 steps.
<center><img src="/assets/figures/pyro/3_sviloss.png" width="600" ></center>

If we look at the inferred parameters, we get
```python
global_guide(X_train)
#{'weights': tensor([0.2937, 0.7063], grad_fn=<ExpandBackward>),
# 'scales': tensor([ 3.8789, 12.5630], grad_fn=<ExpandBackward>),
# 'locs': tensor([58.0435, 36.4738], grad_fn=<ExpandBackward>)}
```
They are almost the same as the following result in ESL.
<center><img src="/assets/figures/pyro/3_eslresult.png" width="600" ></center>

Now we can plot the density estimation obtained by the above model.
<center><img src="/assets/figures/pyro/3_sviplot.png" width="800" ></center>

# 2. GMM using MCMC

Instead of using variational inference, we can also use MCMC to infer the above probabilistic model.

It's quite simple to perform MCMC in `Pyro`.

```python
from pyro.infer.mcmc.api import MCMC
from pyro.infer.mcmc import NUTS
pyro.set_rng_seed(31)
# No-U-Turn Sampler kernel, which provides an efficient and convenient way to run Hamiltonian Monte Carlo.
kernel = NUTS(model)
# The samples generated during the warmup phase are discarded.
mcmc = MCMC(kernel, num_samples=400, warmup_steps=200)
# run MCMC
mcmc.run(X_train)
# get 1000 samples from the posterior distribution
posterior_samples = mcmc.get_samples(num_samples=1000)

# sample: 100%|██████████| 600/600 [01:12<00:00,  8.29it/s, step size=3.38e-01, acc. prob=0.932]
```

The inferred parameters are still very close to the one in ESL.
```python
print(posterior_samples["locs"].mean(dim=0))
print(posterior_samples["scales"].mean(dim=0))
print(posterior_samples["weights"].mean(dim=0))
# tensor([36.2628, 57.8483])
# tensor([12.5356,  4.0826])
# tensor([0.6955, 0.3045])
```

If we look at the heatmap of those 1000 posterior samples, we can clearly see there's only one hottest spot. 
<center><img src="/assets/figures/pyro/3_mcmcheat.png" width="600" ></center>
Also all the posterior samples are well-distinguished with small standard deviations.
<center><img src="/assets/figures/pyro/3_mcmcsample.png" width="800" ></center>


