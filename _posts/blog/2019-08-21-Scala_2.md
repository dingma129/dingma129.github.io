---
title: Scala (part 2)
subtitle: MapReduce Examples Using Scala
author: Ding Ma
layout: post
categories: [blog]
---

<span style="font-weight:bold;font-size:32px">0. Introduction</span>

<span style="font-weight:bold;font-size:32px">1. Max/Min/Avg Temperature</span>

The dataset I used for this section comes from [<span style="color:blue">Kaggle</span>](https://www.kaggle.com/muthuj7/weather-dataset). This dataset contains a lot of information, and I only used the `Formatted Date` and `Temperature (C)` column.
```scala
>>> val df1 = spark.read.option("header", "true").option("inferSchema", "true")
>>> .csv("data/1/*").select("Formatted Date","Temperature (C)")
>>> .withColumnRenamed("Formatted Date","date").withColumnRenamed("Temperature (C)","temperature")
// there are 96453 observations in this dataset
>>> println(df1.count())
96453
// preview
>>> df1.show(5,false)
```
```bash
+-----------------------------+-----------------+
|date                         |temperature      |
+-----------------------------+-----------------+
|2006-04-01 00:00:00.000 +0200|9.472222222222221|
|2006-04-01 01:00:00.000 +0200|9.355555555555558|
|2006-04-01 02:00:00.000 +0200|9.377777777777778|
+-----------------------------+-----------------+
only showing top 3 rows
```
Using UDFs, we can parse the `date` column into `year`, `month`, `day`, `hour` as follows.
```scala
>>> import org.apache.spark.sql.functions._
// define udfs to parse the date
>>> val parseYear = udf((s:String) => s.split(" ")(0).slice(0,4).toInt)
>>> val parseMonth = udf((s:String) => s.split(" ")(0).slice(5,7).toInt)
>>> val parseDay = udf((s:String) => s.split(" ")(0).slice(8,10).toInt)
>>> val parseHour = udf((s:String) => s.split(" ")(1).slice(0,2).toInt)
>>> val cols = Seq("year","month","day","hour","temperature")
>>> val df1Parsed = df1.withColumn("year",parseYear(col("date"))).withColumn("month",parseMonth(col("date")))
>>> .withColumn("day",parseDay(col("date"))).withColumn("hour",parseHour(col("date")))
>>> .select(cols.head, cols.tail: _*)
// preview
>>> df1Parsed.show(3,false)
```
```
+----+-----+---+----+-----------------+
|year|month|day|hour|temperature      |
+----+-----+---+----+-----------------+
|2006|4    |1  |0   |9.472222222222221|
|2006|4    |1  |1   |9.355555555555558|
|2006|4    |1  |2   |9.377777777777778|
+----+-----+---+----+-----------------+
only showing top 3 rows
```
* max/min/avg temperature by year
```scala
>>> df1Parsed.groupBy("year")
>>> .agg(round(max("temperature"),2).alias("max_temp"),
>>>      round(min("temperature"),2).alias("min_temp"),
>>>      round(avg("temperature"),2).alias("avg_temp"))
>>> .sort($"year".asc).show()
+----+--------+--------+--------+
|year|max_temp|min_temp|avg_temp|
+----+--------+--------+--------+
|2006|   34.01|  -14.09|   11.22|
|2007|   39.91|  -10.16|   12.14|
|2008|   37.76|  -11.13|   12.16|
|2009|   36.11|  -16.67|   12.27|
|2010|   34.93|  -15.48|    11.2|
|2011|    37.8|   -11.7|   11.52|
|2012|   38.86|  -21.82|   11.99|
|2013|   37.87|   -8.98|   11.94|
|2014|   33.91|  -13.26|   12.53|
|2015|   37.19|  -13.07|   12.31|
|2016|   34.81|  -10.13|   11.99|
+----+--------+--------+--------+
```
* avg temperature by year and month as a pivot table
```scala
>>> df1Parsed.groupBy("year").pivot("month")
>>> .agg(round(avg("temperature"),2)).sort($"year".asc).show()
+----+-----+-----+----+-----+-----+-----+-----+-----+-----+-----+----+-----+
|year|    1|    2|   3|    4|    5|    6|    7|    8|    9|   10|  11|   12|
+----+-----+-----+----+-----+-----+-----+-----+-----+-----+-----+----+-----+
|2006|-1.67|-0.06|4.53|12.63|15.67|19.33|23.58|19.49|18.17|12.86|7.13| 2.23|
|2007| 4.87| 4.86|8.45|12.35|17.54|21.96|23.71|23.24|14.54|10.47|3.86| -0.8|
|2008| 0.82| 4.08|7.11|11.78|17.19|21.54|21.89|22.65| 15.8| 12.6|7.18|  3.0|
|2009|-1.17| 1.83|6.66|14.56|17.86|19.62|23.16|23.03| 19.4|11.48|7.56| 2.59|
|2010|-1.32| 1.43| 6.7|12.17| 16.7|20.19|23.16|21.67|15.61| 8.83|8.62|-0.25|
|2011| 0.16|-0.16| 5.9|13.36|16.85|21.15|21.57|22.95|20.03| 10.3|2.48|  2.9|
|2012| 1.14|-5.15|7.81|12.57|17.16|22.19|24.57|23.66| 19.5|12.11|7.91|-0.29|
|2013| 1.16| 3.12|4.59|12.96| 17.3|20.12|22.55| 23.2|15.45|12.86|7.93| 1.47|
|2014| 2.78|  5.0|9.55|12.97|15.93|19.99|21.89| 20.7|17.33|12.73|7.84| 3.21|
|2015|  2.3| 2.42|7.24|11.61|17.11| 20.5|23.73|23.78|18.37|10.62|6.68| 2.66|
|2016|-0.12| 6.41|7.43|13.36| 16.3| 21.3|22.78|21.43|18.48|  9.9|5.29| 1.24|
+----+-----+-----+----+-----+-----+-----+-----+-----+-----+-----+----+-----+
```

<span style="font-weight:bold;font-size:32px">2. Deduplication</span>

In this section, I will use the following data contained in 2 files.
```scala
// file 1
2012-3-1 a
2012-3-2 b
2012-3-3 c
2012-3-4 d
2012-3-5 a
2012-3-6 b
2012-3-7 c
2012-3-3 c
// file 2
2012-3-1 b
2012-3-2 a
2012-3-3 b
2012-3-4 d
2012-3-5 a
2012-3-6 c
2012-3-7 d
2012-3-3 c
```
* deduplication using Spark DataFrame
```scala
// load data as a DataFrame
>>> val df2 = spark.read.csv("data/2/*")
// using .distinct to deduplicate
>>> df2.distinct.sort($"_c0".asc).show()
+----------+
|       _c0|
+----------+
|2012-3-1 a|
|2012-3-1 b|
|2012-3-2 a|
|2012-3-2 b|
|2012-3-3 b|
|2012-3-3 c|
|2012-3-4 d|
|2012-3-5 a|
|2012-3-6 b|
|2012-3-6 c|
|2012-3-7 c|
|2012-3-7 d|
+----------+
```
* deduplication using Spark RDD
```scala
// load data as a RDD
>>> val rdd2 = sc.textFile("data/2/*")
// using distinct
>>> rdd2.map{_.trim}.distinct.sortBy(identity).collect()
// using groupByKey
>>> rdd2.map{x=>(x.trim,"")}.groupByKey().sortByKey().keys.collect()
// using reduceByKey
>>> rdd2.map{x=>(x.trim,"")}.reduceByKey(_+_).sortByKey().keys.collect()
// all of them returns
Array(2012-3-1 a, 2012-3-1 b, 2012-3-2 a, 2012-3-2 b, 2012-3-3 b, 2012-3-3 c, 2012-3-4 d, 2012-3-5 a, 2012-3-6 b, 2012-3-6 c, 2012-3-7 c, 2012-3-7 d)
```

<span style="font-weight:bold;font-size:32px">3. Sorting with Rank</span>

In this section, I will use the following data contained in 3 files.
```scala
// file 1
2
32
654
32
15
756
65223
// file 2
5956
22
650
92
// file 3
26
54
6
```
* sorting using Spark DataFrame
```scala
// load data as a DataFrame
>>> val df3 = spark.read.csv("data/3/*")
// without rank, use sort
>>> df3.map{_.getString(0).trim.toInt}.sort($"value".asc).show()
skipped
// with rank use Window and row_number
>>> import org.apache.spark.sql.expressions.Window
>>> val w = Window.orderBy("value") 
df3.map{_.getString(0).trim.toInt}
.withColumn("rank", row_number.over(w)).show()
+-----+----+
|value|rank|
+-----+----+
|    2|   1|
|    6|   2|
|   15|   3|
|   22|   4|
|   26|   5|
|   32|   6|
|   32|   7|
|   54|   8|
|   92|   9|
|  650|  10|
|  654|  11|
|  756|  12|
| 5956|  13|
|65223|  14|
+-----+----+
```
* sorting using Spark RDD
```scala
// load data as a RDD
>>> val rdd3 = sc.textFile("data/3/*")
>>> rdd3.map{_.trim.toInt}.sortBy(identity).zipWithIndex
>>> .map{p => (p._1,p._2+1)}.collect()
Array((2,1), (6,2), (15,3), (22,4), (26,5), (32,6), (32,7), (54,8), (92,9), (650,10), (654,11), (756,12), (5956,13), (65223,14))
```

<span style="font-weight:bold;font-size:32px">4. Group Average</span>

In this section, I will use the following data contained in 3 files.
```scala
// file Chinese
Mike 78
John 89
Steve 96
Andrew 67
// file English
Mike 80
John 82
Steve 84
Andrew 86
// file Math
Mike 88
John 99
Steve 66
Andrew 77
```
* group average using Spark DataFrame
```scala
// load data
>>> val df4 = spark.read.csv("data/4/*")
>>> df4.map{ row => row.getString(0).trim.split(" ")}
>>> .map{p => (p(0),p(1).toInt)}.groupBy("_1")
>>> .agg(round(mean($"_2"),2).as("avg")).show()
+------+-----+
|    _1|  avg|
+------+-----+
| Steve| 82.0|
|  John| 90.0|
|Andrew|76.67|
|  Mike| 82.0|
+------+-----+
```

<span style="font-weight:bold;font-size:32px">5. Max/Min</span>

In this section, I will use the following data contained in 2 files.
```scala
// file 1
102
10
39
109
200
11
3
90
28
// file 2
5
2
30
838
10005
```
* max/min using Spark DataFrame
```scala
// load data, parse into Int
>>> val df5 = spark.read.csv("data/5/*").map{ row => row.getString(0).toInt}
// using select and sql.functions
>>> df5.select(max("value").as("max"),min("value").as("min"),count("value").as("count")).show()
// using groupBy and agg
>>> df5.groupBy().agg(max("value").as("max"),min("value").as("min"),count("value").as("count")).show()
// both of them has the same physical plan
+-----+---+-----+
|  max|min|count|
+-----+---+-----+
|10005|  2|   14|
+-----+---+-----+
```

<span style="font-weight:bold;font-size:32px">6. Word Count</span>

In this section, I will use Pride and Prejudice by Jane Austen from [<span style="color:blue">Gutenberg</span>](https://www.gutenberg.org/ebooks/1342).

* word count using Spark RDD
```scala
// load data as a RDD, transform into lower case and remove special char
>>> val rdd6 = sc.textFile("data/6/pride.txt").filter{_.length>0}
>>> .map{line => line.trim.toLowerCase.split("[\\W]+")}
// using reduceByKey
>>> rdd6.flatMap{x=>x zip Array.fill(x.size)(1)}
>>> .reduceByKey(_+_).sortBy{-_._2}.collect()
Array((the,4331), (to,4162), (of,3610), (and,3585), (her,2203), (i,2066), (a,1953), (in,1880), (was,1843), (she,1695), (that,1541), (it,1535), (not,1421), (you,1327), (he,1324), ("",1283), (his,1258), (be,1240), (as,1180), (had,1173), (for,1060), (with,1052), (but,1002), (is,857), (have,840), (at,787), (mr,785), (him,753), (on,715), (my,704), (s,649), (by,636), (elizabeth,635), (all,621), (they,597), (so,589), (were,564), (which,538), (could,526), (been,515), (from,493), (no,490), (very,485), (what,478), (would,468), (this,442), (their,442), (your,439), (me,431), (them,429), (darcy,417), (will,408), (said,401), (such,386), (when,373), (an,362), (there,350), (if,350), (mrs,343), (do,341), (are,338), (much,326), (am,323), (more,323), (bennet,323), (bingley,...
```

<span style="font-weight:bold;font-size:32px">7. Character Count</span>
I will use the same data as in Section 6.
* character count using Spark RDD
```scala
// load data as a RDD, transform into lower case and remove special char
>>> val rdd7 = sc.textFile("data/6/pride.txt").filter{_.length>0}
>>> .map{line => line.toLowerCase.trim.replaceAll("\\W", "").toList}
// using reduceByKey
>>> rdd7.flatMap{x=>x zip Array.fill(x.size)(1)}
>>> .reduceByKey(_+_).sortBy{-_._2}.collect()
Array((e,69371), (t,46643), (a,41686), (o,40039), (i,37832), (n,37691), (h,34067), (s,33110), (r,32298), (d,22302), (l,21592), (u,14987), (m,14764), (c,13462), (y,12706), (w,12305), (f,12000), (g,10030), (b,9088), (p,8227), (v,5726), (k,3208), (z,936), (j,873), (x,839), (_,808), (q,627), (1,19), (2,18), (5,17), (4,16), (3,16), (6,10), (8,7), (0,6), (7,6), (9,6))
```

<span style="font-weight:bold;font-size:32px">8. Bigram Count of Chinese</span>

In this section, I will use [<span style="color:blue">Romance of the Three Kingdoms</span>](https://en.wikipedia.org/wiki/Romance_of_the_Three_Kingdoms) from [<span style="color:blue">Gutenberg</span>](https://www.gutenberg.org/ebooks/23950).
```scala
// load data as a RDD, remove all special characters in Chinese
// use sliding(2) to get bigrams
>>> val rdd8 = sc.textFile("data/8/threekingdom.txt").filter{_.length>0}
>>> .map{line => line.trim.replaceAll("[　：「」『』。？！，、★]", "").sliding(2).toList}
>>> rdd8.flatMap{x=>x zip Array.fill(x.size)(1)}
>>> .reduceByKey(_+_).sortBy{-_._2}.collect()
Array((玄德,1765), (孔明,1642), (曹操,918), (將軍,915), (曰吾,776), (卻說,644), (司馬,561), (不可,548), (二人,544), (丞相,530), (關公,504), (引兵,472), (曰此,436), (雲長,429), (荊州,406), (德曰,389), (夏侯,383), (不能,383), (明曰,382), (蜀兵,380), (大喜,372), (曰汝,368), (如此,366), (呂布,361), (張飛,359), (如何,339), (商議,338), (諸葛,338), (軍馬,333), (天下,321), (操曰,321), (主公,320), (大怒,320), (魏延,316), (軍士,315), (孫權,315), (一人,313), (大驚,312), (趙雲,303), (魏兵,300), (左右,300), (笑曰,286), (劉備,284), (馬懿,283), (姜維,278), (夫人,276), (次日,266), (曰今,262), (去了,257), (問曰,256), (東吳,251), (於是,246), (袁紹,245), (視之,245), (十餘,240), (不知,237), (今日,237), (不敢,237), (引軍,236), (周瑜,235), (諸將,234), (何不,232), (眾將,231), (而去,229), (而走,226), (人馬,225), (大將,222), (之事,220), (漢中,218), (陛下,213), (馬超,213), (都督,212), (曰某,209), (有一,203), (二十,201), (天子,200...
```
Top 3 bigrams are the names of three main characters.