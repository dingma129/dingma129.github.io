---
title: "Julia: Notes-06"
layout: splash
excerpt: "MNIST: CNN"
categories: [Julia]
tags: [Study Note, Classification, MNIST, Flux, CNN]
---

# 0. load package


```julia
using Flux, Images,Plots, Statistics, MLBase
gr()
```




    Plots.GRBackend()



# 1. load data
We load data exactly the same way as in the last blog.


```julia
X = Flux.Data.MNIST.images();
y = Flux.Data.MNIST.labels();
```


```julia
size(X[1])
```




    (28, 28)



# 2. preprocessing data
Here I divides data into minibatchs with batch_size = 100. This can make training faster, but the objective function does change smoothly comparing to the non-minibatched version.

Also by using `Float32` instead of `Float64`, we can save some time and spaces.


```julia
# using Float32 makes computation faster
function create_minibatch(X,y,r)
    # create a 4D empty array of shape (28,28,1,batch_size)
    X_batch = Array{Float32}(undef,28,28,1,length(r))
    for i in 1:length(r)
        X_batch[:,:,1,i] = Float32.(X[r[i]])
    end
    y_batch = Flux.onehotbatch(y[r],0:9) 
    return (X_batch, y_batch)
end
    
batch_size = 100
# create indices for each mini_batch
mb_idxs = Iterators.partition(1:5000, batch_size)
# size = 5000, batch_size = 100
train_set = [create_minibatch(X[1:5000], y[1:5000], i) for i in mb_idxs];
# size = 5000, only one batch
test_set = create_minibatch(X[5001:10000], y[5001:10000], 1:5000);
```


```julia
println(size(train_set[1][1]))  # size of X_batch
println(size(train_set[1][2]))  # size of y_batch
println(size(test_set[1]))      # test set contains only one batch
println(size(test_set[2]))
```

    (28, 28, 1, 100)
    (10, 100)
    (28, 28, 1, 5000)
    (10, 5000)


# 3. CNN with 3 convolutional layers

The structure of our CNN is shown as below..
![png](/assets/figures/julia_notes/06_structure.png)


```julia
model = Chain(
    # (28,28,1,-1) => (28,28,8,-1) => (14,14,8,-1)
    Conv((3, 3), 1=>8, pad=(1,1), relu), MaxPool((2,2)),

    # (14,14,8,-1) => (14,14,16,-1) => (7,7,16,-1)
    Conv((3, 3), 8=>16, pad=(1,1), relu), MaxPool((2,2)),

    # (7,7,16,-1) => (7,7,8,-1) => (3,3,8,-1)
    Conv((3, 3), 16=>8, pad=(1,1), relu), MaxPool((2,2)),

    # reshape (3,3,8,-1) => (3*3*8=72,-1)
    x -> reshape(x, :, size(x, 4)),
    
    # (72,-1) => (10,-1)
    Dense(72, 10),

    softmax
)
```




    Chain(Conv((3, 3), 1=>8, relu), MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2)), Conv((3, 3), 8=>16, relu), MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2)), Conv((3, 3), 16=>8, relu), MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2)), #23, Dense(72, 10), softmax)




```julia
# feed model with 1st batch to see whether it compiles well
model(train_set[1][1])
```




    Tracked 10×100 Array{Float32,2}:
     0.0137187    0.00254569   0.00174597   …  0.00487373   0.00827201
     0.550319     0.838346     0.147642        0.130377     0.397707  
     0.00667734   0.000828594  0.00251577      0.00815865   0.0176692 
     0.000559231  0.000229944  0.00234129      0.000210579  0.00322588
     0.324949     0.0897815    0.78801         0.668173     0.32944   
     0.00269534   0.000507316  0.000778211  …  0.000442059  0.00351026
     0.0518562    0.00187702   0.0111518       0.105689     0.0597307 
     0.0082341    0.00706233   0.0176519       0.00560355   0.0910309 
     0.0353528    0.058523     0.0206755       0.0735759    0.0744027 
     0.00563831   0.000298173  0.0074881       0.00289689   0.0150109 



# 4. train and evaluate model
Here I only trained the model for 5 epochs. We can see that the accuracy is improving rapidly.
* start:   0.1446
* epoch 1: 0.7906
* epoch 2: 0.8930
* epoch 3: 0.9190
* epoch 4: 0.9342
* epoch 5: 0.9380


```julia
# loss function
loss(x, y) = sum(Flux.crossentropy(model(x), y))
# optimizer
opt = ADAM(0.005) 
# accuracy: using onecold to transfer from 10-dim onehot vector => 1-dim vector with values in 1:10
accuracy(x, y) = mean(Flux.onecold(model(x), 1:10) .== Flux.onecold(y, 1:10))

train_loss2 = Float32[]
test_loss2 = Float32[]

# train 5 epochs
for e in 1:5
    println("epoch $(e):")
    Flux.train!(loss, params(model), train_set, opt,cb=() -> @show accuracy(test_set...))
end
```

    epoch 1:
    accuracy(test_set...) = 0.1446
    accuracy(test_set...) = 0.1504
    accuracy(test_set...) = 0.1632
    accuracy(test_set...) = 0.1518
    accuracy(test_set...) = 0.137
    accuracy(test_set...) = 0.1294
    accuracy(test_set...) = 0.1378
    accuracy(test_set...) = 0.1534
    accuracy(test_set...) = 0.1696
    accuracy(test_set...) = 0.1734
    accuracy(test_set...) = 0.1836
    accuracy(test_set...) = 0.195
    accuracy(test_set...) = 0.2042
    accuracy(test_set...) = 0.2244
    accuracy(test_set...) = 0.2448
    accuracy(test_set...) = 0.2704
    accuracy(test_set...) = 0.3004
    accuracy(test_set...) = 0.3314
    accuracy(test_set...) = 0.3476
    accuracy(test_set...) = 0.3634
    accuracy(test_set...) = 0.3734
    accuracy(test_set...) = 0.3824
    accuracy(test_set...) = 0.3948
    accuracy(test_set...) = 0.4068
    accuracy(test_set...) = 0.4138
    accuracy(test_set...) = 0.4274
    accuracy(test_set...) = 0.4396
    accuracy(test_set...) = 0.4658
    accuracy(test_set...) = 0.505
    accuracy(test_set...) = 0.541
    accuracy(test_set...) = 0.5764
    accuracy(test_set...) = 0.6072
    accuracy(test_set...) = 0.6202
    accuracy(test_set...) = 0.6306
    accuracy(test_set...) = 0.6384
    accuracy(test_set...) = 0.6496
    accuracy(test_set...) = 0.6608
    accuracy(test_set...) = 0.6708
    accuracy(test_set...) = 0.6854
    accuracy(test_set...) = 0.7036
    accuracy(test_set...) = 0.7186
    accuracy(test_set...) = 0.7244
    accuracy(test_set...) = 0.7446
    accuracy(test_set...) = 0.7568
    accuracy(test_set...) = 0.757
    accuracy(test_set...) = 0.7482
    accuracy(test_set...) = 0.7572
    accuracy(test_set...) = 0.7734
    accuracy(test_set...) = 0.7834
    accuracy(test_set...) = 0.7906
    epoch 2:
    accuracy(test_set...) = 0.7934
    accuracy(test_set...) = 0.8
    accuracy(test_set...) = 0.8014
    accuracy(test_set...) = 0.7968
    accuracy(test_set...) = 0.7908
    accuracy(test_set...) = 0.7958
    accuracy(test_set...) = 0.8028
    accuracy(test_set...) = 0.8182
    accuracy(test_set...) = 0.8288
    accuracy(test_set...) = 0.8334
    accuracy(test_set...) = 0.8402
    accuracy(test_set...) = 0.844
    accuracy(test_set...) = 0.847
    accuracy(test_set...) = 0.8486
    accuracy(test_set...) = 0.853
    accuracy(test_set...) = 0.8584
    accuracy(test_set...) = 0.8596
    accuracy(test_set...) = 0.862
    accuracy(test_set...) = 0.8616
    accuracy(test_set...) = 0.8576
    accuracy(test_set...) = 0.8616
    accuracy(test_set...) = 0.8636
    accuracy(test_set...) = 0.8716
    accuracy(test_set...) = 0.8732
    accuracy(test_set...) = 0.8696
    accuracy(test_set...) = 0.8632
    accuracy(test_set...) = 0.8602
    accuracy(test_set...) = 0.8624
    accuracy(test_set...) = 0.8684
    accuracy(test_set...) = 0.8748
    accuracy(test_set...) = 0.8718
    accuracy(test_set...) = 0.8734
    accuracy(test_set...) = 0.8844
    accuracy(test_set...) = 0.888
    accuracy(test_set...) = 0.8822
    accuracy(test_set...) = 0.878
    accuracy(test_set...) = 0.88
    accuracy(test_set...) = 0.8852
    accuracy(test_set...) = 0.8908
    accuracy(test_set...) = 0.8924
    accuracy(test_set...) = 0.8868
    accuracy(test_set...) = 0.8856
    accuracy(test_set...) = 0.8918
    accuracy(test_set...) = 0.8998
    accuracy(test_set...) = 0.906
    accuracy(test_set...) = 0.9086
    accuracy(test_set...) = 0.9032
    accuracy(test_set...) = 0.8974
    accuracy(test_set...) = 0.8928
    accuracy(test_set...) = 0.893
    epoch 3:
    accuracy(test_set...) = 0.8968
    accuracy(test_set...) = 0.9008
    accuracy(test_set...) = 0.902
    accuracy(test_set...) = 0.9032
    accuracy(test_set...) = 0.9034
    accuracy(test_set...) = 0.8994
    accuracy(test_set...) = 0.8988
    accuracy(test_set...) = 0.899
    accuracy(test_set...) = 0.897
    accuracy(test_set...) = 0.8994
    accuracy(test_set...) = 0.9036
    accuracy(test_set...) = 0.9046
    accuracy(test_set...) = 0.9052
    accuracy(test_set...) = 0.906
    accuracy(test_set...) = 0.9058
    accuracy(test_set...) = 0.9068
    accuracy(test_set...) = 0.9094
    accuracy(test_set...) = 0.911
    accuracy(test_set...) = 0.913
    accuracy(test_set...) = 0.9164
    accuracy(test_set...) = 0.9142
    accuracy(test_set...) = 0.9136
    accuracy(test_set...) = 0.9144
    accuracy(test_set...) = 0.9126
    accuracy(test_set...) = 0.9118
    accuracy(test_set...) = 0.913
    accuracy(test_set...) = 0.9134
    accuracy(test_set...) = 0.912
    accuracy(test_set...) = 0.9086
    accuracy(test_set...) = 0.907
    accuracy(test_set...) = 0.9054
    accuracy(test_set...) = 0.904
    accuracy(test_set...) = 0.9066
    accuracy(test_set...) = 0.9104
    accuracy(test_set...) = 0.9128
    accuracy(test_set...) = 0.9122
    accuracy(test_set...) = 0.9106
    accuracy(test_set...) = 0.9096
    accuracy(test_set...) = 0.9086
    accuracy(test_set...) = 0.912
    accuracy(test_set...) = 0.9132
    accuracy(test_set...) = 0.9148
    accuracy(test_set...) = 0.9176
    accuracy(test_set...) = 0.9196
    accuracy(test_set...) = 0.9236
    accuracy(test_set...) = 0.925
    accuracy(test_set...) = 0.9244
    accuracy(test_set...) = 0.923
    accuracy(test_set...) = 0.9192
    accuracy(test_set...) = 0.919
    epoch 4:
    accuracy(test_set...) = 0.9216
    accuracy(test_set...) = 0.9234
    accuracy(test_set...) = 0.9198
    accuracy(test_set...) = 0.919
    accuracy(test_set...) = 0.9172
    accuracy(test_set...) = 0.9128
    accuracy(test_set...) = 0.9112
    accuracy(test_set...) = 0.9098
    accuracy(test_set...) = 0.9086
    accuracy(test_set...) = 0.9082
    accuracy(test_set...) = 0.915
    accuracy(test_set...) = 0.9174
    accuracy(test_set...) = 0.9196
    accuracy(test_set...) = 0.9212
    accuracy(test_set...) = 0.9196
    accuracy(test_set...) = 0.9188
    accuracy(test_set...) = 0.9184
    accuracy(test_set...) = 0.9204
    accuracy(test_set...) = 0.9208
    accuracy(test_set...) = 0.9242
    accuracy(test_set...) = 0.928
    accuracy(test_set...) = 0.9282
    accuracy(test_set...) = 0.9284
    accuracy(test_set...) = 0.9276
    accuracy(test_set...) = 0.927
    accuracy(test_set...) = 0.9264
    accuracy(test_set...) = 0.926
    accuracy(test_set...) = 0.927
    accuracy(test_set...) = 0.9254
    accuracy(test_set...) = 0.9228
    accuracy(test_set...) = 0.9212
    accuracy(test_set...) = 0.92
    accuracy(test_set...) = 0.921
    accuracy(test_set...) = 0.9238
    accuracy(test_set...) = 0.926
    accuracy(test_set...) = 0.9262
    accuracy(test_set...) = 0.926
    accuracy(test_set...) = 0.9256
    accuracy(test_set...) = 0.9252
    accuracy(test_set...) = 0.926
    accuracy(test_set...) = 0.926
    accuracy(test_set...) = 0.9252
    accuracy(test_set...) = 0.9248
    accuracy(test_set...) = 0.9284
    accuracy(test_set...) = 0.9326
    accuracy(test_set...) = 0.9346
    accuracy(test_set...) = 0.934
    accuracy(test_set...) = 0.9348
    accuracy(test_set...) = 0.9348
    accuracy(test_set...) = 0.9342
    epoch 5:
    accuracy(test_set...) = 0.932
    accuracy(test_set...) = 0.9324
    accuracy(test_set...) = 0.9334
    accuracy(test_set...) = 0.933
    accuracy(test_set...) = 0.9302
    accuracy(test_set...) = 0.9318
    accuracy(test_set...) = 0.9294
    accuracy(test_set...) = 0.9238
    accuracy(test_set...) = 0.9216
    accuracy(test_set...) = 0.9178
    accuracy(test_set...) = 0.92
    accuracy(test_set...) = 0.922
    accuracy(test_set...) = 0.9254
    accuracy(test_set...) = 0.928
    accuracy(test_set...) = 0.9302
    accuracy(test_set...) = 0.9314
    accuracy(test_set...) = 0.9294
    accuracy(test_set...) = 0.9292
    accuracy(test_set...) = 0.9272
    accuracy(test_set...) = 0.927
    accuracy(test_set...) = 0.932
    accuracy(test_set...) = 0.9318
    accuracy(test_set...) = 0.9332
    accuracy(test_set...) = 0.934
    accuracy(test_set...) = 0.9346
    accuracy(test_set...) = 0.9356
    accuracy(test_set...) = 0.9366
    accuracy(test_set...) = 0.9364
    accuracy(test_set...) = 0.9342
    accuracy(test_set...) = 0.932
    accuracy(test_set...) = 0.931
    accuracy(test_set...) = 0.9296
    accuracy(test_set...) = 0.9292
    accuracy(test_set...) = 0.9298
    accuracy(test_set...) = 0.929
    accuracy(test_set...) = 0.93
    accuracy(test_set...) = 0.927
    accuracy(test_set...) = 0.927
    accuracy(test_set...) = 0.927
    accuracy(test_set...) = 0.9264
    accuracy(test_set...) = 0.9278
    accuracy(test_set...) = 0.9296
    accuracy(test_set...) = 0.932
    accuracy(test_set...) = 0.9348
    accuracy(test_set...) = 0.9384
    accuracy(test_set...) = 0.94
    accuracy(test_set...) = 0.94
    accuracy(test_set...) = 0.94
    accuracy(test_set...) = 0.9402
    accuracy(test_set...) = 0.938


# 5. visualize confusion matrix
We can use confusion matrix to see how does the misclassification distribute.

We can either use the `MLBase` package to create a confusion matrix, or create it manually.


```julia
pred_test_labels = Flux.onecold(model(test_set[1]), 1:10)
true_test_labels = Flux.onecold(test_set[2], 1:10)

cm = MLBase.confusmat(10,true_test_labels,pred_test_labels)
```




    10×10 Array{Int64,2}:
     499    0    3    1    0   10    2    0    6    1
       0  542    5    3    3    3    0    2    6    0
       3    2  474    8    1    0    1    4   10    0
       2    2    4  503    1   13    0   12    1    1
       0    2    0    0  430    0    4    6    2    1
       1    0    0    2    1  419    1    1    4    0
       4    0    1    0    2    4  494    0    8    0
       0    0    7    2    5    2    0  497    3    4
       5    7    4    2    2    5    9    6  441    1
       7    1    0    8   16    9    0   39   12  391



Using `heapmap`, we can easily visualize the confusion matrix.


```julia
heatmap(cm, c=:inferno, title="Confusion Matrix", 
    ylabel="True label", 
    xlabel= "Predicted label", 
    xticks=(1:10, 0:9), 
    yticks=(1:10, 0:9))
```




![svg](/assets/figures/julia_notes/06_output_16_0.svg)



We can see that it's quite simple to use Flux just as `TensorFlow` or `PyTorch`. 
