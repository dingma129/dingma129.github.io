---
title: Recommender Systems (part 2)
subtitle: Model-Based Collaborative Filtering
author: Ding Ma
layout: post
categories: [blog]
---

<span style="font-weight:bold;font-size:32px">0. Introduction</span>

The neighborhood-based methods of the previous blog can be viewed as generalizations
of k-nearest neighbor classifiers. They are instance-based learning methods.

In model-based methods, a summarized model of the data is created up front, as with
supervised or unsupervised machine learning methods. Almost all traditional machine learning models can be generalized to the collaborative filtering scenario, just as k-nearest neighbor classifiers can be generalized to neighborhood-based models for collaborative filtering.

Even though neighborhood-based methods were among the earliest collaborative filtering
methods and were also among the most popular because of their simplicity, they are not
necessarily the most accurate models available today. In fact, some of the most accurate
methods are based on model-based techniques in general, and on latent factor models in
particular.

--
<span style="font-weight:bold;font-size:32px">1. Latent Factor Models</span>

Latent factor models are considered to be state-of-the-art in recommender systems. The goal is to use dimensionality reduction methods to directly estimate the data matrix in one shot. Under the covers, dimensionality reduction methods leverage the row and column correlations to create the fully specified and reduced representation. Most dimensionality reduction methods can also be expressed as matrix factorizations. Matrix factorization methods provide a neat way to leverage all row and column correlations in one shot to estimate the entire data matrix.

<center><img src="https://dingma129.github.io/assets/figures/rec/rec_2_mat_fact.png" width="600" ></center>
The key differences among various matrix factorization methods arise in terms of the
constraints imposed on U and V and the objective function.

<span style="font-weight:bold;font-size:32px">2. Various Matrix Factorization Methods</span>

---
<span style="font-weight:bold;font-size:28px">2.1 Unconstrained Matrix Factorization</span>

<span style="font-weight:bold;font-size:24px">2.1.1 SVD and ALS</span>

The most fundamental form of matrix factorization is the unconstrained case, in which
no constraints are imposed on the factor matrices U and V. Much of the recommendation
literature refers to unconstrained matrix factorization as singular value decomposition
(SVD). It can be summarized as the following figure. Extra regularization terms can also be added to the objective function as usual.

<center><img src="https://dingma129.github.io/assets/figures/rec/rec_2_uncon_mf.png" width="1000" ></center>
`SVD` is implemented in `Surprise` using stochastic gradient descent, and `ALS` is implemented in `spark.ml`. In the following, I will use `SVD` and `ALS` to predict the ratings using the same dataset as in the last blog.
```python
from surprise import SVD
# set biased=False means using vanilla SVD and latent dimension = 3
algosvd = SVD(n_factors=3,n_epochs=100,biased=False,random_state=42,reg_all=1,lr_all=0.01)
algosvd.fit(traindata)
# surprise.SVD does not provide methods to compute U or V
svdPred = np.zeros((5,6))
for i in range(5):
    for j in range(6):
        svdPred[i][j] = round(algosvd.estimate(i,j),2)
print(svdPred)
[[2.91 3.56 3.92 4.05 4.29 4.15]
 [1.37 1.67 1.84 1.9  2.02 1.95]
 [2.65 3.23 3.56 3.69 3.9  3.77]
 [1.48 1.78 1.96 2.05 2.14 2.07]
 [2.24 2.7  2.97 3.11 3.26 3.14]]
# objective J = 122.34
```
```python
from pyspark.ml.recommendation import ALS
# transform from pandas.DataFrame to spark.DataFrame
dfSurprise.columns = ["user","item","rating"]
df = spark.createDataFrame(dfSurprise)
# set latent dimension = 3, and train an ALS model
alsModel = ALS(rank=3,maxIter=10,regParam=0.01,seed=42,numUserBlocks=1,numItemBlocks=1).fit(df)
# get user/item factors
uF = alsModel.userFactors
iF = alsModel.itemFactors
# transform to numpy arrays in order to perform matrix computation
uFnp = np.array(uF.rdd.map(lambda row: row[1]).collect())
iFnp = np.array(iF.rdd.map(lambda row: row[1]).collect())
# get all predictions in one shot
alsPred = np.round(uFnp@iFnp.T,2)
print(alsPred)
[[4.99 6.02 7.   3.82 3.16 2.91]
 [4.26 2.54 2.89 4.17 4.7  4.15]
 [1.75 2.95 3.97 1.31 0.77 1.49]
 [6.93 4.   3.02 6.08 6.89 3.97]
 [0.93 0.73 3.04 1.94 2.14 4.94]]
# objective J = 0.2245
```
The following figure shows the relation between R, U and V.
<center><img src="https://dingma129.github.io/assets/figures/rec/rec_2_als.png" width="1000" ></center>

<span style="font-weight:bold;font-size:24px">2.1.2 SVD with Baselines</span>

A variation on SVD is to incorporate variables that can learn user and item biases. 
* first, there is a global bias term which is defined by global mean mu;
* associated with each user i, we have a variable o_i, which indicates the general bias of
users to rate items. For example, if user i is a generous person, who tends to rate all items
highly, then the variable oi will be a positive quantity. On the other hand, the value of o_i
will be negative for a person who rates most items negatively;
* similarly, the variable p_j denotes the bias in the ratings of item j.

<center><img src="https://dingma129.github.io/assets/figures/rec/rec_2_svdb.png" width="1000" ></center>

In fact, using only the bias variables can often provide reasonably good rating predictions. This means that a significant part of the ratings can be explained by user generosity and item popularity, rather than any specific personalized preferences of users for items. Such a model is equivalent to setting k=0 in the above SVD with Baselines models, which are also called baseline models. 

One can use such a baseline rating to enhance any off-the-shelf collaborative filtering model. To do so, one can simply subtract each Bij from the (i, j)th (observed) entry of the ratings matrix before applying collaborative filtering. These values are added back in a postprocessing
phase to the predicted values. 

In `Surprise`, we have the following models with baselines:
* `BaselineOnly` is the vanilla baseline model
* `KNNBaseline` is the KNN model with baseline
* `SVD(biased=True)` is the SVD model with baseline

<span style="font-weight:bold;font-size:24px">2.1.3 SVD++</span>

Even in cases in which users explicitly rate items, the identity of the items they rate can be viewed as an implicit feedback. In other words, a significant predictive value is captured by the identity of the items that users rate, irrespective of the actual values of the ratings.

Various frameworks such as SVD++ have been proposed to incorporate implicit feedback. SVD++ uses two different user factor matrices U and F Y , corresponding to explicit and implicit feedback, respectively.

<center><img src="https://dingma129.github.io/assets/figures/rec/rec_2_svdpp.png" width="1000" ></center>

SVD++ is implemented in `Surprise` as `SVDpp`, and the parameters are learned using SGD.

<span style="font-weight:bold;font-size:28px">2.2 Singular Value Decomposition</span>

Singular value decomposition (SVD) is a form of matrix factorization in which the columns
of U and V are constrained to be mutually orthogonal. Throughout this section, SVD includes contraints of orthogonality for U and V. 

For complete matrices, we can perform truncated SVD directly.
<center><img src="https://dingma129.github.io/assets/figures/rec/rec_2_tsvd1.png" width="1000" ></center>

For incomplete matrices, we should perform the following algorithm iteratively until convergence.
<center><img src="https://dingma129.github.io/assets/figures/rec/rec_2_tsvd2.png" width="1000" ></center>

We can use `sklearn.decomposition.TruncatedSVD` to perform the above algorithm.
```python
import numpy as np
from sklearn.decomposition import TruncatedSVD
# data
R = np.array([[1,-1,1,-1,1,-1],
              [1,1,np.nan,-1,-1,-1],
              [np.nan,1,1,-1,-1,np.nan],
              [-1,-1,-1,1,1,1],
              [-1,np.nan,-1,1,1,1]])
# a list to trace all predictions
missingList = []
# initialization: filling missing value using row means
Rf = np.where(np.isnan(R), np.nanmean(R,axis=1,keepdims=True), R)  
for _ in range(15):
    # keep track of (originally) missing values
    missing = np.where(np.isnan(R), Rf, np.nan)
    missingList.append(missing[~np.isnan(missing)])
    # step 1: truncated SVD of Rf
    # another choice algorithm="randomized" is much faster
    tsvd = TruncatedSVD(algorithm="arpack",n_components=2)
    tsvd.fit(Rf)
    # step 2: update Rf using the result of step 1
    Rf = np.where(np.isnan(R), tsvd.transform(Rf)@tsvd.components_, R)  
```
The algorithm is also explained in the following figure.
<center><img src="https://dingma129.github.io/assets/figures/rec/rec_2_tsvd3.png" width="1000" ></center>
The traces of the predictions of four missing values over 15 epochs are plotted in the following figure.
<center><img src="https://dingma129.github.io/assets/figures/rec/rec_2_plot.png" width="1000" ></center>
It can be seen from the above figure that truncated SVD converges rather fast. The prediction of r_23 and r_31 should be 1 and the prediction of r_36 and r_52 should be -1.

<span style="font-weight:bold;font-size:28px">2.3 Non-negative Matrix Factorization</span>

Non-negative matrix factorization (NMF) may be used for ratings matrices that are nonnegative. The major advantage of this approach is not necessarily one of accuracy, but that of the high level of interpretability it provides in understanding the user-item interactions. 

Its greatest interpretability advantages arise in cases in which users have a mechanism to specify a liking for an item, but no mechanism to specify a dislike. Such matrices include unary ratings matrices or matrices in which the non-negative entries correspond to the activity frequency. These data sets are also referred to as implicit feedback data sets.

In this case, it is often reasonably possible to set the unspecified entries to 0, rather than
treat them as missing values.

`NMF` is implemented in both `sklearn.decomposition.NMF` and `surprise.NMF`. 