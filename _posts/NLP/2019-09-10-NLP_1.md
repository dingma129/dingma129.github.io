---
title: "NLP - 1"
layout: splash
excerpt: "Topic Modeling using LDA"
categories: [Python]
tags: [LDA, NLP, NLTK, TF-IDF]
---

# 0. Introduction

Latent Dirichlet Allocation (LDA) is a generative probabilistic model for collections of discrete dataset such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents. 

Words in the corpus are the only data that we observe. The latent variables determine the random mixture of topics in the corpus and the distribution of words in the documents. The goal of LDA is to use the observed words to infer the hidden topic structure.

In this blog, I will introduce how to use `NLTK` and `LDA` to perform topic modeling in Python. In the first section, I will also show you how to use `BeautifulSoup` to parse the messy webpages obtained from Wikipedia using `urllib`.

# 1. Parsing pages from Wikipedia

I will first write a helper function to get the urls of all pages in a given wikipedia category.

Notice that in any given category page, both pages of subcategories and pages in the category is in a `<div>` tag with `class="mw-category"`. The following is an example from [category: number theory](https://en.wikipedia.org/wiki/Category:Number_theory).

<p align="middle">
<img src="/assets/figures/nlp/1_wiki1.png" width="400" >
<img src="/assets/figures/nlp/1_wiki2.png" width="400" >
</p>

Since I only want to keep the urls of pages in the category, I will only search for the last `<div class="mw-category>` tag. Here for this blog, I will only return the first 30 links in each category.

```python
import urllib
from bs4 import BeautifulSoup
def getCatAllUrl(category):
    base_url = "https://en.wikipedia.org"
    url = "https://en.wikipedia.org/wiki/Category:{}".format(category)
    page = urllib.request.urlopen(url)    
    # parsing using BeautifulSoup
    soup = BeautifulSoup(page,"lxml")
    # list for saving urls
    links = []
    # get the links of items in this category (index -1), not the links of subcategories (index 0)
    for i in soup.findAll("div",attrs={'class':"mw-category"})[-1]:
        links += [base_url+link['href'] for link in i.findAll("a")]    
    return links
# get data from the following 2 categories    
links1 = getCatAllUrl("Musical_terminology")
links2 = getCatAllUrl("Artificial_neural_networks")
```

Let's focus on two categories: musical_terminology and Artificial neural networks. They share some similarities but are also easy to be distinguished by human beings. 

Since I want to do a NLP task on those topics, I will first get all texts (in tag `<p>`) from those links in above two categories.

```python
def getTextFromUrl(links):
    all_text = []
    for link in links:
        page = urllib.request.urlopen(link)    
        soup = BeautifulSoup(page,"lxml")
        all_text.append(" ".join([para.text for para in soup.findAll("p")]))
    return all_text
# get text from all pages in 2 categories
texts1 = getTextFromUrl(links1)
texts2 = getTextFromUrl(links2)
# concatenate them into a single list
X_raw = texts1 + texts2
# check the number of pages
len(X_raw)
# 330 = 169 + 161
```

# 2. Latent Dirichlet Allocation model

I will first define a stemmer to remove all non-alphabets tokens and then lemmatize the remaining tokens.

```python
def stemmer1(doc):
    # doc = string
    stemmer = nltk.stem.WordNetLemmatizer()
    return [stemmer.lemmatize(w.lower()) for w in nltk.word_tokenize(doc) if w.isalpha()]
```

Now I will first use `sklearn.feature_extraction.text.TfidfVectorizer` to convert from lists of tokens into TF-IDF vectors. After that, I will use  `sklearn.decomposition.LatentDirichletAllocation` to contruct an LDA model with 3 topics.

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.pipeline import Pipeline
# lists of tokens => TF-IDF vectors
# remove stop words in english, and keep only the most frequent 6000 unigrams/bigrams/trigrams
tfidf = TfidfVectorizer(tokenizer=stemmer1,stop_words="english",max_features=6000,min_df=2,ngram_range=(1,3))
# number of topics =  2
lda = LatentDirichletAllocation(n_components=2,random_state=42)
# get a pipeline
pipeline = Pipeline([("tfidf",tfidf),("lda",lda)])
# train the model
pipeline.fit(X_raw)
```

# 3. Keywords for each topic

Now let's look at the top 50 key words for the two latent topics generated by our LDA model.
```python
step1 = pipeline.named_steps['tfidf']
step2 = pipeline.named_steps['lda']

feature_names = step1.get_feature_names()
topics = dict()
# keep the last 50 features
k = 50
for idx, topic in enumerate(step2.components_):
    features = topic.argsort()[:-(k+1): -1]
    tokens = [feature_names[i] for i in features]
    topics[idx] = tokens
for key,value in topics.items():
    print("topic #{}:".format(key))
    print(", ".join(value))
    print()
```
```
topic #0:
network, neural, neural network, neuron, model, function, input, learning, layer, output, data, algorithm, x, weight, j, vector, artificial, training, image, node, n, unit, wa, used, artificial neural, artificial neural network, machine, deep, hidden, memory, ha, activation, value, w, using, t, application, set, recognition, pattern, time, k, signal, task, problem, method, rule, cell, space, linear

topic #1:
music, musical, song, term, wa, composer, note, chord, instrument, band, work, voice, used, piano, piece, bass, section, form, example, sound, tone, italian, century, composition, use, orchestra, musician, opera, melody, rhythm, scale, play, ha, player, performance, major, vocal, german, played, usually, key, genre, classical, word, theme, recording, jazz, beat, time, style
```
Clearly, topic 0 is more related to artificial neural networks, and topic 1 is more related to musical terminology.

# 4. classify wiki pages using LDA

Now let's look at the model performances for some wiki pages clearly related to one of those two categories.
```python
for t in ["The_Beatles","Ordinary_least_squares","Linkin_Park","Generative_adversarial_network","DeepDream","Yann_LeCun","Geoffrey_Hinton","Andrew_Ng","Ludwig_van_Beethoven","Data_cleansing","PyTorch","TensorFlow","Support-vector_machine"]:
    print(t)
    texts_test = getTextFromUrl(["https://en.wikipedia.org/wiki/{}".format(t)])
    print(pipeline.transform(texts_test)[0])
    print()
```
Clearly, our model made all classifications correctly.
```
The_Beatles
[0.04515872 0.95484128]

Ordinary_least_squares
[0.94638694 0.05361306]

Linkin_Park
[0.0742331 0.9257669]

Generative_adversarial_network
[0.92972168 0.07027832]

DeepDream
[0.93317069 0.06682931]

Yann_LeCun
[0.91390139 0.08609861]

Geoffrey_Hinton
[0.90650789 0.09349211]

Andrew_Ng
[0.85251756 0.14748244]

Ludwig_van_Beethoven
[0.04212354 0.95787646]

Data_cleansing
[0.79204267 0.20795733]

PyTorch
[0.93926638 0.06073362]

TensorFlow
[0.9237822 0.0762178]

Support-vector_machine
[0.95479583 0.04520417]
```

Even for words not that clearly belonging to one category, our LDA model can still make reasonable classifications.
```python
for t in ["Opera","Dance","SQL","Microsoft","Apple_Inc.","Apple","Dog","Python_(genus)","Python_(programming_language)"]:
    print(t)
    texts_test = getTextFromUrl(["https://en.wikipedia.org/wiki/{}".format(t)])
    print(pipeline.transform(texts_test)[0])
    print()
```

```
Opera
[0.04462752 0.95537248]

Dance
[0.08123513 0.91876487]

SQL
[0.81414406 0.18585594]

Microsoft
[0.73312333 0.26687667]

Apple_Inc.
[0.56972415 0.43027585]

Apple
[0.41908536 0.58091464]

Dog
[0.38471877 0.61528123]

Python_(genus)
[0.35804103 0.64195897]

Python_(programming_language)
[0.85942891 0.14057109]
```
Apple the fruit and Python the snake belongs to topic 1, while Apple the company and Python the programming language belongs to topic 0.










