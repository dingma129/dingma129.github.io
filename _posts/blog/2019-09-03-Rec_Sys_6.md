---
title: "Recommender Systems - 6"
layout: splash
excerpt: "AutoGrad: SVD using Julia"
categories: [Julia]
tags: [Recommender System, Matrix Factorization, SVD, AutoGrad]
---

<span style="font-weight:bold;font-size:32px">0. Introduction</span>

In the [previous blog](/python/Rec_Sys_4/), I have discussed how to use `PyTorch` to implement `SVD` with baselines. I will use Julia to implement them, and compare the performances of those two implementations. Before doing that, I will first introduce how to do automatic differentiation use `AutoGrad.jl`.

As usual, I will import some packages first.
```julia
import CSV
using LinearAlgebra,BenchmarkTools,Random, Statistics, Arpack, DataFrames, AutoGrad, Plots
# plotting using Plotly-Backend
plotly()
```

<span style="font-weight:bold;font-size:32px">1. Basics of `AutoGrad.jl`</span>

The `AutoGrad` package provides automatic differentiation for Julia. The following figures show the basics of this package.
<center><img src="https://dingma129.github.io/assets/figures/julia/autograd1.png" width="1000" ></center>
<span style="font-weight:bold;font-size:28px">1.1 GD and SGD using `AutoGrad.jl`</span>

I will use an example of linear regression to explain how to do gradient descent and stochastic gradient descent on the loss function using `AutoGrad`. I will record the coefficient and plot them using `Plots` with `Plotly`-backend.

I will first generate some fake datas.
```julia
Random.seed!(42)
X = randn((100,2))
w_real = [3,-2]
b_real = [-4]
error = randn((100))/20
# y = 3x1 - 2x2 - 4 + error
Y_real = X*w_real .+ b_real .+ error;
```

Now I will create a GD and a SGD model.
```julia
Random.seed!(42)
# Model 1: GD
# random initialization
w = Param(randn(2)/5)
b = Param(randn(1)/5)
# record w
w_gd_history = Array{Float64,1}[]
# 20 epochs
for e in 1:20
    x = X[:,:]
    y = Y_real[:]
    # sum of square errors, use @diff
    loss = @diff sum(abs2,x*w.+b-y)
    # gradient descent
    for p in AutoGrad.params(loss)
        # get gradient
        g = grad(loss,p)
        # learning rate = 0.001
        p .-= 0.001*g
    end
    # record w
    push!(w_gd_history,[value(w[1]),value(w[2])])
end
# transform into a 2D Array
w_gd_history = hcat(w_gd_history...)';

# Model 2: SGD
w = Param(randn(2)/5)
b = Param(randn(1)/5)
w_sgd_history = Array{Float64,1}[]
for e in 1:20
    # batch_size = 1
    for i in 1:size(X,1)
        x = X[[i],:]
        y = Y_real[[i]]
        loss = @diff sum(abs2,x*w.+b-y)
        for p in AutoGrad.params(loss)
            g = grad(loss,p)
            p .-= 0.002*g
        end
        push!(w_sgd_history,[value(w[1]),value(w[2]]))
    end
end
w_sgd_history = hcat(w_sgd_history...)';
```

Now we can visualize the learning process of the coefficients using `Plots`.
```julia
scatter([3],[-2],markersize = 8,labels="real",xlims = (0.5,3.5),ylims = (-2.3,0))
# using plot! to plot on the same figure
plot!(w_sgd_history[1:2:end,1],w_sgd_history[1:2:end,2],seriestype=[:line,:scatter],markersize = 2,labels="SGD", linealpha = 0.5,linewidth = 4)
plot!(w_gd_history[:,1],w_gd_history[:,2],seriestype=[:line,:scatter],markersize = 2,labels="GD", linealpha = 0.5,linewidth = 4)
```
<center><embed src="https://dingma129.github.io/assets/active_image/julia/sgd_1.html" width="400" heigh="200" ></center>
<span style="font-weight:bold;font-size:32px">2. SVD for incomplete matrics using `AutoGrad.jl` </span>

The idea of the implementation of `SVD` with baselines here is almost the same as the one in the [previous blog](/python/Rec_Sys_4/). We will need to use a mask `W` to get the loss function summing over only the non-missing entries. I will preprocess as in the last [blog](/julia/Rec_Sys_5/).

```julia
df_raw = CSV.read("/data/mlen/u.data",
header=["userId", "itemId", "rating", "timestamp"],datarow=1);
df_raw_p = unstack(df_raw,:userId,:itemId,:rating)
select!(df_raw_p, Not(:userId));
R_raw = Matrix{Union{Missing, Float64}}(df_raw_p);
# train/test split
Random.seed!(42)
W = rand(1:10,(size(R_raw,1),size(R_raw,2)))
# train/test set with missing
R_train = Matrix([(~ismissing(R_raw[i,j]) && W[i,j] <= 8) ? R_raw[i,j] : missing for i in 1:size(R_raw,1), j in 1:size(R_raw,2)]);
R_test = Matrix([(~ismissing(R_raw[i,j]) && W[i,j] > 8) ? R_raw[i,j] : missing for i in 1:size(R_raw,1), j in 1:size(R_raw,2)]);
# train/test set with missing being filled as 0
R_train_fill = Matrix([ismissing(R_train[i,j]) ? 0 : R_train[i,j] for i in 1:size(R_raw,1), j in 1:size(R_raw,2)]);
R_test_fill = Matrix([ismissing(R_test[i,j]) ? 0 : R_test[i,j] for i in 1:size(R_raw,1), j in 1:size(R_raw,2)]);
# 2 masks for train/test entries
W_train = Matrix([ismissing(R_train[i,j]) ? 0 : 1 for i in 1:size(R_raw,1), j in 1:size(R_raw,2)]);
W_test = Matrix([ismissing(R_test[i,j]) ? 0 : 1 for i in 1:size(R_raw,1), j in 1:size(R_raw,2)]);
```

The `SVD` with baselines can be implemented as following.
```julia
function SVD_sgd(R_train_fill,R_test_fill,W_train,W_test;k=2,lr=0.01,epoch=20, reg=0.0)
    n_train = sum(W_train)
    n_test = sum(W_test)
    mu = mean(skipmissing(W_train))
    # random initialization
    U = Param(randn(size(R_train_fill,1),k)/10)
    V = Param(randn(k,size(R_train_fill,2))/10)
    bu = Param(randn(size(R_train_fill,1),1)/10)
    bv = Param(randn(1,size(R_train_fill,2))/10)
    for e in 1:epoch
        # loss function with regularization
        loss = @diff sum(abs2, (mu .+ bu .+ bv .+ U*V) .* W_train - R_train_fill)/n_train + reg * mean(abs2, U) + reg * mean(abs2, V)+ reg * mean(abs2, bu) + reg * mean(abs2, bv)
        # gradient descent
        for p in AutoGrad.params(loss)
            g = grad(loss,p)
            p .-= lr*g
        end
    end
    R_pred =  mu .+ bu .+ bv .+ U*V
    # print RMSE and MAE
    println("RMSE(train)\t$(√(sum(abs2, R_pred .* W_train - R_train_fill)/n_train))")
    println("RMSE(test)\t$(√(sum(abs2, R_pred .* W_test - R_test_fill)/n_test))")
    println("MAE(train)\t$((sum(abs, R_pred .* W_train - R_train_fill)/n_train))")
    println("MAE(test)\t$((sum(abs, R_pred .* W_test - R_test_fill)/n_test))")
    return (values(U),value(V),value(bu),value(bv),mu)
end
```

Now let's check the performances of this implementation.
```julia
@time _=SVD_sgd(R_train_fill,R_test_fill,W_train,W_test; k=2, epoch=20,lr=130,reg=0.004);
# RMSE(train)	0.9425618099137921
# RMSE(test)	0.9717423938041019
# MAE(train)	0.7442596213560989
# MAE(test) 	0.7665142579366203
# 11.676871 seconds (112.41 M allocations: 3.691 GiB, 50.94% gc time)
```
We can see that its RMSE and MAE is almost the same as `Python`'s '`Surprise` package and the `PyTorch` implementation in my previous [blog](/python/Rec_Sys_4/). However, its runtime is much longer. 

The main step that uses over 95% of time is the following line
```julia
@diff sum(abs2, (mu .+ bu .+ bv .+ U*V) .* W_train - R_train_fill)/n_train
```
Although it is possible to compute the square loss using `sum(skipmissing(...))`, it does not integrate with `@diff` well. This above line computes the sum over a dense matrix instead of a sparse one. That's the main reason why our `PyTorch` implementation is much faster.