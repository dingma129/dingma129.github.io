---
title: Scala (part 3)
subtitle: Logistic Regression using spark.ml
author: Ding Ma
layout: post
categories: [blog]
---

<span style="font-weight:bold;font-size:32px">0. Introduction</span>

In this blog, I will introduce how to use `spark.ml` to train basic machine learning models. After loading the dataset from raw csv files into a Spark DataFrame, I will preprocess the DataFrame and train a Logistic Regression model. At the end, I will also evaluate the model using accuracy.

---
<span style="font-weight:bold;font-size:32px">1. Dataset</span>

The dataset I use for this blog is the Record Linkage Comparison Patterns Data Set from [<span styple="color:blue">here</span>](https://archive.ics.uci.edu/ml/datasets/record+linkage+comparison+patterns). It contains element-wise comparison of records with personal data from a record linkage setting. The task is to decide from a comparison pattern whether the underlying records belong to one person. There are 5,749,132 record pairs in total, of which 20,931 are matches. 

It contains the following features:
1. id_1: internal identifier of first record. 
2. id_2: internal identifier of second record. 
3. cmp_fname_c1: agreement of first name, first component 
4. cmp_fname_c2: agreement of first name, second component 
5. cmp_lname_c1: agreement of family name, first component 
6. cmp_lname_c2: agreement of family name, second component 
7. cmp_sex: agreement sex 
8. cmp_bd: agreement of date of birth, day component 
9. cmp_bm: agreement of date of birth, month component 
10. cmp_by: agreement of date of birth, year component 
11. cmp_plz: agreement of postal code 
12. is_match: matching status (TRUE for matches, FALSE for non-matches) 

The agreement of name components (3-6 above) is measured as a real number in the interval [0,1], where 0 denotes maximal disagreement and 1 equality of the underlying values. For the other comparisons (7-11 above), only the values 0 (not equal) and 1 (equal) are used. 

---
<span style="font-weight:bold;font-size:32px">2. Load data into a Spark DataFrame</span>
```scala
val prev = spark.read.csv("data/linkage/*.csv")
// we see that the 1st row should be the header
prev.show(5,false)
```
<center><img src="https://dingma129.github.io/assets/figures/scala/scala_2_dataset_preview1.png" width="1000" ></center>

We can see that the first row should be the header, and the null values are denoted by "?" in this dataset.
```scala
val parsed = spark.read
.option("header","true")  // there is a header in this dataset
.option("nullValue","?")  // null value is given by "?"
.option("inferSchema","true")  // let Spark infer the data schema
.csv("data/linkage/*.csv")  // filepath to dataset
parsed.show(5,false)
```
<center><img src="https://dingma129.github.io/assets/figures/scala/scala_2_dataset_preview2.png" width="1000" ></center>

Now the dataset has been successfully loaded by Spark, and we can double check the schema of the DataFrame.

```scala
>>> parsed.printSchema()
root
 |-- id_1: integer (nullable = true)
 |-- id_2: integer (nullable = true)
 |-- cmp_fname_c1: double (nullable = true)
 |-- cmp_fname_c2: double (nullable = true)
 |-- cmp_lname_c1: double (nullable = true)
 |-- cmp_lname_c2: double (nullable = true)
 |-- cmp_sex: integer (nullable = true)
 |-- cmp_bd: integer (nullable = true)
 |-- cmp_bm: integer (nullable = true)
 |-- cmp_by: integer (nullable = true)
 |-- cmp_plz: integer (nullable = true)
 |-- is_match: boolean (nullable = true)  // being successfully inferred as boolean
```
---
<span style="font-weight:bold;font-size:32px">3. Basic Analyzing with DataFrame API</span>

First, let's check the number of records for this dataset. 
```scala
// cache the parsed DataFrame
>>> parsed.cache()
[id_1: int, id_2: int ... 10 more fields]
>>> parsed.count()
5749132
```
Next, let's check the number of postive and negative classes. Without `.show()`, all other operations are transformations and no actual computation is happening.

```scala
>>> import org.apache.spark.sql.functions._
// method 1: using .count()
>>> parsed.groupBy("is_match").count().show()
+--------+---------------+
|is_match|count(is_match)|
+--------+---------------+
|    true|          20931|
|   false|        5728201|
+--------+---------------+

// method 2: using .agg()
// count() and col() are from org.apache.spark.sql.functions._
>>> parsed.groupBy("is_match").agg(count("is_match")).show()
>>> parsed.groupBy("is_match").agg(count(col("is_match"))).show()
>>> parsed.groupBy("is_match").agg(count($"is_match")).show()

// method 3: using SQL queries that returns the result as a DataFrame
>>> parsed.createOrReplaceTempView("parsed")
>>> spark.sql("SELECT is_match, COUNT(*) AS count FROM parsed GROUP BY is_match").show()
```
Using `.describe` method, you can get a fast summary statistics of a DataFrame. If you want to filter a DataFrame, you can use either `.where` or `.filter` as follows.

```scala
// .where is exactly the same as .filter
// you can either use SQL expression "is_match = true"
val matches = parsed.where("is_match = true") 
// or column object expression: col("is_match") === false
val misses = parsed.filter(col("is_match") === false) 
```
---
<span style="font-weight:bold;font-size:32px">4. Preprocessing and Train/Test Split</span>

We will use all columns with enough values and fill in all null values using 0. We also need to rename the target column `is_match` as `label` for later use.
```scala
import org.apache.spark.sql.types._
val df = parsed.select("cmp_fname_c1","cmp_lname_c1","cmp_sex","cmp_bd","cmp_bm","cmp_by","cmp_plz","is_match")
.na.fill(0).selectExpr("*", "CAST(is_match AS double) AS label")
```

We can make train/test split using `.randomSplit` method.
```scala
>>> val Array(df_train,df_val,df_test) = df.randomSplit(weights=Array(0.8,0.1,0.1),seed=42)
>>> println((df_train.count(),df_val.count(),df_test.count()))
(4599691,574914,574527)
```
---
<span style="font-weight:bold;font-size:32px">5. Classification Using `spark.ml`</span>

I will use logistic regression model for this classification problem. `VectorAssembler` is a transformer that combines a given list of columns into a single vector column in order to train ML models like logistic regression and decision trees.

```scala
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.Pipeline
// VectorAssembler: combines 7 columns into one single column "features"
val va1 = new VectorAssembler()
.setInputCols(Array("cmp_fname_c1","cmp_lname_c1","cmp_sex","cmp_bd","cmp_bm","cmp_by","cmp_plz"))
.setOutputCol("features")
// create a Logistic Regression model and set fetures/label column
val lr1 = new LogisticRegression()
.setFeaturesCol("features")
.setLabelCol("label")
// create a Pipeline
val pipeline1 = new Pipeline()
.setStages(Array(va1,lr1))
// train the model
val model1 = pipeline1.fit(df_train)
```
---
<span style="font-weight:bold;font-size:32px">6. Evaluating Model</span>

We can use predefined metrics to evaluate the model as follows.

```scala
>>> import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
>>> val mce = new MulticlassClassificationEvaluator()
>>> .setMetricName("accuracy")

>>> println(f"accuracy(train):\t${mce.evaluate(model1.transform(df_train))}%.8f")
>>> println(f"accuracy(val):\t\t${mce.evaluate(model1.transform(df_val))}%.8f")
>>> println(f"accuracy(tes):\t\t${mce.evaluate(model1.transform(df_test))}%.8f")
accuracy(train):	0.99998391
accuracy(val):		0.99997565
accuracy(test):		0.99998608
```