---
title: Bokeh (part 1)
author: Ding Ma
layout: post
categories: [blog]
---

<span style="font-weight:bold;font-size:36px">0. Introduction</span>

Variational autoencoders (VAEs) were defined in 2013 by [<span style="color: blue">Kingma et al.</span>](https://arxiv.org/abs/1312.6114) and [<span style="color: blue">Rezende et al.</span>](https://arxiv.org/abs/1401.4082). VAEs are a deep learning technique for learning latent representation.

A classical autoencoder takes an image, maps it to a latent vector space via an encoder, and then decodes it back via a decoder. But such an autoencoder does not lead to a nice latent space. If we pick a random point from the latent space, its decoded output from the decoder is usually not a nice image.

However, a VAE will learn a continuous and highly structured latent space. Instead of compressing its the input into a fixed representation in the latent space, VAE turns it into the mean and variance of a multivariate normal distribution. 

<center><img src="https://dingma129.github.io/assets/figures/blog/VAE_1.png" width="1000" ></center>
If we pick choose any random point following such a distribution, its decoded output from the decoder is usually a nice image. The following is a plot showing the decoded images of the Keras VAE model trained below of a 2D grid. Every decoded image looks very similar to a real MNIST sample.
<center><img src="https://dingma129.github.io/assets/figures/blog/VAE_grid_plot.png" width="600" ></center>
---
<span style="font-weight:bold;font-size:36px">1. Implementation in Keras</span>

The following is the structure of VAE implementation in Keras. It uses several convolutional layers. The model contains a encoder part and a decoder part. The decoder part is used to decode a latent vector.

<center><img src="https://dingma129.github.io/assets/figures/blog/VAE_CNN.png" width="1000" ></center>
```python
import keras.backend as K
from keras.layers import Input,Dense,Flatten,Reshape, Conv2D, Conv2DTranspose, Lambda
from keras.models import Model
# 2 dimensional latent space
latent_dim = 2
# Encoder: encoder_input => (z_mean,z_log_var)
encoder_input = Input(shape=(28,28))
x = Reshape((28,28,1))(encoder_input)
x = Conv2D(16,3,padding="same",activation="relu")(x)
x = Conv2D(32,3,padding="same",activation="relu",strides=(2,2))(x)
x = Conv2D(32,3,padding="same",activation="relu")(x)
x = Flatten()(x)
x = Dense(32,activation="relu")(x)
z_mean = Dense(latent_dim)(x)
z_log_var = Dense(latent_dim)(x)
# sampling function
def sampling(args):
    z_mean, z_log_var = args
    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=1.)
    return z_mean + K.exp(z_log_var) * epsilon
# take a random sample z
z = Lambda(sampling)([z_mean, z_log_var])
# Decoder: decoder_input => decoder_output
decoder_input = Input(K.int_shape(z)[1:])
x = Dense(14*14*32,activation="relu")(decoder_input) # upsample
x = Reshape((14,14,32))(x)
x = Conv2DTranspose(32, 3, padding='same',activation='relu',strides=(2,2))(x)
x = Conv2D(1, 3, padding='same',activation='sigmoid')(x)
decoder_output = Reshape((28,28))(x)
decoder = Model(inputs=decoder_input,outputs=decoder_output)
# z decoded
z_decoded = decoder(z)
# model=Encoder+Decoder: 
# encoder_input => (z_mean,z_log_var) => z => z_decoded
model = Model(inputs=encoder_input,outputs=z_decoded)
```
---
<span style="font-weight:bold;font-size:36px">2. Model training</span>

In order to train the model, we first need to preprocess the MNIST dataset. The MNIST dataset used here is from the keras pacakge.
```python
from keras.datasets import mnist
# we won't use y in the training
(X_train, y_train), (X_test, y_test) = mnist.load_data()
# rescale X into 0-1 range
X_train=X_train/255.0
X_test=X_test/255.0
```
Next, we want to define a new loss function according to the original paper. The loss function contains two parts:
1. a `binary_crossentropy` loss measuring the difference between model input and model output; (decoding quality)
2. a KL loss, which is the sum of all Kullbackâ€“Leibler divergence; this loss encourages the encoder to distribute all encodings evenly around the center of the latent space. (encoding quality)
Training the model on this loss function will make a balance between the decoding quality and encoding quality.
```python
def my_loss(y_true,y_pred):
    loss_1 = keras.metrics.binary_crossentropy(K.flatten(encoder_input),K.flatten(z_decoded))
    # the coefficient -5e-4 to make a balance between two parts
    loss_2 = -5e-4 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)   
    return K.mean(loss_1 + loss_2)
```
Now we are ready to train the model.
```python
model.compile(optimizer='rmsprop',loss=my_loss)
# to save time, just run 10 epochs
model.fit(X_train,X_train,epochs=10,batch_size=128)
```
<center><img src="https://dingma129.github.io/assets/figures/blog/VAE_training.png" width="600" ></center>
We can see that the model is still improving. Here I don't use a validation set, because overfitting is not a problem for this case. We want the model fitting as harde as possible on this MNIST dataset.

---
<span style="font-weight:bold;font-size:36px">3.Decoding visualization</span>

The following is an interactive plot of the Decoder. 
<center><embed src="https://dingma129.github.io/assets/active_image/bokeh/VAE.html" width="500" height="250"></center>