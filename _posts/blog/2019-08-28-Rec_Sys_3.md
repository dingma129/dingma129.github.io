---
title: "Recommender Systems - 3"
layout: splash
excerpt: "Content-Based Recommender Systems"
categories: [Python]
tags: [Recommender System]
---

<span style="font-weight:bold;font-size:32px">0. Introduction</span>

Content-based systems are designed to exploit scenarios in which items can be described with descriptive sets of attributes.

Unlike collaborative systems, which explicitly leverage the ratings of other users in addition to that of the target user, content-based systems largely focus on the target userâ€™s own ratings and the attributes of the items liked by the user. Therefore, the other users have little, if any, role to play in content-based systems.

Content-based systems are mainly dependent on the following two sources of data:
1. a description of various items in terms of content-centric attributes
2. a user profile, which is generated from user feedback (might be explicit or implicit) about various items

Content-based systems are largely used in scenarios in which a significant amount of attribute information is available at hand.

When an item is new, it is not possible to obtain the ratings of other users for that item. Content-based methods enable recommendations in such settings because they can extract the attributes from the new item, and use them to make predictions. On the other hand, the cold-start problem for new users cannot be addressed with content-based recommender systems.

By not using the ratings of other users, one reduces the diversity and novelty of the recommended items.

The content-based systems include the following 3 main components:
1. (offline) preprocessing and feature extraction: convert features into a keyword-based vector representation
2. (offline) content-based learning of user profiles: a user-specific model is constructed to predict user interests in items, based on their past history of either buying or rating items
3. (online) filtering and recommendation

In the following sections, I will discuss each of these 3 phases in detail.

---
<span style="font-weight:bold;font-size:32px">1. Preprocessing and Feature Extraction</span>

<span style="font-weight:bold;font-size:28px">1.1 Feature Extraction</span>

In the feature extraction phase, the descriptions of various items are extracted. The most common approach is to extract keywords from the descriptions. In some cases, these descriptions can be converted into a bag of keywords. In other cases, one might work directly with a multidimensional (structured) representation. 

<span style="font-weight:bold;font-size:28px">1.2 Feature Representation and Cleaning</span>

By removing stop-words, stemming, and extracting phrases, the keywords are converted into a vector-space representation. In the vector-space representation, documents are represented as bags of words, together with their frequencies. Because commonly occurring words are often statistically less discriminative, the most popular model is called the TF-IDF model, where the term frequency (TF) are discounted by multiplying the inverse document frequency (IDF).

<span style="font-weight:bold;font-size:28px">1.3 Supervised Feature Selection and Weighting</span>
It is suggested that the number of extracted words should be somewhere between 50 and 300. The basic idea is that the noisy words often result in overfitting and should therefore be
removed a priori.

There are two distinct aspects to incorporating the feature informativeness in the document
representation.

* feature selection: removing un-important words, e.g. removing stop-words
* feature weighting: giving larger coefficient for more important words, e.g. using TF-IDF

But removing stop-words or using TF-IDF are unsupervised ways of feature selection and weighting. 

I will now introduce some supervised methods for feature selection, which take the user ratings into account for evaluating feature informativeness. 

The following figure shows three main statistics used for supervised feature selection.
<center><img src="https://dingma129.github.io/assets/figures/rec/rec_3_feat_select1.png" width="800" ></center>
Here is an example of how to use those 3 methods to select important features.
<center><img src="https://dingma129.github.io/assets/figures/rec/rec_3_feat_select2.png" width="800" ></center>
We can use `sklearn.feature_selection.chi2` to compute the chi-square statistics.
```python
import numpy as np
from sklearn.feature_selection import chi2
X = np.array([[1, 0, 1, 0, 0, 0],[1, 1, 1, 0, 1, 0],[1, 1, 0, 0, 0, 0],
              [0, 0, 0, 1, 1, 0],[0, 1, 0, 1, 1, 1],[0, 0, 0, 0, 1, 1]])
y = np.array([0,0,0,1,1,1])
print("chi_square:\n{}".format(chi2(X,y)[0]))
print("p-value:\n{}".format(chi2(X,y)[1]))
# chi_square:
# [3.         0.33333333 2.         2.         1.         2.        ]
# p-value:
# [0.08326452 0.56370286 0.15729921 0.15729921 0.31731051 0.15729921]
```
---
<span style="font-weight:bold;font-size:32px">2. Learning User Profiles and Filtering</span>

The training data contain the ratings assigned by the active user to these documents. These documents are used to construct a training model. Note that the labels assigned by other users (than the active user) are not used in the training process. Therefore, the training models are specific to particular users, and they cannot be used for arbitrarily chosen users. The training model for a specific user represents the user profile.

I will continue the previous example of feature selection. This time, I will train a logistic regression model and a KNN model to predict the ratings of two new items. Both a label (0 or 1) and the class probabilities can be obtained using those two models.
```python
# select 0,2,3,5th feature as train set
X_train = X[:,[0,2,3,5]]
y_train = y
# test set, also keep 0,2,3,5th feature only
X_test = np.array([[0,0,0,1,0,1],[0,1,1,0,0,0]])[:,[0,2,3,5]]

# Logistic Regression
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(solver="lbfgs")
lr.fit(X_train,y_train)
# prediction
print("lr prediction:\n{}".format(lr.predict(X_test)))
print("lr prob_prediction:\n{}".format(lr.predict_proba(X_test)))
# lr prediction:
# [1 0]
# lr prob_prediction:
# [[0.21431273 0.78568727]
# [0.56459651 0.43540349]]

# KNN (k=3)
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3,metric="cosine")
knn.fit(X_train,y_train)
# prediction
print("knn prediction:\n{}".format(knn.predict(X_test)))
print("knn prob_prediction:\n{}".format(knn.predict_proba(X_test)))
# knn prediction:
# [1 0]
# knn prob_prediction:
# [[0. 1.]
# [1. 0.]]
```
<center><img src="https://dingma129.github.io/assets/figures/rec/rec_3_feat_select3.png" width="1000" ></center>
---
<span style="font-weight:bold;font-size:32px">3. Content-Based vs Collaborative</span>

Content-based methods have several advantages and disadvantages as compared to collaborative methods.
* Advantages of Content-Based Methods:
	1. Collaborative systems have cold-start problems both for new users and new items,
whereas content-based systems have cold-start problems only for new users.
	2. Content-based methods provide explanations in terms of the features of items. This is often not possible with collaborative recommendations.
	3. Content-based methods can generally be used with off-the-shelf text classifiers.
* Disadvantages of Content-Based Methods:
	1. Overspecialization and lack of serendipity are the two most significant challenges of content-based recommender systems.
	2. Content-based systems do not help in resolving cold-start problems for new users.

Content-based systems often complement collaborative systems quite well because of their ability to leverage content-based knowledge in the recommendation process. This complementary behavior is often leveraged in hybrid recommender systems, in which the goal is to combine the best of both worlds to create an even more robust recommender system.