---
title: "Probabilistic Modeling - 2"
layout: splash
excerpt: "Bayesian Linear Regression using Variational Inference"
categories: [Python]
tags: [Edward, Bayesian Inference, Variational Inference, TensorBoard, Regression]
---

# 0. Introduction

In this blog, I will use MAP, Laplace, KLpq and KLqp variational inference method to infer a Bayesian linear regression model of a small dataset of only 10 points. I will compare the performances of them and create some visualizations of the posterior. At the end, I will also provide several TensorBoard plots showing the performances.

```python
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import edward as ed
%matplotlib inline
from edward.models import Normal, Uniform, PointMass
# set a random seed
ed.set_seed(42)
```


# 1. Data

In this blog, I will generate a linear regression data with only 10 data points.

```python
# helper function to generate datas
def build_toy_dataset(N, w, b):
    D = len(w)
    x = np.random.normal(0.0, 2.0, size=(N, D))
    y = np.dot(x, w) + b + np.random.normal(0.0, 0.5, size=N)
    return x, y

N = 10  # number of data points
D = 1  # number of features
# generate w_true and b_true
w_true = np.random.randn(D) * 2 + 6
b_true = np.random.randn(1) * 5 + 10
# build train/test data
X_train, y_train = build_toy_dataset(N, w_true,b_true)
X_test, y_test = build_toy_dataset(N, w_true,b_true)
plt.scatter(X_train,y_train,alpha=0.6,label="train");
plt.scatter(X_test,y_test,alpha=0.6,label="test");
plt.legend();
```
<center><img src="/assets/figures/edward/2_data.png" width="500" ></center>
# 2. Four forms of Variational Inferences

## 2.1 MAP

For MAP, I will use `edward.models.PointMass` for the posterior `qw` and `qb`. 

```python
# set the prior to be standard normal distribution
with tf.name_scope('model'):
    X = tf.placeholder(tf.float32, [N, D])
    w = Normal(loc=tf.zeros([D],name="w/loc"), scale=tf.ones([D],name="w/scale"))
    b = Normal(loc=tf.zeros([1],name="b/loc"), scale=tf.ones([1],name="b/scale"))
    y = Normal(loc=ed.dot(X, w) + b, scale=tf.ones(N,name="y/scale"))
# posterior
with tf.variable_scope('posterior',reuse=False):
    qw_loc = tf.get_variable("qw/loc", [D])    
    qb_loc = tf.get_variable("qb/loc", [1])
with tf.name_scope('posterior'):    
    qw = PointMass(params=qw_loc)
    qb = PointMass(params=qb_loc)
```
Now we can perform the MAP inference using `ed.MAP`. Here we set a `logdir` so that we can use TensorBoard to visualize the inference later.
```python
inference1 = ed.MAP({w: qw, b: qb}, data={X: X_train, y: y_train})
inference1.run(n_iter=500, logdir='log/MAP')
```
Now we can use the infered posterior distribution `qw` and `qb` to construct the posterior prediction of `y`.
```python
y_post = Normal(loc=ed.dot(X, qw) + qb, scale=tf.ones(N))
```
We can check its test error.
```python
print("Mean squared error on test data:")
print(ed.evaluate('mean_squared_error', data={X: X_test, y_post: y_test}))
print("Mean absolute error on test data:")
print(ed.evaluate('mean_absolute_error', data={X: X_test, y_post: y_test}))
```
```
Mean squared error on test data:
0.9095848
Mean absolute error on test data:
0.75373983
```
We can also compare `qw`,`qb` with `w_true` and `b_true`.
```python
print("w_true = {}".format(w_true[0]))
print("prior:")
print("\tmean = {}\n\tstd = {}".format(w.loc.eval()[0],w.scale.eval()[0]))
print("posterior:")
print("\tMAP = {}\n".format(qw.params.eval()[0]))
print("b_true = {}".format(b_true[0]))
print("prior:")
print("\tmean = {}\n\tstd = {}".format(b.loc.eval()[0],b.scale.eval()[0]))
print("posterior:")
print("\tMAP = {}\n".format(qb.params.eval()[0]))
```
```
w_true = 6.993428306022466
prior:
	mean = 0.0
	std = 1.0
posterior:
	MAP = 6.815334320068359

b_true = 9.308678494144077
prior:
	mean = 0.0
	std = 1.0
posterior:
	MAP = 8.305010795593262
```
Using the following helper function, we can sample some `w` and `b` from prior or posterior distribution and plot the corresponding regression lines.
```python
def visualise(X_data, y_data, w, b, n_samples=10):
    # generate samples from w and b
    w_samples = w.sample(n_samples)[:, 0].eval()
    b_samples = b.sample(n_samples).eval()
    plt.scatter(X_data[:, 0], y_data)
    inputs = np.linspace(-8, 8, num=400)
    for ns in range(n_samples):
        output = inputs * w_samples[ns] + b_samples[ns]
        plt.plot(inputs, output,color="red",alpha=0.1)
# regression lines from prior
visualise(X_test,y_test,w,b,50);
# regression lines from posterior
visualise(X_test,y_test,qw,qb,50);
```

<p align="middle">
<img src="/assets/figures/edward/2_map_prior.png" width="500" >
<img src="/assets/figures/edward/2_map_posterior.png" width="500" >
</p>

From the above figure, we can see that after 500 iterations of MAP inference, the regression lines are already very good.

## 2.2 Laplace

For the Laplace inference, the posteriors are not Point Mass distributions anymore. We will use Normal distribution in this case.

```python
# prior
with tf.name_scope('model'):
    X = tf.placeholder(tf.float32, [N, D])
    w = Normal(loc=tf.zeros([D],name="w/loc"), scale=tf.ones([D],name="w/scale"))
    b = Normal(loc=tf.zeros([1],name="b/loc"), scale=tf.ones([1],name="b/scale"))
    y = Normal(loc=ed.dot(X, w) + b, scale=tf.ones(N,name="y/scale"))
# posterior
with tf.variable_scope('posterior',reuse=False):
    qw_loc = tf.get_variable("qw/loc", [D])
    qw_scale = tf.get_variable("qw/scale", [D])
    qb_loc = tf.get_variable("qb/loc", [1])
    qb_scale = tf.get_variable("qb/scale", [1])
with tf.name_scope('posterior'):    
    qw = Normal(loc=qw_loc,scale=tf.nn.softplus(qw_scale))
    qb = Normal(loc=qb_loc,scale=tf.nn.softplus(qb_scale))
```
Running the Laplace inference using `ed.Laplace`, we get the following test errors and samples of regression lines.
```python
inference2 = ed.Laplace({w: qw, b: qb}, data={X: X_train, y: y_train})
inference2.run(n_iter=500, logdir='log/Laplace')
```
```
Mean squared error on test data:
0.8377434
Mean absolute error on test data:
0.7955419

w_true = 6.993428306022466
prior:
	mean = 0.0
	std = 1.0
posterior:
	mean = 6.815545558929443
	std = 0.710837185382843
b_true = 9.308678494144077
prior:
	mean = 0.0
	std = 1.0
posterior:
	mean = 8.304498672485352
	std = 0.7396344542503357
```

<p align="middle">
<img src="/assets/figures/edward/2_laplace_prior.png" width="500" >
<img src="/assets/figures/edward/2_laplace_posterior.png" width="500" >
</p>
This time we get normal distributions `qw` and `qb` with means `w_MAP` and `b_MAP`.

## 2.3 KLpq

We use the same model for KLpq inference as in the Laplace case. Running the KLpq inference using `ed.KLpq`, we get the following test errors and samples of regression lines.
```python
inference3 = ed.KLpq({w: qw, b: qb}, data={X: X_train, y: y_train})
inference3.run(n_samples = 5, n_iter=500, logdir='log/KLpq')
```
```
Mean squared error on test data:
0.77384746
Mean absolute error on test data:
0.68606544

w_true = 6.993428306022466
prior:
	mean = 0.0
	std = 1.0
posterior:
	mean = 6.76392126083374
	std = 0.2481892853975296
b_true = 9.308678494144077
prior:
	mean = 0.0
	std = 1.0
posterior:
	mean = 8.379722595214844
	std = 0.44464728236198425
```
<p align="middle">
<img src="/assets/figures/edward/2_pq_prior.png" width="500" >
<img src="/assets/figures/edward/2_pq_posterior.png" width="500" >
</p>

## 2.4 KLqp

We use the same model for KLqp inference as in the Laplace case. Running the KLqp inference using `ed.KLqp`, we get the following test errors and samples of regression lines.
```python
inference4 = ed.KLqp({w: qw, b: qb}, data={X: X_train, y: y_train})
inference4.run(n_samples = 5, n_iter=500, logdir='log/KLqp')
```
```
Mean squared error on test data:
0.89542735
Mean absolute error on test data:
0.7729857

w_true = 6.993428306022466
prior:
	mean = 0.0
	std = 1.0
posterior:
	mean = 6.8216633796691895
	std = 0.18559934198856354
b_true = 9.308678494144077
prior:
	mean = 0.0
	std = 1.0
posterior:
	mean = 8.29655647277832
	std = 0.30638447403907776
```
<p align="middle">
<img src="/assets/figures/edward/2_qp_prior.png" width="500" >
<img src="/assets/figures/edward/2_qp_posterior.png" width="500" >
</p>

# 3. Visualization and Comparison

The following is a table summarizing the above 4 variational inference models.

<center><img src="/assets/figures/edward/2_table.png" width="800" ></center>

KLqp has the smallest standard deviations for both parameters, and KLpq has the smallest MSE on test data.

We can also use TensorBoard to visualize the inference procedure.
<p align="middle">
<img src="/assets/figures/edward/2_legend.png" width="400" >
<img src="/assets/figures/edward/2_tb1.png" width="600" >
</p>
From the above figure, we can see that for runtime speed MAP > Laplace > KLqp > KLpq. Also MAP, Laplace and KLqp are much smoother and converge earlier than KLpq.
<center><img src="/assets/figures/edward/2_tb2.png" width="1000" ></center>
This figure of the histograms of gradients also shows that MAP, Laplace and KLqp are much smoother and converge earlier than KLpq.
