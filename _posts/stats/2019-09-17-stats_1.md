---
title: "Statistics - 1"
layout: splash
excerpt: "t-test and chi-squared test"
categories: [Python]
tags: []
---

# 0. Introduction

In this blog, I will introduce t-test and chi-squared test. Both of them are very useful when we are doing A/B test. Let's first import some packages.

```python
import numpy as np
import pandas as pd
import scipy as sp
import matplotlib.pyplot as plt
plt.style.use('ggplot')
```

I will use the South African Heart Disease dataset from [here](http://web.stanford.edu/~hastie/ElemStatLearn/datasets/sachs.data). 

```python
df = pd.read_csv("data/SAheart.data",index_col=0)
df["famhist"] = df["famhist"].apply(lambda x: 1.0 if x=="Present" else 0.0)
df.head()
```
After encoding the `famhist` column, the dataset looks like the following.
<center><img src="/assets/figures/stats/1_data.png" width="600" ></center>


# 1. t-test of sample means

The **t-test** can be used to determine if the means of two sets of data are significantly different from each other.

In this section, I will use t-test to determine whether there is a significant different between the average age between two groups of people with `chd=0` and `chd=1`. Let's first look at the histogram of those two groups.
```python
age0 = np.array(df[df["chd"]==0].age)
age1 = np.array(df[df["chd"]==1].age)
plt.hist(age0,bins=50,alpha=0.5)
plt.hist(age1,bins=50,alpha=0.5);
```
<center><img src="/assets/figures/stats/1_hist.png" width="600" ></center>


## 1.1 equal variance

When it can be assumed that the two distributions have the same variance. The t-statistic can be calculated as follows:

```python
# 1) calculate by formula
def tEqualVar(a,b):
    n0 = len(a)
    n1 = len(b)
    mu0 = np.mean(a)
    mu1 = np.mean(b)
    s0 = np.std(a,ddof=1)
    s1 = np.std(b,ddof=1)
    # pooled standard deviation of the two samples
    s = np.sqrt(((n0-1)*s0**2+(n1-1)*s1**2)/(n0+n1-2))
    # t-statistic
    t = (mu0-mu1)/(s*np.sqrt(1/n0+1/n1))
    # returns t-stat and p-value (t-distribution of total degree of freedom = n0+n1-2)
    return t,sp.stats.t.cdf(t, n0+n1-2)*2
    
print(tEqualVar(age0,age1))
# (-8.621496303728742, 1.0741818171806074e-16)

# 2) calculate using scipy
print(sp.stats.ttest_ind(age0,age1,equal_var=True))
# Ttest_indResult(statistic=-8.621496303728742, pvalue=1.0741818171806074e-16)
```

## 1.2 unequal variance

When it cannot be assumed that the two distributions have the same variance. The t-statistic can be calculated as follows:

```python
# 1) calculate by formula
def tUnEqualVar(a,b):
    n0 = len(a)
    n1 = len(b)
    mu0 = np.mean(a)
    mu1 = np.mean(b)
    s0 = np.std(a,ddof=1)
    s1 = np.std(b,ddof=1)
    # the unbiased estimator of the standard deviation 
    s = np.sqrt(s0**2/n0+s1**2/n1)
    # t-statistic
    t = (mu0-mu1)/(s)
    # degrees of freedom
    f = (s0**2/n0+s1**2/n1)**2/((s0**2/n0)**2/(n0-1)+(s1**2/n1)**2/(n1-1))
    # returns t-stat and p-value (t-distribution of total degree of freedom = f)
    return t,sp.stats.t.cdf(t, f)*2
    
print(tUnEqualVar(age0,age1))
# (-9.525610527883082, 1.316985998595387e-19)

# 2) calculate using scipy
print(sp.stats.ttest_ind(age0,age1,equal_var=False))
# Ttest_indResult(statistic=-9.525610527883082, pvalue=1.316985998595387e-19)
```
Both methods gave us the same t-statistic and p-value after using the correct degrees of freedom. Since `p-value = 1.316985998595387e-19 < 0.05`, there is a significant difference between the means of those two groups.


# 2. chi-squared test

The **chi-squared test** is used to determine whether there is a significant difference between the expected frequencies and the observed frequencies in one or more categories.

In the following example, we want to test whether there is a significant difference between the expected frequencies and the observed frequencies of `famhist` for two groups of people with `chd=0` and `chd=1`.

```python
cm = pd.crosstab(df['famhist'], df['chd'])
print(cm)
# chd        0   1
# famhist         
# 0.0      206  64
# 1.0       96  96
```

The above table gives us the observed frequencies. We can compute the expected frequencies based on it.

```python
O = np.array([[206,64],[96,96]])
print(O)
# [[206  64]
#  [ 96  96]]
 
# compute expected frequencies E
col_sum = np.sum(O,axis=0,dtype=np.float64)
row_sum = np.sum(O,axis=1,dtype=np.float64)
total = np.sum(O,dtype=np.float64)
E = np.zeros_like(O,dtype=np.float64)
for i in range(O.shape[0]):
    for j in range(O.shape[1]):
        E[i,j] = row_sum[i]*col_sum[j]/total
print(E)
# [[176.49350649  93.50649351]
#  [125.50649351  66.49350649]]
```

After getting `O` and `E`, the chi-squared statistic and the p-value can be obtained easily.
```python
# 1) calculate by formula
D = np.sum(np.square(E-O)/E)
# degrees of freedom is (n_rows-1)*(n_cols-1)=(2-1)*(2-1)=1
print(D,sp.stats.chi2.sf(D,df=1))
# 34.27434878587198 4.786492073300045e-09

# 2) calculate using scipy
# d.f. = k-1-ddof 
# want to have d.f.=1 => 4-1-ddof=1 => ddof = 2
print(sp.stats.chisquare(O,E,ddof=2))
# Power_divergenceResult(statistic=34.27434878587198, pvalue=4.786492073300045e-09)
```
Again both methods gave us the same chi-squared statistic and p-value after using the correct degrees of freedom. Since `p-value = 4.786492073300045e-09 < 0.05`, there is a significant difference between the expected frequencies and the observed frequencies of `famhist` for two groups of people with `chd=0` and `chd=1`.








