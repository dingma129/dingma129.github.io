---
title: XGBoost Feature Extraction
author: Ding Ma
layout: post
categories: [blog]
---
<span style="font-weight:bold;font-size:36px">0. introduction</span>

In this post, I will introduce the idea of using XGBoost to extract features for classification problem. This method uses XGBoost model to extract features, and it is useful when we have a very large training sample. For simplicity, I will just choose logistic regression model for classification.
<center><img src="https://dingma129.github.io/assets/figures/ESL/xgb_lr.png" width="1000"></center>
* On the left hand side, it is the standard procedure to train a classification model.
	1. split (X,y) into (X_train,y_train) and (X_test,y_test)
	2. train a Logistic Regression model using (X_train,y_train)
	3. test the model using (X_test,y_test)
	4. if we want to tune hyperparameters of the model, we can use validation set of cross-validation
* On the right hand side, it is the method that I want to introduce in this blog.
	1. split (X,y) into 3 parts (X_xgb,y_xgb), (X_lr,y_lr) and (X_test,y_test)
	
	2. train a XGBoost using (X_xgb,y_xgb)
	
	3. the prediction of this XGBoost model will be the leaf nodes of each tree in the ensemble
	
	4. use the prediction of the XGBoost model and y_xgb to train an One-Hot Encoder
	
	5. use the trained XGBoost and One-Hot Encoder to transform x_lr, then train a Logistic Regression model using this transformed input and y_lr
	
	6. our finalized model will be the pipeline of XGBoost, One-Hot Encoder and Logistic Regression
	
---
<span style="font-weight:bold;font-size:36px">1. one toy example</span>

```python
from sklearn.datasets import make_classification
from xgboost import XGBClassifier
from sklearn.preprocessing import OneHotEncoder
X, y = make_classification(n_samples=10,n_features=2,n_redundant=0,n_classes=2,random_state=42)
xgb = XGBClassifier(n_estimators=3,max_depth=3,random_state=42)
xgb.fit(X,y)
print(X)
# [[ 1.06833894 -0.97007347]
# [-1.14021544 -0.83879234]
# [-2.8953973   1.97686236]
# [-0.72063436 -0.96059253]
# [-1.96287438 -0.99225135]
# [-0.9382051  -0.54304815]
# [ 1.72725924 -1.18582677]
# [ 1.77736657  1.51157598]
# [ 1.89969252  0.83444483]
# [-0.58723065 -1.97171753]] 
print(y)
# [1 0 0 0 0 1 1 1 1 0]
X_leaf_index = xgb.apply(X)
print(X_leaf_index)
#[[2 2 2]
# [1 1 1]
# [1 1 1]
# [1 1 1]
# [1 1 1]
# [1 1 1]
# [2 2 2]
# [2 2 2]
# [2 2 2]
# [1 2 2]]
one_hot = OneHotEncoder()
X_one_hot = one_hot.fit_transform(X_leaf_index)
# <10x6 sparse matrix of type '<class 'numpy.float64'>'
# with 30 stored elements in Compressed Sparse Row format>
print(one_hot.get_feature_names())
# ['x0_1.0' 'x0_2.0' 'x1_1.0' 'x1_2.0' 'x2_1.0' 'x2_2.0']
print(X_one_hot)
#matrix([[0., 1., 0., 1., 0., 1.],
#        [1., 0., 1., 0., 1., 0.],
#        [1., 0., 1., 0., 1., 0.],
#        [1., 0., 1., 0., 1., 0.],
#        [1., 0., 1., 0., 1., 0.],
#        [1., 0., 1., 0., 1., 0.],
#        [0., 1., 0., 1., 0., 1.],
#        [0., 1., 0., 1., 0., 1.],
#        [0., 1., 0., 1., 0., 1.],
#        [1., 0., 0., 1., 0., 1.]])
```
Let us use first training sample as an example.
* by training a XGBoost model, [ 1.06833894 -0.97007347] is predicted to be on leaf node 2, leaf node 2, and leaf node 2 for each of the 3 trees in the ensamble (we set `n_estimators=3`)

* One-Hot Encoder get features to be leaf node 1,2 of each of the 3 trees, therefore there are 6 features in total

* One-Hot Encoder will then transform the prediction [2,2,2] to be [0., 1., 0., 1., 0., 1.].

<center><img src="https://dingma129.github.io/assets/figures/ESL/xgb_toy.png" width="1000"></center>

In this example, originally X has only 2 features, instead of doing manual feature engineering, we use XGBoost + One-Hot Encoder to get 6 features. The final number of features depends mostly on the hyperparameters `n_estimators` and `max_depth` of `XGBRFClassifier`.

---
<span style="font-weight:bold;font-size:36px">2. model performances</span>

In this section, I will compare the model performances to vanilla Logistic Regression on different kinds of sample sets generated by `sklearn.datasets.make_classification`.

```python
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve,roc_auc_score,auc

def baseline_performance(X_train,y_train,X_test,y_test):    
    # train Logistic Regression
    lr = LogisticRegression()
    lr.fit(X_train,y_train)
    y_pred_test = lr.predict_proba(X_test)[:,1]    
    # evaluation
    fpr,tpr,thresholds = roc_curve(y_test,y_pred_test)
    roc_auc = auc(fpr,tpr)    
    return fpr,tpr,roc_auc    
    
def model_performance(X_train,y_train,X_test,y_test,n_estimators=10,max_depth=3):
    X_xgb,X_lr,y_xgb,y_lr = train_test_split(X_train,y_train,test_size=0.5,random_state=42)
    xgb = XGBClassifier(n_estimators=n_estimators,max_depth=max_depth)
    one_hot = OneHotEncoder()
    lr = LogisticRegression()    
    # using (X_xgb,y_xgb)
    # train XGBoost
    xgb.fit(X_xgb,y_xgb)
    X_xgb_leaf_index = xgb.apply(X_xgb)
    # train One-Hot Encoder
    one_hot.fit(X_xgb_leaf_index)    
    # using (X_lr,y_lr)
    # transform using XGBoost + One-Hot Encoder
    X_lr_leaf_index = one_hot.transform(xgb.apply(X_lr))
    # train Logistic Regression
    lr.fit(X_lr_leaf_index,y_lr)    
    # prediction
    y_pred_test = lr.predict_proba(one_hot.transform(xgb.apply(X_test)))[:,1]    
    # evaluation
    fpr,tpr,thresholds = roc_curve(y_test,y_pred_test)
    roc_auc = auc(fpr,tpr)    
    return fpr,tpr,roc_auc
```
For each kind of data set, I run both models 30 times with different initialization of data set. I also run the mean test of 30 AUCs of those two models.

<center><img src="https://dingma129.github.io/assets/figures/ESL/xgb_table.png" width="700"></center>
<center><img src="https://dingma129.github.io/assets/figures/ESL/xgb_compare.png" width="1000"></center>
---
<span style="font-weight:bold;font-size:36px">3. conclusions</span>

* From the above figure and p-values (all less than 1e-3) in the above table, we can conclude that this new model performs significantly better than the vanilla Logistic Regression model. 
* When `max_depth=3`, the number of features extracted by XGBoost and One-Hot Encoder is between `2*n_estimators` and `7*n_estimators`, and is very close to `4*n_estimators` in most of the cases.
* If we choose a suitable `n_estimators` such that the dimension of feature vectors extracted by XGBoost and One-Hot Encoder is larger enough than the original feature dimension, those features extracted by the leaf node indices of each tree from the ensembled XGBClassifier (also GradientBoostingClassifier) are indeed very informative.
* If we only use XGBoost model, usually we need `n_estimators` to be hundreds or thousands. But linear models are faster to train. With the help of this method (XGBoost + One-Hot Encoder), we can get a competitive but faster result even comparing with XGBoost itself. 