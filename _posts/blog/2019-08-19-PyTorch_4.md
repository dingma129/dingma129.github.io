---
title: Generative Adversarial Networks (GAN)
subtitle: GAN using PyTorch
author: Ding Ma
layout: post
categories: [blog]
---

<span style="font-weight:bold;font-size:36px">0. Introduction</span>


<span style="font-weight:bold;font-size:36px">1. Structure of GAN</span>
<center><img src="https://dingma129.github.io/assets/figures/pytorch/pytorch_4_gan_structure.png" width="1000" ></center>

<span style="font-weight:bold;font-size:36px">2. How to train GAN?</span>
<center><img src="https://dingma129.github.io/assets/figures/pytorch/pytorch_4_gan_train.png" width="1000" ></center>

<span style="font-weight:bold;font-size:36px">3. Toy Example: simple 2D Curve</span>

<span style="font-weight:bold;font-size:32px">3.1 Dataset</span>
The dataset I will use in this section is the following one generated by `sklearn.datasets.make_circles`.
```python
import torch
from torch.utils.data import TensorDataset,DataLoader
from sklearn.datasets import make_circles
n_examples = 1000
# generate 2 circles
X1,y1=make_circles(n_samples=n_examples*2,noise=0.0)
# only keep one of them
X1 = X1[y1==1]
X1 = torch.tensor(X1,dtype=torch.float32)
y1 = torch.ones((n_examples,1),dtype=torch.float32)
batch_size = 4
trainLoader1 = DataLoader(TensorDataset(X1, y1), batch_size=batch_size, shuffle=True,drop_last=True)
```
<center><img src="https://dingma129.github.io/assets/figures/pytorch/pytorch_4_1_data.png" width="600" ></center>

<span style="font-weight:bold;font-size:32px">3.2 Model</span>
```python
import torch.nn as nn
import torch.optim as optim
# Generator with 2D noise input
netG = nn.Sequential(
    nn.Linear(2,8),nn.Tanh(),
    nn.Linear(8,8),nn.Tanh(),
    nn.Linear(8,2))
# Discriminator
netD = nn.Sequential(
    nn.Linear(2,8),nn.Tanh(),
    nn.Linear(8,8),nn.Tanh(),
    nn.Linear(8,1),nn.Sigmoid())
# have applied Sigmoid in netD, so choose BCELoss
loss = nn.BCELoss()
# optimizers training G and D
trainerG = optim.RMSprop(netG.parameters(),lr=0.001)
trainerD = optim.RMSprop(netD.parameters(),lr=0.005)
# a default real/fake label for later use
real_label = torch.ones((batch_size,1),dtype=torch.float32)
fake_label = torch.zeros((batch_size,1),dtype=torch.float32)
```

<span style="font-weight:bold;font-size:32px">3.3 Training</span>
I will train the generator and discriminator for 100 epochs as discussed in Section 2.
```python
for epoch in range(100):
    lossD = 0
    lossG = 0
    correct = 0
    total = 0
    for i, data in enumerate(trainLoader1):
        trainerD.zero_grad()
        trainerG.zero_grad()
        # Step 1: Update D
        # real data
        real_inputs, real_label = data
        noise = torch.randn((batch_size, 2))
        real_output = netD(real_inputs)
        errD_real = loss(real_output, real_label)
        # fake data
        fake_inputs = netG(noise)
        # need .detach() since only train D part
        fake_output = netD(fake_inputs.detach())
        errD_fake = loss(fake_output, fake_label)
        # errD from both real and fake data
        errD = errD_real + errD_fake
        lossD += errD.item()
        # back propagate errD
        errD.backward()
        # get stats
        correct += ((real_output>=0.5) == (real_label==1)).sum().item()
        correct += ((fake_output<0.5) == (fake_label==0)).sum().item()
        total += 2*batch_size
        # updating parameters of D
        trainerD.step()
        # Step2: Update G
        fake_output = netD(fake_inputs)
        errG = loss(fake_output, real_label)
        lossG += errG.item()
        # back propagate errG
        errG.backward()
        # updating parameters of G
        trainerG.step() 
    # print results for every 20 epochs
    if epoch%20==19:
        # save model
        torch.save(netG, "model/gan1_epoch{}".format(epoch+1))
        print("Epoch:{}\tAcc:{:.4f}\tLossD:{:.4f}\tLossG:{:.4f}".format(epoch+1,correct/total,lossD/(total/batch_size),lossG/(total/batch_size))) 
        # generate random noise
        noise = torch.randn((100, 2))
        fake_inputs = netG(noise)
        fig = plt.figure(figsize=(1,1))
        # plot real data
        plt.scatter(X1[:,0].numpy(), X1[:,1].numpy())
        # plot fake data generated by G
        plt.scatter(fake_inputs[:,0].detach().numpy(), fake_inputs[:,1].detach().numpy())
        plt.axis('off')
        plt.show();
```
The result of training is shown in the following figure.
<center><img src="https://dingma129.github.io/assets/figures/pytorch/pytorch_4_1_train.png" width="600" ></center>

<span style="font-weight:bold;font-size:32px">3.4 Generate some fake data</span>
I will now use the trained generator to produce some fake datas. 
```python
# load the trained generator (100 epochs)
netG1 = torch.load("model/gan1_epoch100")
noise = torch.randn((10000, 2))
fake_inputs = netG1(noise)
fig = plt.figure(figsize=(6,6))
plt.scatter(X1[:,0].numpy(), X1[:,1].numpy())
plt.scatter(fake_inputs[:,0].detach().numpy(), fake_inputs[:,1].detach().numpy(),alpha=0.1);
```
It can seen from the figure that they are quite similar to the real data.
<center><img src="https://dingma129.github.io/assets/figures/pytorch/pytorch_4_1_test.png" width="600" ></center>

<span style="font-weight:bold;font-size:36px">4. Example: MNIST</span>







We will use the famous MNIST dataset to train the CNN. MNIST is available in `torchvision.datasets`, and we will load it using `torch.utils.data.DataLoader` with `batch_size = 128`.
```python
import torch
import torchvision
import torchvision.transforms as transforms
# transform datasets into Tensor
transform = transforms.Compose([transforms.ToTensor()])
# load train set
trainset = torchvision.datasets.MNIST(root='./data', train=True,download=True, transform=transform)
# set batch_size = 128, and shuffle the data
trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,shuffle=True)
# load train set
testset = torchvision.datasets.MNIST(root='./data', train=False,download=True, transform=transform)
# set batch_size = 128
testloader = torch.utils.data.DataLoader(testset, batch_size=128,shuffle=False)
```
---
<span style="font-weight:bold;font-size:36px">2. Define Model Using `torch.nn.Sequential`</span>

Building a neural network model using `torch.nn.Sequential` is quite similar to `keras.models.Sequential`.
```python
import torch.nn as nn
# define the model
model1 = nn.Sequential(
    nn.Conv2d(1,8,3),nn.ReLU(),nn.MaxPool2d((2,2)),
    nn.Conv2d(8,16,3),nn.ReLU(),nn.MaxPool2d((2,2)),
    nn.Flatten(),
    nn.Linear(5*5*16,64),nn.ReLU(),
    nn.Linear(64,32),nn.ReLU(),
    nn.Linear(32,10))
import torch.optim as optim
# use CrossEntropyLoss here since we don't apply softmax for the last layer
criterion = nn.CrossEntropyLoss(reduction='mean')
# choose RMSprop optimizer
optimizer = optim.RMSprop(model1.parameters(),lr=0.001,momentum=0.9)
```
Training CNN is the same as what I did in the [<span style="color:blue">last blog</span>](https://dingma129.github.io/blog/2019/08/15/PyTorch_2.html) when training a linear regression model.
```python
# train the model for 10 epochs
for epoch in range(10):  
    # reset loss for each epoch
    running_loss = 0.0
    for i, data in enumerate(trainloader):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data
        # reset parameter gradients
        optimizer.zero_grad()
        # forward propagation
        outputs = model1(inputs)
        loss = criterion(outputs, labels)
        # backward propagation
        loss.backward()
        # perform a single optimization step
        optimizer.step()
        # add batch loss to running_loss of current epoch
        running_loss += loss.item()
    # print statistics
    print('{}\tloss:\t{:.4f}'.format(epoch + 1, running_loss / (i+1)))

# 1	loss:	0.2298
# 2	loss:	0.0815
# 3	loss:	0.0670
# 4	loss:	0.0576
# 5	loss:	0.0518
# 6	loss:	0.0484
# 7	loss:	0.0480
# 8	loss:	0.0440
# 9	loss:	0.0435
# 10	loss:	0.0431
# Finished Training
```
---
<span style="font-weight:bold;font-size:36px">3. Define Model by Inheriting from `torch.nn.Module`</span>

By inheriting from `torch.nn.Module`, we can define a model functionally as in Keras functional API.
```python
import torch.nn as nn
import torch.nn.functional as F
# channel first
class Net(nn.Module):
    def __init__(self):
        super(Net,self).__init__()
        # layers
        self.conv1 = nn.Conv2d(1,8,3)   # (1,28,28) => (8,26,26)
        self.conv2 = nn.Conv2d(8,16,3)  # (8,13,13) => (16,11,11)
        self.fc1 = nn.Linear(5*5*16,64)
        self.fc2 = nn.Linear(64,32)
        self.fc3 = nn.Linear(32,10) 
    def forward(self,x):
        # forward propagation
        x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))
        x = F.max_pool2d(F.relu(self.conv2(x)),(2,2))
        # flatten
        x = x.view(-1, torch.numel(x[0]))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
model2 = Net()
# use the same code for training
# 1	loss:	0.2158
# 2	loss:	0.0795
# 3	loss:	0.0644
# 4	loss:	0.0553
# 5	loss:	0.0502
# 6	loss:	0.0440
# 7	loss:	0.0441
# 8	loss:	0.0391
# 9	loss:	0.0380
# 10	loss:	0.0359
# Finished Training
```
---
<span style="font-weight:bold;font-size:36px">4. Comparison</span>

* `nn.Sequential` is similar to `keras.models.Sequential`, and the structure of layers can only be linear;
* `nn.Module`is similar to Keras' functional API `keras.models.Model`, and the structure of layers can be any (not necessary linear).