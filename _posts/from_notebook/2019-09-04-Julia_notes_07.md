---
title: "Julia: Notes-07"
layout: splash
excerpt: "RNN"
categories: [Julia]
tags: [Regression, Time Series, Flux, RNN, LSTM]
---

# 0. Introduction

In this blog, I will introduce how to construct an RNN model using `Flux.jl`. I will use it to study some simple time series data.

First I will load some necessary packages.
```julia
using Flux, Statistics, Plots
gr()
```

# 1. Data

I will create some fake time series data as below. It's the composition of two sine functions with some normal distributed error term.

```julia
X = sin.(range(1,30,length=1000)) + 0.3 * sin.(range(1,30,length=1000)*4) + randn(1000)/30;
plot(X,label="");
```
![png](/assets/figures/julia_notes/07_data.png = 600x)

# 2. Problem description

The data `X` is a time series data of length `1000`. The problem I will study in this blog can be described as
* For every continuous part of the data of length 50, I want to predict the values in the next 50 timestamps.
* But if we build a model that uses `[t,t+49]` to predict `[t+50,t+99]` directly, this model will not behave well.
* Instead, I will build a model that uses `[t,t+49]` to predict `[t+1,t+50]`. Next, I will use `[t+1,t+49]` along with the predicted value at `t+50` (49 given and 1 prediction) to predict `[t+2,t+51]`. At each step, only the last value will be added into the prediction set. Iteratively, we can get the prediction at `[t+50,t+99]`.

# 3. Data preprocessing 

Since I will build a model that uses `[t,t+49]` to predict `[t+1,t+50]` for any timestamp `t`, I will preprocess the data as follows.

```julia
# seqlen: length of seq, in our case 50
# step_size: used to sample the data, I will use 3 later
# batch_size: batch size, I will use 100
function create_batch(X,seqlen,step_size,batch_size)
    # determine the number of sequences and batchs
    n_seq = div((length(X)-1-seqlen),step_size) 
    n_batchs = div(n_seq,batch_size)       
    # an array storing all batchs
    data = []
    # create two 2D empty arrays of shape (seqlen,batch_size)
    # one for X_batch and the other for y_batch
    for nb in 0:(n_batchs-1) 
        X_batch = fill(Float32(0.0),seqlen,batch_size)  
        y_batch = fill(Float32(0.0),seqlen,batch_size)
        for s in 0:(batch_size-1)  
            loc = 1+s*step_size+nb*batch_size*step_size 
            for i in 0:(seqlen-1)
                X_batch[i+1,s+1] = X[loc+i]
                # y_batch will just be the next of the values in X_batch
                y_batch[i+1,s+1] = X[loc+i+1]
            end
        end
        push!(data,(X_batch,y_batch))
    end
    data
end

data = create_batch(X,50,3,100);
@show length(data)
# length(data) = 3
@show size(data[1][1])
# size((data[1])[1]) = (50, 100)
```

Now I have splitted the trainning data into 3  mini-batchs. In each mini-batch, X_batch and y_batch both have size being `(50,100)`, where `50` is the length of the sequence and `100` is the batch_size I set.

# 4. LSTM model and training

I will build an LSTM model with 4 layers using `Flux`.

```julia
# LSTM model
model = Chain(
    LSTM(50,64),         #(50,100) => (64,100)
    LSTM(64,64),         #(64,100) => (64,100)
    Dense(64,32,relu),   #(64,100) => (32,100)
    Dense(32,50))        #(32,100) => (50,100)
# loss function
function loss(xs,ys)
    l = Flux.mse(model(xs),ys)
    Flux.reset!(model)  # Reset the hidden state of a recurrent layer back to its original value.
    return l
end
# Adam optimizer
opt = ADAM(0.002)
# train for 100 epochs
epochs = 100
for epoch in 1:epochs
    for minibatch in data
        Flux.train!(loss5, params(model5),[minibatch], opt)
    end
    # print loss for all three mini-batchs every 10 epochs
    if epoch % 10 ==0
        println("epoch $epoch:")
        println("$(Tracker.data(loss5(data[1]...))),\t\t$(Tracker.data(loss5(data[2]...))),\t\t$(Tracker.data(loss5(data[3]...)))")
    end
end
```

```
epoch 10:
0.20505875,		0.21414189,		0.18985265
epoch 20:
0.06137546,		0.061469544,		0.05728308
epoch 30:
0.026156224,		0.026726793,		0.027290648
epoch 40:
0.021102335,		0.021497905,		0.022359867
epoch 50:
0.011918176,		0.011924048,		0.012644442
epoch 60:
0.005181433,		0.0053714043,		0.0052413577
epoch 70:
0.0038366097,		0.0040489896,		0.0038878573
epoch 80:
0.002491883,		0.0026454614,		0.0025166033
epoch 90:
0.0016024113,		0.0017000829,		0.0015301167
epoch 100:
0.0014567217,		0.0014943186,		0.0013296758
```

We can see the loss function decreases pretty well. 


# 5. Visualization

The following figure contains 5 prediction plots
1. use [1,50] to predict [51,250]
2. use [251,300] to predict [301,500]
3. use [501,550] to predict [551,750]
4. use [751,800] to predict [801,1000]
5. use [1,50] to predict [51,1000]

```julia
X_pred_1 = X[1:50]
X_pred_2 = X[251:300]
X_pred_3 = X[501:550]
X_pred_4 = X[751:800]
# remember to call Flux.reset! after each step
for i in 1:200
    Flux.reset!(model5)
    push!(X_pred_2,Tracker.data(model5(X_pred_2[i:49+i])[end]))
    Flux.reset!(model5)
    push!(X_pred_3,Tracker.data(model5(X_pred_3[i:49+i])[end]))
    Flux.reset!(model5)
    push!(X_pred_4,Tracker.data(model5(X_pred_4[i:49+i])[end]))
end
for i in 1:950
    Flux.reset!(model5)
    push!(X_pred_1,Tracker.data(model5(X_pred_1[i:49+i])[end]))
end
# set a layout, 4 subplots in row 1 and 1 subplot in row 2
l = @layout [a b c d; f]
plot(1:250,X[1:250],label="")
p1 = plot!(51:250,X_pred_1[51:250],lw=5,linealpha=0.4,c=:red,label="")
plot(251:500,X[251:500],label="")
p2 = plot!(301:500,X_pred_2[51:250],lw=5,linealpha=0.4,c=:red,label="")
plot(501:750,X[501:750],label="")
p3 = plot!(551:750,X_pred_3[51:250],lw=5,linealpha=0.4,c=:red,label="")
plot(751:1000,X[751:1000],label="")
p4 = plot!(801:1000,X_pred_4[51:250],lw=5,linealpha=0.4,c=:red,label="")
plot(X,label="real")
p5 = plot!(51:1000,X_pred_1[51:1000],lw=5,linealpha=0.4,c=:red,label="prediction")
plot(p1,p2,p3,p4,p5,layout=l,size=(800,400))
```

![png](/assets/figures/julia_notes/07_prediction.png = 1000x)

It can be seen from the above figure that our prediction is quite well.
