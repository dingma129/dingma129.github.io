---
title: Recommender Systems (part 4)
subtitle: Implementation of ALS and SVD Using PyTorch
author: Ding Ma
layout: post
categories: [blog]
---

<span style="font-weight:bold;font-size:32px">0. Introduction</span>

In this blog, I will implement ALS, SVD and SVD with baselines using PyTorch. I will use them on the famous MovieLens 100K Dataset and its new version. Those 3 methods have already been discussed in previous blog [<span style="color:blue">here</span>](https://dingma129.github.io/blog/2019/08/27/Rec_Sys_2.html). For simplicity, all matrix factorization would be R=UV instead of R=UV^T during this blog.

<span style="font-weight:bold;font-size:32px">1. ALS</span>

<span style="font-weight:bold;font-size:28px">1.1 ALS for complete matrices</span>

For complete matrices, we can use `torch.lstsq` to solve the least-squares problems directly without using optimization methods. 
<center><img src="https://dingma129.github.io/assets/figures/rec/rec_4_als1.png" width="1000" ></center>
```python
import numpy as np
import torch
def completeALS(X,k=5,epochs=10):
    n_rows,n_cols = X.shape
    # losses
    loss_list = []
    # random initialization
    U = torch.randn((n_rows,k),dtype=torch.float32)
    # repeating
    for epoch in range(epochs):
        # solve for V, keep only the first k rows
        V = torch.lstsq(X,U)[0][:k,:]
        # solve for U, keep only the first k rows
        U = torch.lstsq(X.T,V.T)[0][:k,:].T
        # record RMSE
        loss_list.append(torch.sqrt((U@V-X).pow(2).mean()).item())
    # return a detached cloned version of U and V and a list of losses
    return U.detach().clone(),V.detach().clone(),loss_list
```

<span style="font-weight:bold;font-size:28px">1.2 ALS for incomplete matrices</span>

For incomplete matrices, we do not have a function to use directly. So we need to use an optimizer to update U and V. The full idea of the implementation is explained in the following figure. By using the boolean mask for train, we can compute the train loss only over the observed ratings using `torch`'s methods only. The boolean mask for test is used when recording the test loss.
<center><img src="https://dingma129.github.io/assets/figures/rec/rec_4_als2.png" width="1000" ></center>
```python
def incompleteALS(X, k=5, weights_train=weights_train, weights_test=weights_test,
                    optimizer=torch.optim.SGD, lr=0.01,epochs=20,reg=0.0,tol=1e-4):
    n_rows,n_cols = X.shape
    # size of train/test, used to compute mean
    n_observed = weights_train.sum()
    n_unobserved = weights_test.sum()
    # record loss of train/test
    train_loss_list = []
    test_loss_list = []
    # random initialization, normal distribution with mean=0 and std=0.1
    U = (torch.randn((n_rows,k),dtype=torch.float32)*0.1).clone().requires_grad_(True)
    V = (torch.randn((k,n_cols),dtype=torch.float32)*0.1).clone().requires_grad_(True)
    # 2 optimizers for U and V
    optimizer_U = optimizer([U], lr=lr)
    optimizer_V = optimizer([V], lr=lr)
    # repeating
    for epoch in range(epochs):
        # using optimizer to find the V minimizing loss, until the improvement < tol
        last_loss = -1
        while True:
            # clean the gradients
            optimizer_V.zero_grad()
            # only learning V, so detach U
            X_pred = U.detach()@V
            # mean MSE + regularization terms
            loss = ((X_pred-X)*weights_train).pow(2).sum()/n_observed + reg * V.pow(2).mean()
            loss.backward()
            optimizer_V.step()
            # if does not improve > tol, quit the loop
            if torch.abs(last_loss-loss)<tol:
                break
            else:
                last_loss = loss.item()
        # do the same for U
        last_loss = -1
        while True:
            optimizer_U.zero_grad()
            X_pred = U@V.detach()
            loss = ((X_pred-X)*weights_train).pow(2).sum()/n_observed + reg * U.pow(2).mean()
            loss.backward()
            optimizer_U.step()
            if torch.abs(last_loss-loss)<tol:
                break
            else:
                last_loss = loss.item()
        # record losses
        train_loss_list.append(torch.sqrt(((X_pred-X)*weights_train).pow(2).sum()/n_observed).item())
        test_loss_list.append(torch.sqrt(((X_pred-X)*weights_test).pow(2).sum()/n_unobserved).item())
    # return a detached cloned version of U and V
    return U.detach().clone(),V.detach().clone(),train_loss_list,test_loss_list
```

<span style="font-weight:bold;font-size:32px">2. SVD</span>

In this section, I will only implement SVD with/without baselines for incomplete matrices. The models for complete matrices are just special cases of it. Again, as in the above implementation of ALS, I used two boolean masks for train/test datas.

```python
def incompleteSVD(X, k=5, weights_train=weights_train, weights_test=weights_test,
                    baseline = True,optimizer=torch.optim.SGD, l2=True, reg=0.0, lr=0.1, epochs=200):
    n_rows,n_cols = X.shape
    # used to compute mean
    n_observed = weights_train.sum()
    n_unobserved = weights_test.sum()
    # record loss of train/test
    train_loss_list = []
    test_loss_list = []
    # mu set to be 
    mu = (X*weights_train).sum()/n_observed
    # random initialization
    U = (torch.randn((n_rows,k),dtype=torch.float32)*0.1).clone().requires_grad_(True)
    V = (torch.randn((k,n_cols),dtype=torch.float32)*0.1).clone().requires_grad_(True)
    bu = torch.randn((n_rows,1),dtype=torch.float32,requires_grad=True)
    bv = torch.randn((1,n_cols),dtype=torch.float32,requires_grad=True)
    # specified optimizer with learning rate = lr
    if baseline:
        optimizer = optimizer([U,V,bu,bv], lr=lr)
    else:
        optimizer = optimizer([U,V], lr=lr)
    # training
    for epoch in range(epochs):
        # clear all gradients
        optimizer.zero_grad()
        # with baseline
        if baseline:
            X_pred = mu+bu+bv+U@V
            # compute loss function with regularization
            loss = ((X_pred-X)*weights_train).pow(2).sum()/n_observed + reg * U.pow(2).mean() + reg * V.pow(2).mean() + reg * bu.pow(2).mean() + reg * bv.pow(2).mean()
        # without baseline
        else:
            X_pred = U@V
            loss = ((X_pred-X)*weights_train).pow(2).sum()/n_observed + reg * U.pow(2).mean() + reg * V.pow(2).mean()
        # back propagation
        loss.backward()
        optimizer.step()
        # record loss
        train_loss_list.append(torch.sqrt(((X_pred-X)*weights_train).pow(2).sum()/n_observed).item())
        test_loss_list.append(torch.sqrt(((X_pred-X)*weights_test).pow(2).sum()/n_unobserved).item())
    # return a detached cloned version of U and V
    return U.detach().clone(),V.detach().clone(),train_loss_list,test_loss_list
```

<span style="font-weight:bold;font-size:32px">3. Comparing with `Surprise`'s implementation</span>

The following are the model performances of SVD (k=2) on MovieLens 100k with 5-fold cross-validation using `Surprise` and the above implementation. Both model runs for 20 epochs.
```python
from surprise import SVD
from surprise import Dataset
from surprise import accuracy
from surprise.model_selection import cross_validate,KFold
# Load the movielens-100k dataset
data = Dataset.load_builtin('ml-100k')
svd = SVD(n_factors=2)
cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=KFold(n_splits=5,random_state=42), 
               return_train_measures=True,  verbose=True);
Evaluating RMSE, MAE of algorithm SVD on 5 split(s).
```
```javascript
Evaluating RMSE, MAE of algorithm SVD on 5 split(s).

                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     
RMSE (testset)    0.9376  0.9378  0.9445  0.9344  0.9395  0.9388  0.0033  
MAE (testset)     0.7397  0.7403  0.7467  0.7383  0.7424  0.7415  0.0029  
RMSE (trainset)   0.9070  0.9113  0.9052  0.9017  0.9092  0.9069  0.0033  
MAE (trainset)    0.7170  0.7203  0.7149  0.7124  0.7188  0.7167  0.0028  
Fit time          1.72    1.76    1.61    1.74    1.73    1.71    0.05    
Test time         0.21    0.17    0.37    0.17    0.35    0.25    0.09    
```
```python
from time import time
# using randint to assign 0-4 to each position uniformly
# each number corresponding to one fold
mask_ml = torch.randint_like(X_ml,low=0,high=5,dtype=torch.float32)
time_list,rmse1_list,rmse2_list,mae1_list,mae2_list = [],[],[],[],[]
# 5 fold cross-validation
for i in range(5):    
    print("Fold {}:".format(i+1))
    # set train and test boolean mask using mask_ml
    weights_train = torch.where((mask_ml!=i) & (~torch.isnan(X_ml)),torch.ones_like(X_ml,dtype=torch.float32),torch.zeros_like(X_ml,dtype=torch.float32))
    weights_test = torch.where((mask_ml==i) & (~torch.isnan(X_ml)),torch.ones_like(X_ml,dtype=torch.float32),torch.zeros_like(X_ml,dtype=torch.float32))
    # record model runtime
    start = time()
    U_pred, V_pred, train_rmse_list, test_rmse_list, train_mae_list, test_mae_list  = incompleteSVD(X_true_ml, 2, weights_train, weights_test, optimizer=torch.optim.RMSprop, lr=0.3,l2=True, reg=0.008, epochs=20)
    end = time()
    time_list.append(round(end-start,2))
    rmse1_list.append(round(train_rmse_list[-1],4)); rmse2_list.append(round(test_rmse_list[-1],4))
    mae1_list.append(round(train_mae_list[-1],4)); mae2_list.append(round(test_mae_list[-1],4))
    print("\tFold Size:\t{}\t\tTime Used:\t{} seconds".format((mask_ml==1).sum().item(),time_list[-1]))
    print("\tRMSE(train):\t{}\t\tRMSE(test):\t{}".format(rmse1_list[-1],rmse2_list[-1]))
    print("\tMAE(train):\t{}\t\tMAE(test):\t{}".format(mae1_list[-1],mae2_list[-1]))
# print average performance
print("Mean:")
print("\t\t\t\t\tTime Used:\t{} seconds".format(round(np.mean(time_list),2)))
print("\tRMSE(train):\t{}\t\tRMSE(test):\t{}".format(round(np.mean(rmse1_list),4),round(np.mean(rmse2_list),4)))
print("\tMAE(train):\t{}\t\tMAE(test):\t{}".format(round(np.mean(mae1_list),4),round(np.mean(mae2_list),4)))
```
```javascript
Fold 1:
	Fold Size:	316856		Time Used:	1.29 seconds
	RMSE(train):0.9087		RMSE(test):	0.9432
	MAE(train):	0.7159		MAE(test):	0.7401
Fold 2:
	Fold Size:	316856		Time Used:	1.24 seconds
	RMSE(train):0.9099		RMSE(test):	0.9398
	MAE(train):	0.7201		MAE(test):	0.7432
Fold 3:
	Fold Size:	316856		Time Used:	1.27 seconds
	RMSE(train):0.9182		RMSE(test):	0.9501
	MAE(train):	0.7168		MAE(test):	0.7438
Fold 4:
	Fold Size:	316856		Time Used:	1.49 seconds
	RMSE(train):0.909		RMSE(test):	0.9462
	MAE(train):	0.7137		MAE(test):	0.7434
Fold 5:
	Fold Size:	316856		Time Used:	1.62 seconds
	RMSE(train):0.9084		RMSE(test):	0.9422
	MAE(train):	0.7167		MAE(test):	0.7443
Mean:
                            Time Used:	1.38 seconds
	RMSE(train):0.9108		RMSE(test):	0.9443
	MAE(train):	0.7166		MAE(test):	0.743
```
From the above comparison, we can see that the `PyTorch` implementation of `SVD` is much faster than the `Surprise` implementation.

<span style="font-weight:bold;font-size:32px">4. TensorBoard</span>

<center><img src="https://dingma129.github.io/assets/figures/rec/rec_4_tbtest.svg" width="1000" ></center>



<span style="font-weight:bold;font-size:32px">0. Introduction</span>
<span style="font-weight:bold;font-size:32px">0. Introduction</span>
<span style="font-weight:bold;font-size:32px">0. Introduction</span>


