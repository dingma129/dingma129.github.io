---
title: "Kaggle - 1"
layout: splash
excerpt: "Competition: Dont overfit II"
categories: [Python]
tags: [Kaggle, Classification]
---

# 0. Introduction

In this blog, I will introduce the method I used in the Kaggle competition: [Don't Overfit! II](https://www.kaggle.com/c/dont-overfit-ii). 

This is a competition that challenged mere mortals to model a 20,000x200 matrix of continuous variables using only 250 training samplesâ€¦ without overfitting. Just as the name of the competition, the main problem here is how to deal with overfitting.

# 1. Data

Let's first look at the training data.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
# load data using pandas
df = pd.read_csv("train.csv",index_col=0)
# split features and labels as X and Y
X = df.iloc[:,1:].to_numpy()
y = df.iloc[:,0].to_numpy()
print("Shape of X:\t{}".format(X.shape))
print("Shape of y:\t{}".format(y.shape))
# Shape of X:	(250, 300)
# Shape of y:	(250,)
```
<center><img src="/assets/figures/kaggle/1_data.png" width="1000" ></center>

If we look at each feature separately, we can see that the means are mostly very close to 0 and the standard deviations are most very close to 1.
<center><img src="/assets/figures/kaggle/1_mean.png" width="800" ></center>

# 2. Idea

In order to avoid overfitting badly, I will do both feature selection and upsampling. At the end, a logistic regression will be applied.

## 2.1 Feature selection

Since there are too many features, we need to perform some kind of feature selection.

I will use `scipy.stats.ttest_ind` on the groups of label 0 and 1. This method calculates the T-test for the means of two independent samples of scores. I will keep the features with the p-value of t-score being less than 0.05. Among those features, those with p-values less than 0.005 are defined to be important features (which will be used in the next section for upsampling).

```python
from scipy.stats import ttest_ind
idx_list=[]
important_list=[]
for i in range(300):
    p = ttest_ind(X[y==0,i],X[y==1,i],equal_var=False)[1]
    if p < 0.05:
        idx_list.append(i)
        if p< 0.005:
            important_list.append(i)
print(len(idx_list),len(important_list))
# 34 5
# 34 features are selected, among them 5 are important
```

## 2.2 Upsampling

The original 300 samples are too less for model to be not overfitting. So we also need to upsample. The idea is for any given sample, I will create 1000 samples close to it, and assume them having the same label as the given one. How to create those 1000 samples?
1. for those 5 important features, new samples will obtained from a normal distribution with a relative small standard deviation (0.1 times sample std in my model);
2. for the rest 34-5=29 selected features, they are from a normal distribution with larger standard deviation (1.0 times sample std in my mode).

```python
# weight
col_std_weight = np.ones((1,300))*1.0    # un-important
col_std_weight[:,important_list] = 0.1   # important
# sample std
col_std = X.std(axis=0).reshape(1,-1)
# fix a seed
np.random.seed(42)
upsampled_X = []
upsampled_y = []
for row in range(250):
    # (1,300) => (1000,300)
    upsampled_X.append(np.random.normal(X[row,:], col_std_weight*col_std, (1000,300)))
    upsampled_y.append(np.ones(1000)*y[row])
X_upsampled = np.vstack(upsampled_X)  # (250,300) => (250000,300)
y_upsampled = np.hstack(upsampled_y)  # (250,) => (250000,)
```

## 2.3 Logistic regression model

Finally, I will fit the model using upsampled dataset with selected features only.

```python
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(solver='lbfgs',C=100.0)
lr.fit(X_upsampled[:,idx_list],y_upsampled)
```

This model got the following quite decent ROC AUC score.

<center><img src="/assets/figures/kaggle/1_auc.png" width="800" ></center>

# 3. Summary

Most high ranking models posted in the Kaggle discussion are using LeaderBoard-Probing. Here I have shown another way of dealing with the overfitting problem: feature selection + upsampling, which is also very effective.

