---
title: "PyTorch - 4"
layout: splash
excerpt: "GAN using PyTorch"
categories: [Python]
tags: [PyTorch, GAN, Image Processing]
---

<span style="font-weight:bold;font-size:36px">0. Introduction</span>

A generative adversarial network (GAN) is a class of machine learning systems invented by Ian Goodfellow and his colleagues in 2014. Two neural networks contest with each other in a game. Given a training set, this technique learns to generate new data with the same statistics as the training set.

In this blog, I will introduce the basics of GAN and how to train a GAN. I will give 2 examples of GAN using `PyTorch`, one for a simple 2D curve and the other one for MNIST.

<span style="font-weight:bold;font-size:36px">1. Structure of GAN</span>

GAN involves 2 models as shown in the following figure: generator and discriminator. The generator generates candidates while the discriminator evaluates them. 
<center><img src="https://dingma129.github.io/assets/figures/pytorch/pytorch_4_gan_structure.png" width="1000" ></center>
<span style="font-weight:bold;font-size:36px">1.1 Generator</span>

The generator learns to map from a latent space to a data distribution of interest. The objective of the generator is to fool the discriminator (i.e., to increate the error rate of the discriminator). The generator is typically a deconvolutional neural network. In the above figure, the generator maps noises from the latent space into fake data, and the goal of generator is trying to mimic real data.

<span style="font-weight:bold;font-size:36px">1.2 Discriminator</span>

The generator learns to distinguish candidates produced by the generator from the true data distribution. A known dataset serves as the initial data for the discriminator. Training the discriminator involves presenting it with samples from the training dataset.  The discriminator is typically a convolutional neural network. In the above figure, the discriminator learns to distinguish fake data from real data.

---
<span style="font-weight:bold;font-size:36px">2. How to train GAN?</span>

The algorithm of training a GAN is described in the following figures.
<center><img src="https://dingma129.github.io/assets/figures/pytorch/pytorch_4_gan_train.png" width="1000" ></center>
It involves repeating the following two steps.
1. Training Discriminator: We first generate some fake data using the generator. The discriminator is a classifier trained using fake data (label = 0) and real data (label = 1). 
2. Training Generator: The generator is trained only using noise. The discriminator is fed using fake data (label = 1) with binary cross entropy loss function. So minimizing it will lead the generator to learn to generate "good" fake data (successfully fool the discriminator). At this stage, only generator is being training. 

---
<span style="font-weight:bold;font-size:36px">3. Toy Example: simple 2D Curve</span>

In this example, I will build a GAN model to generate points on a 2D circle.

<span style="font-weight:bold;font-size:32px">3.1 Dataset</span>

The dataset used in this section is the following one generated by `sklearn.datasets.make_circles`.
```python
import torch
from torch.utils.data import TensorDataset,DataLoader
from sklearn.datasets import make_circles
n_examples = 1000
# generate 2 circles
X1,y1=make_circles(n_samples=n_examples*2,noise=0.0)
# only keep one of them
X1 = X1[y1==1]
X1 = torch.tensor(X1,dtype=torch.float32)
y1 = torch.ones((n_examples,1),dtype=torch.float32)
batch_size = 4
trainLoader1 = DataLoader(TensorDataset(X1, y1), batch_size=batch_size, shuffle=True,drop_last=True)
```
<center><img src="https://dingma129.github.io/assets/figures/pytorch/pytorch_4_1_data.png" width="500" ></center>
<span style="font-weight:bold;font-size:32px">3.2 Models</span>
For this toy example, I simply use fully connected neural network.
```python
import torch.nn as nn
import torch.optim as optim
# Generator with 2D noise input
netG = nn.Sequential(
    nn.Linear(2,8),nn.Tanh(),
    nn.Linear(8,8),nn.Tanh(),
    nn.Linear(8,2))
# Discriminator
netD = nn.Sequential(
    nn.Linear(2,8),nn.Tanh(),
    nn.Linear(8,8),nn.Tanh(),
    nn.Linear(8,1),nn.Sigmoid())
# have applied Sigmoid in netD, so choose BCELoss
loss = nn.BCELoss()
# optimizers training G and D
trainerG = optim.RMSprop(netG.parameters(),lr=0.001)
trainerD = optim.RMSprop(netD.parameters(),lr=0.005)
# a default real/fake label for later use
real_label = torch.ones((batch_size,1),dtype=torch.float32)
fake_label = torch.zeros((batch_size,1),dtype=torch.float32)
```

<span style="font-weight:bold;font-size:32px">3.3 Training</span>

I will train the generator and discriminator for 100 epochs using the algorithm as discussed in Section 2.
```python
for epoch in range(100):
    lossD = 0
    lossG = 0
    correct = 0
    total = 0
    for i, data in enumerate(trainLoader1):
        trainerD.zero_grad()
        trainerG.zero_grad()
        # Step 1: Update D
        # real data
        real_inputs, real_label = data
        noise = torch.randn((batch_size, 2))
        real_output = netD(real_inputs)
        errD_real = loss(real_output, real_label)
        # fake data
        fake_inputs = netG(noise)
        # need .detach() since the noise=>fake_inputs part will be used later when training G
        fake_output = netD(fake_inputs.detach())
        errD_fake = loss(fake_output, fake_label)
        # errD from both real and fake data
        errD = errD_real + errD_fake
        lossD += errD.item()
        # back propagate errD
        errD.backward()
        # get stats
        correct += ((real_output>=0.5) == (real_label==1)).sum().item()
        correct += ((fake_output<0.5) == (fake_label==0)).sum().item()
        total += 2*batch_size
        # updating parameters of D
        trainerD.step()
        # Step2: Update G
        fake_output = netD(fake_inputs)
        # using real_label since G is trying to fool D
        errG = loss(fake_output, real_label)
        lossG += errG.item()
        # back propagate errG
        errG.backward()
        # updating parameters of G
        trainerG.step() 
    # print results for every 20 epochs
    if epoch%20==19:
        # save model
        torch.save(netG, "model/gan1_epoch{}".format(epoch+1))
        print("Epoch:{}\tAcc:{:.4f}\tLossD:{:.4f}\tLossG:{:.4f}".format(epoch+1,correct/total,lossD/(total/batch_size),lossG/(total/batch_size))) 
        # generate random noise
        noise = torch.randn((100, 2))
        fake_inputs = netG(noise)
        fig = plt.figure(figsize=(1,1))
        # plot real data
        plt.scatter(X1[:,0].numpy(), X1[:,1].numpy())
        # plot fake data generated by G, need .detach to create a new view
        plt.scatter(fake_inputs[:,0].detach().numpy(), fake_inputs[:,1].detach().numpy())
        plt.axis('off')
        plt.show();
```
The result of training is shown in the following figure.
<center><img src="https://dingma129.github.io/assets/figures/pytorch/pytorch_4_1_train.png" width="400" ></center>
<span style="font-weight:bold;font-size:32px">3.4 Generate some fake data</span>

I will now use the trained generator to produce 5000 new fake datas.
```python
# load the trained generator (100 epochs)
netG1 = torch.load("model/gan1_epoch100")
noise = torch.randn((5000, 2))
fake_inputs = netG1(noise)
fig = plt.figure(figsize=(6,6))
plt.scatter(X1[:,0].numpy(), X1[:,1].numpy(),label="real")
plt.scatter(fake_inputs[:,0].detach().numpy(), fake_inputs[:,1].detach().numpy(),alpha=0.1,label="fake");
plt.legend();
```
It can be seen from the figure that all those fake datas are quite similar to the real datas.
<center><img src="https://dingma129.github.io/assets/figures/pytorch/pytorch_4_1_test.png" width="500" ></center>
---
<span style="font-weight:bold;font-size:36px">4. Example: MNIST</span>

In this example, I will build a GAN model to generate images of 5's that are similar to the ones in MNIST.

<span style="font-weight:bold;font-size:32px">4.1 Dataset</span>

I will only use the images of 5's from the MNIST dataset.
```python
import torchvision
import torchvision.transforms as transforms
# load data from torchvision as Tensor
transform = transforms.Compose([transforms.ToTensor()])
trainset = torchvision.datasets.MNIST(root='./data', train=True,download=True, transform=transform)
trainloader = DataLoader(trainset, batch_size=60000,shuffle=False)
# convert from Tensor to numpy array in order to filter the data
X_train = next(iter(trainloader))[0].numpy()
y_train = next(iter(trainloader))[1].numpy()
# only keep the images of 5, and rescale X into [0,1]
X_train_5 = X_train[y_train==5]/255.0
y_train_5 = np.ones((X_train_5.shape[0],1))
# create a DataLoader with batch_size=64
batch_size = 64
trainLoader5 = DataLoader(TensorDataset(torch.tensor(X_train_5,dtype=torch.float32), torch.tensor(y_train_5,dtype=torch.float32)), batch_size=batch_size, shuffle=True,drop_last=True)
```
<span style="font-weight:bold;font-size:32px">4.2 Models</span>

The models I created involve reshaping layers. There is no `Reshape` layer implemented in PyTorch. So instead of using `nn.Sequential`, I will use `nn.Module`.
```python
import torch.nn as nn
import torch.nn.functional as F

dim_latent = 32
# noise (-1,dim_latent) => fake_image (-1,1,28,28)
class NetG(nn.Module):
    def __init__(self):
        super(NetG,self).__init__()
        # layers
        self.fc1 = nn.Linear(dim_latent,32*14*14)
        self.convt1 = nn.ConvTranspose2d(32,32,(3,3),(2,2),padding=1,output_padding=1)
        self.conv1 = nn.Conv2d(32,1,3,padding=1)
        
    def forward(self,x):
        x = F.relu(self.fc1(x))  #(-1,dim_latent) => (-1,32*14*14)
        x = x.view(-1,32,14,14)  #(-1,32*14*14) => (-1,32,14,14)
        x = F.relu(self.convt1(x))  #(-1,32,14,14) => (-1,32,28,28) deconvolution
        x = torch.sigmoid(self.conv1(x))  #(-1,32,28,28) => (-1,1,28,28)
        return x    
    
# image (-1,1,28,28) => label (-1,1)
class NetD(nn.Module):
    def __init__(self):
        super(NetD,self).__init__()
        # layers
        self.conv1 = nn.Conv2d(1,8,3,padding=1)
        self.conv2 = nn.Conv2d(8,16,3,padding=1)
        self.fc1 = nn.Linear(16*7*7,32)
        self.fc2 = nn.Linear(32,8)
        self.fc3 = nn.Linear(8,1)
        
    def forward(self,x):
        x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))  #(-1,1,28,28) => (-1,8,14,14)
        x = F.max_pool2d(F.relu(self.conv2(x)),(2,2))  #(-1,8,14,14) => (-1,16,7,7)
        x = x.view(-1,16*7*7)
        x = F.relu(self.fc1(x))  # (-1,16*7*7) => (-1,32)
        x = F.relu(self.fc2(x))  # (-1,32) => (-1,8)
        x = torch.sigmoid(self.fc3(x))  # (-1,8) => (-1,1)
        return x
# instantiate NetG and NetD
netG = NetG()    
netD = NetD()
loss = nn.BCELoss()
# optimizers training G and D
trainerG = optim.RMSprop(netG.parameters(),lr=0.005)
trainerD = optim.RMSprop(netD.parameters(),lr=0.001)
# a default real/fake label for later use
real_label = torch.ones((batch_size,1),dtype=torch.float32)
fake_label = torch.zeros((batch_size,1),dtype=torch.float32)
```
It can be seen from the right part of the following figure that the generator before training does not generate anything close to a MNIST digit 5.
```python
fig,axes = plt.subplots(1,2,figsize=(8,4))
axes[0].axis("off")
axes[0].imshow(X_train_5[0].reshape(28,28),cmap='gray_r');
axes[1].axis("off")
axes[1].imshow(netG(torch.rand(2,dim_latent)).detach().numpy()[0].reshape(28,28),cmap='gray_r');
```
<center><img src="https://dingma129.github.io/assets/figures/pytorch/pytorch_4_2_data.png" width="300" ></center>
<span style="font-weight:bold;font-size:32px">4.3 Training</span>

The training is exactly the same as the one I provided in Section 3.3, so I will skip the codes here. The selected outputs from training are shown below.
<center><img src="https://dingma129.github.io/assets/figures/pytorch/pytorch_4_2_train1.png" width="600" ></center>
<center><img src="https://dingma129.github.io/assets/figures/pytorch/pytorch_4_2_train2.png" width="600" ></center>
<span style="font-weight:bold;font-size:32px">4.4 Generate some fake data</span>

I will now use the trained generator (26 epochs) to produce 36 new fake datas. They are all quite similar to MNIST digit 5. Training the models for more epochs will produce much better models.
```python
netG2 = torch.load("model/gan2_epoch26")
n_row = 6
noise = torch.randn((n_row**2, dim_latent))
fig,axes = plt.subplots(n_row,n_row,figsize=(4,4))
for i in range(n_row**2):
    axes.flatten()[i].axis("off")
    axes.flatten()[i].imshow(netG2(noise).detach().numpy()[i].reshape(28,28),cmap='gray_r');
plt.show();
```
<center><img src="https://dingma129.github.io/assets/figures/pytorch/pytorch_4_2_test.png" width="400" ></center>
By choosing a better stucture of generator and discriminator and training for a longer time, the images generated can be further improved. 
