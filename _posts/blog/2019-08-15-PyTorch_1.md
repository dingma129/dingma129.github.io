---
title: PyTorch (part 1)
author: Ding Ma
layout: post
categories: [blog]
---
[test](#1)

<span style="font-weight:bold;font-size:24px">accumulative</span>



<a name="1"></a><span style="font-weight:bold;font-size:24px">difference between `retain_graph` and `create_graph`</span>
The `backward` method of a tensor computes the gradient of current tensor w.r.t. graph leaves. There are two important parameters of `backward`, which are `retain_graph` and `create_graph`. According to the documentation, `retain_graph` defaults to the value of `create_graph`.

What are the differences between `retain_graph` and `create_graph`? In short,
* `retain_graph` allows you to re-compute gradient multiple times
* `create_graph` allows you to compute higher order gredient

Let us look at one example.

```python
# retain_graph=False
>>> x = torch.ones(1, requires_grad=True)
>>> y = x**7
>>> y.backward()
>>> print(x.grad)
tensor([7.])
>>> y.backward() # error, graph has been freed, cannot do .backward()
RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.

# retain_graph=True
>>> x = torch.ones(1, requires_grad=True)
>>> y = x**7
>>> y.backward(retain_graph=True)
>>> print(x.grad)
tensor([7.])
>>> y.backward()
>>> print(x.grad)
tensor([14.]) # x.grad is 7+7 (accumulated), this is not 2nd derivative

# create_graph=True  =>  retain_graph=True
>>> x = torch.ones(1, requires_grad=True)
>>> y = x**7
>>> y.backward(create_graph=True) # by default retain_graph=create_graph
>>> print(x.grad)
tensor([7.], grad_fn=<CloneBackward>)
>>> y.backward()
>>> print(x.grad)
tensor([14.], grad_fn=<CloneBackward>)
# what happens if we set (retain_graph=False,create_graph=True)?
# will get RuntimeError
```

In conclusion, if you need to compute first order gradient several times for some reason, set `retain_graph=True`. Don't set `create_graph=True` only when you really need to do, since it will cost more memory.




<span style="font-weight:bold;font-size:24px">difference between `backward` and `autograd.grad`</span>





