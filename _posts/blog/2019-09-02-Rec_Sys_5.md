---
title: "Recommender Systems - 5"
layout: splash
excerpt: "truncated SVD using Julia"
categories: [Julia]
tags: [Matrix Factorization, truncated SVD]

---

<span style="font-weight:bold;font-size:32px">0. Introduction</span>

It is well-known that Julia was designed from the beginning for [high performance](https://julialang.org/benchmarks/). In this blog, I will introduce how to implement truncated SVD in Julia, and compare the perfomances with `SVD++`.

I will first import the following necessary packages in Julia.
```julia
import CSV
using LinearAlgebra,BenchmarkTools,Random, Statistics, Arpack, DataFrames
```

<span style="font-weight:bold;font-size:32px">1. SVD in Julia</span>

In addition to multi-dimensional arrays, Julia provides native implementations of many common and useful linear algebra operations which can be loaded with `using LinearAlgebra`. In particular, its `svd` function computes the sigular value decomposition (SVD) of a matrix. The `Arpack` package provides bindings to ARPACK, which can also be used to perform SVD (using `svds`). For large, sparse matrices, `Arpack.svds` is usually much faster than `Linearalgebra.svd`.

Let's look at a toy example.
```julia
# Matrix is just an alias for Array{T,2}
A = Matrix([1 2 3;4 5 6;7 8 9])
# svd(A) returns a SVD object F with F.U, F.S, F.V and F.Vt
A_svd = svd(A)
A_svd.U * Diagonal(A_svd.S) * A_svd.Vt
#3×3 Array{Float64,2}:
# 1.0  2.0  3.0
# 4.0  5.0  6.0
# 7.0  8.0  9.0

# for truncated svd, use slicing operator
k = 1
A_svd.U[:,1:k] * Diagonal(A_svd.S[1:k]) * A_svd.Vt[1:k,:]
#3×3 Array{Float64,2}:
# 1.73622  2.07174  2.40727
# 4.20715  5.02019  5.83322
# 6.67809  7.96863  9.25917
```

---
<span style="font-weight:bold;font-size:32px">2. An old example revisited</span>

In this section, I will show how to perform one iteration of the algorithm of tuncated SVD for imcomplete matrices. The example I used here is from the end of [Recommender Systems - 2](/python/Rec_Sys_2/).

```julia
A_raw = Matrix(Union{Missing, Float64}[1 -1 1 -1 1 -1;
        1 1 missing -1 -1 -1;
        missing 1 1 -1 -1 missing;
        -1 -1 -1 1 1 1;
        -1 missing -1 1 1 1])

# pre-scan missing locations
toComplete = []
for i in 1:size(A_raw,1), j in 1:size(A_raw,2)
    if ismissing(A_raw[i,j]) push!(toComplete,[i,j]) end
end
println(toComplete)
# Any[[2, 3], [3, 1], [3, 6], [5, 2]]

# compute row_mean
row_mean = []
for i in 1:size(A_raw,1)
    push!(row_mean,mean(skipmissing(A_raw[i,:])))
end
println(row_mean)
# Any[0.0, -0.2, 0.0, 0.0, 0.2]

# initialization: imputation using row means
# copy(A_raw) => don't want to mess up A_raw
A = copy(A_raw)
A = Matrix{Union{Missing, Float64}}([(ismissing(A_raw[i,j])) ? row_mean[i] : A_raw[i,j] for i in 1:size(A_raw,1), j in 1:size(A_raw,2)]);
A
#5×6 Array{Union{Missing, Float64},2}:
#  1.0  -1.0   1.0  -1.0   1.0  -1.0
#  1.0   1.0  -0.2  -1.0  -1.0  -1.0
#  0.0   1.0   1.0  -1.0  -1.0   0.0
# -1.0  -1.0  -1.0   1.0   1.0   1.0
# -1.0   0.2  -1.0   1.0   1.0   1.0

# Step 1: truncated SVD of rank 2
k = 2
Asvd1 = svd(A)
A_tmp = round.(Asvd1.U[:,1:k] * Diagonal(Asvd1.S[1:k]) * Asvd1.Vt[1:k,:],digits=2)  # round to 2 decimals for simplicity
A_tmp
#5×6 Array{Float64,2}:
#  1.06  -1.16   0.97  -0.85   0.8   -1.06
#  0.66   0.9    0.59  -0.92  -1.12  -0.66
#  0.43   0.96   0.38  -0.69  -1.1   -0.43
# -0.94  -0.82  -0.84   1.2    1.13   0.94
# -1.03  -0.21  -0.93   1.15   0.55   1.03

# Step 2: replacing imputed entries using the result of Step 1
A = Matrix([(ismissing(A_raw[i,j])) ? A_tmp[i,j] : A_raw[i,j] for i in 1:size(A_raw,1), j in 1:size(A_raw,2)])
A
#5×6 Array{Float64,2}:
#  1.0   -1.0    1.0   -1.0   1.0  -1.0 
#  1.0    1.0    0.59  -1.0  -1.0  -1.0 
#  0.43   1.0    1.0   -1.0  -1.0  -0.43
# -1.0   -1.0   -1.0    1.0   1.0   1.0 
# -1.0   -0.21  -1.0    1.0   1.0   1.0 
```
We can see that 0.59, 0.43, -0.43, -0.21are exactly the four numbers after the first iteration in the original blog.

---
<span style="font-weight:bold;font-size:32px">3. Implementaion in Julia</span>

In this section, I will implement truncated SVD for imcomplete matrices using both `LinearAlgebra.svd` and `Arpack.svds`. Later we will see that the second one using Lanczos/Arnoldi algorithm is much faster.

```julia
# method 1: using LinearAlgebra.svd
function truncsvd1(A, k, epoch)
    for e in 1:epoch
        Asvd = svd(A)
        A_tmp = Asvd.U[:,1:k] * Diagonal(Asvd.S[1:k]) * Asvd.Vt[1:k,:]
        A = Matrix([(ismissing(A_train[i,j])) ? A_tmp[i,j] : A_train[i,j] for i in 1:size(A_raw,1), j in 1:size(A_raw,2)])
    end
    Asvd = svd(A)
    A_tmp = Asvd.U[:,1:k] * Diagonal(Asvd.S[1:k]) * Asvd.Vt[1:k,:]
    return A_tmp
end
# method 2: using Arpack.svds
function truncsvd2(A, k, epoch)
    for e in 1:epoch
        Asvd = svds(A,nsv=k)[1]
        A_tmp = Asvd.U[:,1:k] * Diagonal(Asvd.S[1:k]) * Asvd.Vt[1:k,:]
        A = Matrix([(ismissing(A_train[i,j])) ? A_tmp[i,j] : A_train[i,j] for i in 1:size(A_raw,1), j in 1:size(A_raw,2)])
    end
    Asvd = svd(A)
    A_tmp = Asvd.U[:,1:k] * Diagonal(Asvd.S[1:k]) * Asvd.Vt[1:k,:]
    return A_tmp
end
```

---
<span style="font-weight:bold;font-size:32px">4. Performances on MovieLens 100k</span>

Now let's look at the model performaces on the famous MovieLens 100k Dataset. Let's first preprocess the data into an incomplete matrix form as before.
```julia
# load dataset
df_raw = CSV.read("/data/mlen/u.data",
header=["userId", "itemId", "rating", "timestamp"],datarow=1);
# pivot table
df_raw_p = unstack(df_raw,:userId,:itemId,:rating)
# remove the 1st column, which is userId column
select!(df_raw_p, Not(:userId));
# transfer into Matrix, also change from Int64 to Float64
A_raw = Matrix{Union{Missing, Float64}}(df_raw_p);
Random.seed!(42)
# W: mask used to do train/test split
W = rand(1:10,size(A_raw))
# train/test split 
# train: W-value 1-8, test: W-value 9-10
A_train = Matrix([(~ismissing(A_raw[i,j]) && W[i,j] <= 8) ? A_raw[i,j] : missing for i in 1:size(A_raw,1), j in 1:size(A_raw,2)]);
A_test = Matrix([(~ismissing(A_raw[i,j]) && W[i,j] > 8) ? A_raw[i,j] : missing for i in 1:size(A_raw,1), j in 1:size(A_raw,2)]);
# pre-scan missing locations in A_train
toComplete = []
for i in 1:size(A_train,1), j in 1:size(A_train,2)
    if ismissing(A_train[i,j]) push!(toComplete,[i,j]) end
end
# compute row_mean
row_mean = []
for i in 1:size(A_train,1)
    push!(row_mean,mean(skipmissing(A_train[i,:])))
end
```
Now let's compare the runtimes of the above two implementations using `BenchmarkTools`. Both models are running with k=2 and epoch=10.
```julia
# model 1: using LinearAlgebra.svd
# initialization
A = copy(A_train)
A = Matrix{Union{Missing, Float64}}([(ismissing(A_train[i,j])) ? row_mean[i] : A_train[i,j] for i in 1:size(A_train,1), j in 1:size(A_train,2)]);
@benchmark A_pred1 = truncsvd1(A,2,10)
#BenchmarkTools.Trial: 
#  memory estimate:  1.71 GiB
#  allocs estimate:  53279099
#  --------------
#  minimum time:     17.567 s (6.73% GC)
#  median time:      17.567 s (6.73% GC)
#  mean time:        17.567 s (6.73% GC)
#  maximum time:     17.567 s (6.73% GC)
#  --------------
#  samples:          1
#  evals/sample:     1

# model 2: using Arpack.svds
# initialization
A = copy(A_train)
A = Matrix{Union{Missing, Float64}}([(ismissing(A_train[i,j])) ? row_mean[i] : A_train[i,j] for i in 1:size(A_train,1), j in 1:size(A_train,2)]);
@benchmark A_pred2 = truncsvd2(A,2,10)
#BenchmarkTools.Trial: 
#  memory estimate:  1.23 GiB
#  allocs estimate:  53280882
#  --------------
#  minimum time:     4.590 s (25.73% GC)
#  median time:      4.591 s (24.78% GC)
#  mean time:        4.591 s (24.78% GC)
#  maximum time:     4.592 s (23.83% GC)
#  --------------
#  samples:          2
#  evals/sample:     1
```
We can also get RMSE and MAE of these 2 models.
```julia
# model 1: using LinearAlgebra.svd
println("RMSE(train):\t$(√(mean(skipmissing((A_train-A_pred1).^2))))")
println("RMSE(test):\t$(√(mean(skipmissing((A_test-A_pred1).^2))))")
println("MAE(train):\t$(mean(skipmissing(abs.(A_train-A_pred1))))")
println("MAE(test):\t$(mean(skipmissing(abs.(A_test-A_pred1))))")
#RMSE(train):	0.8731970048484063
#RMSE(test):	0.922738191787703
#MAE(train):	0.686203275541329
#MAE(test): 	0.7255879245503194


# model 2: using Arpack.svds
println("RMSE(train):\t$(√(mean(skipmissing((A_train-A_pred2).^2))))")
println("RMSE(test):\t$(√(mean(skipmissing((A_test-A_pred2).^2))))")
println("MAE(train):\t$(mean(skipmissing(abs.(A_train-A_pred2))))")
println("MAE(test):\t$(mean(skipmissing(abs.(A_test-A_pred2))))")
#RMSE(train):	0.8704376785373865
#RMSE(test):	0.9216312655772986
#MAE(train):	0.683980611049689
#MAE(test): 	0.7246053852079253
```

The RMSE and MAE here is almost the same as the `Surprise` implementation of `SVD++`, but much faster. The following is the performance of the `Surprise` implementation of `SVD++` adopted from the original blog.
```
Evaluating RMSE, MAE of algorithm SVDpp on 5 split(s).

                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     
RMSE (testset)    0.9202  0.9186  0.9295  0.9198  0.9156  0.9207  0.0047  
MAE (testset)     0.7240  0.7217  0.7329  0.7254  0.7195  0.7247  0.0046  
RMSE (trainset)   0.8685  0.8703  0.8726  0.8709  0.8616  0.8688  0.0038  
MAE (trainset)    0.6848  0.6856  0.6881  0.6864  0.6788  0.6847  0.0032  
Fit time          108.29  117.24  108.81  112.99  110.33  111.53  3.29    
Test time         4.76    4.96    4.34    5.29    4.70    4.81    0.31  
```

