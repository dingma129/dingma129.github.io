---
title: ANOVA
author: Ding Ma
layout: post
categories: [blog]
---
<span style="font-weight:bold;font-size:36px">0. introduction</span>

In this blog, I will discuss about <span style="font-weight:bold">Analysis of Variance (ANOVA)</span>. 

ANOVA is a form of statistical hypothesis testing heavily used in the analysis of experimental data. A test result (calculated from the null hypothesis and the sample) is called statistically significant if it is deemed unlikely to have occurred by chance, assuming the truth of the null hypothesis. A statistically significant result, when a probability (p-value) is less than a pre-specified threshold (significance level), justifies the rejection of the null hypothesis, but only if the a priori probability of the null hypothesis is not high.

---

<span style="font-weight:bold;font-size:36px">1. significance of parameters of a linear model</span>

Consider the following linear regression model

<center><img src="https://latex.codecogs.com/png.latex?f(X)=\beta_0+\sum_{j=1}^{p} X_j\beta_j,"/> </center>
with a training set of size N. We want to know that whether all the coefficients <img src="https://latex.codecogs.com/png.latex?\beta_j"/> are significant or not. In other words, we want to know whether we can replace this linear model by a smaller model.



<span style="font-weight:bold;font-size:32px">1.1 test one parameter</span>

<span style="font-weight:bold"><u>null hypothesis:</u></span> <img src="https://latex.codecogs.com/png.latex?\beta_j=0"/>

Under this null hypothesis, we can compute Z-score (this is included in the next case, so we will skip the definition here), which is distributed as <img src="https://latex.codecogs.com/png.latex?t_{N-p-1}"/>.
In this case, a large absolute value of Z-score will lead to rejection of this null hypothesis.



<span style="font-weight:bold;font-size:32px">1.2 test a group of parameters</span>

<span style="font-weight:bold"><u>null hypothesis:</u></span> <img src="https://latex.codecogs.com/png.latex?\beta_{j_1}=\beta_{j_2}=\cdots=\beta_{j_k}=0"/> (smaller model is correct)

Under this null hypothesis, we can compute the F-statistic
<center><img src="https://latex.codecogs.com/png.latex?F=\frac{(RSS_0-RSS_1)/(p_1-p_0)}{RSS_1/(N-p_1-1)},"/> </center>
where <img src="https://latex.codecogs.com/png.latex?RSS_0,RSS_1"/> is the residual sum of squares of the smaller, bigger model of <img src="https://latex.codecogs.com/png.latex?p_0+1,p_1+1"/> parameters (including constant). 
Under the Gaussian assumptions about error distribution, and the above null hypothesis, this F-statistic will have a <img src="https://latex.codecogs.com/png.latex?F_{p_1-p_0,N-p_1-1}"/> distribution. (In particular, when k=1, F-statistic is the square of the Z-score above.)

In this case, a large value of F-statistic will lead to rejection of this null hypothesis.

---

<span style="font-weight:bold;font-size:36px">2. examples</span>

<span style="font-weight:bold;font-size:32px">2.1Â Prostate Cancer Data (testing main terms only using ANOVA)</span>

We will use Prostate Cancer dataset from [here](http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/prostate.data). This data set contains 67 training examples and 30 test examples.
<center><img src="https://dingma129.github.io/assets/figures/ESL/pcancer_head.png" width="600"></center>
The task is to predict `lpsa`(the log of prostate-specific antigen) using the other features. After normalizing the input data using `sklearn.preprocessing.StandardScaler`, we can fit 2 linear models with the first one using all features and the second one using all features except `gleason` .

```python
import statsmodels.api as sm
# bigger model
model1 = sm.OLS.from_formula('lpsa ~ lcavol + lweight + age + lbph + svi + lcp + gleason + pgg45', data=pd.concat([X_train_scl,y_train],axis=1)).fit()
# smaller model
model0 = sm.OLS.from_formula('lpsa ~ lcavol + lweight + age + lbph + svi + lcp + pgg45', data=pd.concat([X_train_scl,y_train],axis=1)).fit()
```
Under the null hypothesis that the coefficient of `gleason` is 0, we can compute the F-statistic using the above formula.
```python
from sklearn.metrics import mean_squared_error
# bigger model
rss1 = mean_squared_error(y_train, model1.predict(X_train_scl))
# smaller model
rss0 = mean_squared_error(y_train, model0.predict(X_train_scl))
fs = ((rss0-rss1)/(1))/(rss1/(67-8-1))
print("F-value:\t{}".format(fs))
print("p-value:\t{}".format(sp.stats.f.sf(fs, 1 , 58)))

# output
# F-value:	0.02151537632377584   # ~ F(1,58)
# p-value:	0.8838923143371842
```

<span style="font-weight:bold;font-size:28px">Obtain the same result using ANOVA</span>

We can also get the same result by using

```python
# to test main terms, set "typ = 2"
sm.stats.anova_lm(model1, test='F',typ=2)
```
<center><img src="https://dingma129.github.io/assets/figures/ESL/pcancer_anova.png" width="350"></center>
Using this method, we can get all F-statistics (hence testing the null hypothesis of any single coefficient being 0) in one line.

In this example, the `p-value = 0.8838923143371842` is greater than 0.05. So the `gleason` parameter is not significant in this linear model.

<span style="font-weight:bold;font-size:28px">How to remove more terms?</span>

If we want to remove more non-significant parameters, we can either remove them one by one using `sm.stats.anova_lm`, or we can compute F-statistic in Section 1.2 on a group of parameters. For example, to test whether we can remove `gleason`, `age`, `lcp`, `pgg45`, we get

```python
F-value:	1.3358039077100192   # ~ F(4,58)
p-value:	0.2675810323215007
```

So we can remove them all to get a smaller model which is not significantly different from the orginal bigger model.



<span style="font-weight:bold;font-size:32px">2.2 Bone Data (testing main terms and interactions using ANOVA)</span>

We will use Bone dataset from [here](https://web.stanford.edu/~hastie/ElemStatLearn//datasets/bone.data). This data set contains 485 examples.

<center><img src="https://dingma129.github.io/assets/figures/ESL/bone_head.png" width="300"></center>
The task is to predict `spnbmd`(relative spinal bone mineral density measurement) using the other features.

In this example, we include also the interaction term between `age` and`gender`.

```python
from sklearn.model_selection import train_test_split
bone = pd.read_csv('data/bone.data',sep='\t')
bone_train,bone_test= train_test_split(bone, test_size=0.2, random_state=42)
# to test interaction terms, set "typ = 1"
model1 = sm.OLS.from_formula('spnbmd ~ age + gender + age:gender ',data=bone_train).fit()
sm.stats.anova_lm(model1, test='F',typ=1)
```

<center><img src="https://dingma129.github.io/assets/figures/ESL/bone_anova_interaction.png" width="600"></center>
We can see that the intersection term `age:gender` has p-value `2.133763e-02 < 0.05`. So this interaction term is significant.

Next, we want to increase the model complexity using natural spline (instead of polynomial features).

```python
bic_list = []
for k in range(3,15):
    model2 = sm.OLS.from_formula('spnbmd ~ cr(age, df = {}) + gender + age:gender '.format(k),data=bone_train).fit()
    bic_list.append(model2.bic)

# best k = 5
plt.figure(figsize=(6,4))
plt.plot(range(3,15),bic_list,marker='.')
plt.xticks(ticks=range(3,15))
plt.ylabel("BIC");
```

<center><img src="https://dingma129.github.io/assets/figures/ESL/bone_bic.png" width="600"></center>
So we should choose `df = 5` for `age`. 

<center><img src="https://dingma129.github.io/assets/figures/ESL/bone_plot.png" width="700"></center>
If we plot the model prediction along with 95% confidence intervals as above, we can see that there is a significant difference between male and female, which also justifies that the interaction term `age:gender` is significant.