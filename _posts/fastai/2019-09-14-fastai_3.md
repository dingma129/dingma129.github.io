---
title: "FastAI - 3"
layout: splash
excerpt: "Dogs vs Cats: vision.transform"
categories: [Python]
tags: [PyTorch, FastAI, CNN, Classification, Augmentation, One Cycle Policy, Transfer Learning, ResNet, Computer Vision]
---

# 0. Introduction

In this blog, I will introduce the idea of data augmentation and one cycle policy of training. I will use the [Dogs vs Cats dataset](https://www.kaggle.com/c/dogs-vs-cats), which previously is a Kaggle competition.

In `fastai`, the dataset contains a training set of 23000 images, a validation set of 2000 images and a test set of 12500 images. All those images are of various sizes, which is different from the case in MNIST, in which all images are of size 28x28. 

Therefore, we first need to resize all the images into a uniform size, here I will choose 224x224.

# 1. Prepare data

## 1.1 Two ways of loading data

First, let's download the dataset using `fastai`, and check the structure of the dataset folders.
```python
from fastai import *
from fastai.vision import *
dogscats = untar_data(URLs.DOGS)
dogscats
# PosixPath('/root/.fastai/data/dogscats')
!ls /root/.fastai/data/dogscats/
# sample	test1  train  valid
```

I will load the datasets in two ways: with transforms and without transform (resize only). 

```python
# without transform
data_raw = ImageDataBunch.from_folder(
    dogscats,ds_tfms=([],[]),seed=42,
    train="train", valid="valid", test="test1",
    bs=32,size=224,resize_method = ResizeMethod.CROP).normalize(cifar_stats)

# with transforms
data_trans = ImageDataBunch.from_folder(
    dogscats,ds_tfms=get_transforms(),seed=42,
    train="train", valid="valid", test="test1",
    bs=32,size=224,resize_method = ResizeMethod.CROP).normalize(cifar_stats)
```

## 1.2 Sizes of data

If we use `data_raw.train_ds.x`, we will get the original sizes of each image. For example,
```python
for i in range(5):
  print(data_raw.train_ds.x[i].shape)
# torch.Size([3, 375, 499])
# torch.Size([3, 399, 273])
# torch.Size([3, 499, 252])
# torch.Size([3, 376, 399])
# torch.Size([3, 500, 499])

# the type will be Image in this case
type(data_raw.train_ds.x[0])
# fastai.vision.image.Image 
```

If we use `data_raw.one_batch()`, we will get one batch of resized data.
```python
data_raw.one_batch()[0].shape
# torch.Size([32, 3, 224, 224])

# the type will be Tensor in this case
type(data_raw.one_batch()[0][0])
# torch.Tensor
```

## 1.3 Effect of transforms

The `get_transforms()` method is a 2-tuple of list of transforms, in which the first entry is a list of useful transforms of regular photos and the second entry is crop only. When we use the `ds_tfms` in `ImageDataBunch.from_*` factory method, it requires a 2-tuple of list of transforms, where the first entry transforms training data and the second entry transforms validation data.

Let's take a look at the transforms provided by `get_transforms()`
```python
get_transforms()
# train: a lof of useful transforms
#([RandTransform(tfm=TfmCrop (crop_pad), kwargs={'row_pct': (0, 1), 'col_pct': (0, 1), 'padding_mode': 'reflection'}, p=1.0, resolved={}, do_run=True, is_random=True, use_on_y=True),
#  RandTransform(tfm=TfmPixel (flip_lr), kwargs={}, p=0.5, resolved={}, do_run=True, is_random=True, use_on_y=True),
#  RandTransform(tfm=TfmCoord (symmetric_warp), kwargs={'magnitude': (-0.2, 0.2)}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True),
#  RandTransform(tfm=TfmAffine (rotate), kwargs={'degrees': (-10.0, 10.0)}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True),
#  RandTransform(tfm=TfmAffine (zoom), kwargs={'scale': (1.0, 1.1), 'row_pct': (0, 1), 'col_pct': (0, 1)}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True),
#  RandTransform(tfm=TfmLighting (brightness), kwargs={'change': (0.4, 0.6)}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True),
#  RandTransform(tfm=TfmLighting (contrast), kwargs={'scale': (0.8, 1.25)}, p=0.75, resolved={}, do_run=True, is_random=True, use_on_y=True)],
# valid: crop only 
# [RandTransform(tfm=TfmCrop (crop_pad), kwargs={}, p=1.0, resolved={}, do_run=True, is_random=True, use_on_y=True)])
```

Let's look at the effect of those transforms. We will just work on the first image in the training data.
```python
# type Image
img1 = data_raw.train_ds.x[0]
type(img1)
# fastai.vision.image.Image
```
* rotation + flipping:
```python
# random rotation with degree between -45 and 45
# and horizontal flipping with probability 0.5
tfm = [rotate(degrees=(-45,45)),flip_lr(p=0.5)]
fig, axes = plt.subplots(1,5,figsize=(15,10))
for i in range(5):
  # need to run the transform before finding the parameters
  img = img1.apply_tfms(tfm)
  title = "deg={:.2f},flip={}".format(tfm[0].resolved['degrees'],tfm[1].do_run)
  img.show(ax=axes[i],title=title)
```
<center><img src="/assets/figures/fastai/3_trans1.png" width="1000" ></center>
We can see that each image is transformed differently.

* `get_transforms()`:
```python
# get_transforms() are pretty good for regular photos
# it's a 2-tuple containing transforms for train/validation
# train: a lot of transforms
# valid: only crop
tfm = get_transforms()
fig, axes = plt.subplots(2,5,figsize=(15,6))
for i in range(5):
  img1.apply_tfms(tfm[0]).show(ax=axes[0,i])
  img1.apply_tfms(tfm[1]).show(ax=axes[1,i])
```
<center><img src="/assets/figures/fastai/3_trans2.png" width="1000" ></center>
The first row corresponds to `get_transforms()[0]`, which applies a series of random tranforms. The second row corresponds to `get_transforms()[1]`, which is cropping only. Therefore, all the images in the first row look different, while all the images in the second row are exactly the same.

# 2. Train models

In this section, I will train two `ResNet34` models, one on the raw data and one on the transformed data. I will use one cycle policy to determine the learning rate, and train both models for just 1 epoch.

As in the last blog, I will use `cnn_learner` method from `fastai.vision` to create two `Learner` objects, one for `data_raw` and one for `data_trans`.
```python
learn_raw = cnn_learner(data_raw, models.resnet34, metrics=accuracy)
learn_trans = cnn_learner(data_trans, models.resnet34, metrics=accuracy)
```

`Learner.lr_find` will launch an LR range test that will help you select a good learning rate.
```python
# set two bounds for lr: 1e-5 and 1
learn_raw.lr_find(start_lr=1e-5,end_lr=1,num_it=100);
learn_raw.recorder.plot()
```
<center><img src="/assets/figures/fastai/3_lrfind.png" width="600" ></center>
we should choose the learning rate a bit smaller than the learning rate where loss is minimum. For this value, the loss is still decreasing. From the above figure, `1e-2` would be a good choice of learning rate, since `5e-2` is where the loss is minimal. 

Now let's train both models with learning rate `1e-2` using one cycle policy.
```python
learn_raw.fit_one_cycle(1,max_lr=1e-2)
```
<center><img src="/assets/figures/fastai/3_trainraw.png" width="600" ></center>
```python
learn_trans.fit_one_cycle(1,max_lr=1e-2)
```
<center><img src="/assets/figures/fastai/3_traintrans.png" width="600" ></center>
Comparing to the `Learner` without transform, the `Learner` with transforms got lower validation loss and higher accuracy in a longer training time.

Let's check the changes of learning rate during this epoch.
```python
learn_trans.recorder.plot_lr(show_moms=True)
```
<center><img src="/assets/figures/fastai/3_lrchange.png" width="800" ></center>
We can see that at the beginning and the end of the epoch, we have a lower learning rate and a higher momentum. While during the middle, we have a higher learning rate and a lower momentum. This is the so-called one cycle policy. 

# 3. Evaluations

Since this is a classification model, we can use `ClassificationInterpretation` to get a lot of information.

```python
preds,y,losses = learn_trans.get_preds(with_loss=True)
interp = ClassificationInterpretation(learn_trans, preds, y, losses)
```

For example, we can get confusion matrix as follows.

```python
interp.plot_confusion_matrix()
```
<center><img src="/assets/figures/fastai/3_cm.png" width="800" ></center>

We can also get some images with highest losses.
```python
interp.plot_top_losses(9, figsize=(12,12))
```
<center><img src="/assets/figures/fastai/3_toploss.png" width="1000" ></center>

There is no cat and dog in `(0,4)` and `(1,3)`. The faces were cropped in `(0,2)`, `(2,1)`, `(2,2)`, `(4,0)`, `(4,1)`, `(4,3)` and `(4,4)`.