---
title: a faster replacement of pandas.DataFrame.to_sql
author: Ding Ma
layout: post
categories: [blog]
---

<span style="font-weight:bold;font-size:36px">0. Introduction</span>

When a `pandas.DataFrame` is really large, using its `.to_sql` method usually costs a lot of time. In this blog, I will introduce a much faster replacement using `io.StringIO`.

---
<span style="font-weight:bold;font-size:36px">1. Setup</span>

<span style="font-weight:bold;font-size:32px">1.1 DataFrame</span>

I will use the same pandas DataFrame as the one I used in the blog of [<span style="color:blue">Bokeh (part 2)</span>](https://dingma129.github.io/blog/2019/08/12/Bokeh_2.html). Its shape is `(10044,7)`, and it looks like the following
<center><img src="https://dingma129.github.io/assets/figures/blog/bokeh_2_data.png" width="600" ></center>

<span style="font-weight:bold;font-size:32px">1.2 SQL DataBase</span>

I will use a local PostgreSQL Database.
```python
from sqlalchemy import create_engine
# create a new engine
engine = create_engine('postgresql+psycopg2://postgres:{}@localhost/'.format(password))
# create a new database
with engine.connect() as conn:
    conn.execute("commit")
    conn.execute("CREATE DATABASE gapminder")
    conn.close()
# connect to gapminder database
engine = create_engine('postgresql+psycopg2://postgres:{}@localhost/gapminder'.format(password))
```
---
<span style="font-weight:bold;font-size:36px">2. Two Methods</span>
```python
# using pandas.DataFrame.to_sql
def to_sql_pd():
    df.to_sql('data', con=engine,if_exists='append')
    return None
from io import StringIO
# using io.StringIO
def to_sql_stringio():
    output = StringIO()
    df.to_csv(output,sep='\t',index=True,header=False)
    output.getvalue()
    output.seek(0)
    connection = engine.raw_connection()
    cursor = connection.cursor()
    cursor.copy_from(output,"data",null="")
    connection.commit()
    cursor.close()
    return None
```
---
<span style="font-weight:bold;font-size:36px">3. Comparison</span>

We run the program
```python
import timeit
n_times = 5
print("pandas:")
print("average timed used:\t{}s".format(timeit.timeit(lambda: to_sql_pd(), number=n_times)/n_times))
# make sure having correct number of rows
assert(engine.execute("select count(*) from data").fetchall()[0][0]==10044*5)
print("StringIO:")
print("average timed used:\t{}s".format(timeit.timeit(lambda: to_sql_stringio(), number=n_times)/n_times))
# make sure having correct number of rows
assert(engine.execute("select count(*) from data").fetchall()[0][0]==10044*10)
```
<center><img src="https://dingma129.github.io/assets/figures/blog/sql_compare_1.png" width="600" ></center>

If we run those two methods on the same datasets of sizes 10,100,1000,10000,100000 and take average run times over 5 runs, we get the following table
<center><img src="https://dingma129.github.io/assets/figures/blog/sql_compare_2.png" width="300" ></center>

We can see that this new method is much faster than `pandas.DataFrame.to_sql`.