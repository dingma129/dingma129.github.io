---
title: "Recommender Systems - 4"
layout: splash
excerpt: "Implementation of ALS, SVD and SVD++ Using PyTorch"
categories: [Python]
tags: [Recommender System, PyTorch, Surprise, Matrix Factorization, ALS, SVD, SVD++, TensorBoard]

---

<span style="font-weight:bold;font-size:32px">0. Introduction</span>

In this blog, I will implement ALS, SVD (with baselines) and SVD++  using PyTorch. I will use them on the famous MovieLens 100K Dataset. Those 3 methods have already been discussed in previous blog [<span style="color:blue">here</span>](/python/Rec_Sys_2/). For simplicity, all matrix factorization would be R=UV instead of R=UV^T during this blog.

---
<span style="font-weight:bold;font-size:32px">1. ALS</span>

<span style="font-weight:bold;font-size:28px">1.1 ALS for complete matrices</span>

For complete matrices, we can use `torch.lstsq` to solve the least-squares problems directly without using optimization methods. 
<center><img src="https://dingma129.github.io/assets/figures/rec/rec_4_als1.png" width="1000" ></center>
```python
import numpy as np
import torch
def completeALS(X,k=5,epochs=10):
    n_rows,n_cols = X.shape
    # losses
    loss_list = []
    # random initialization
    U = torch.randn((n_rows,k),dtype=torch.float32)
    # repeating
    for epoch in range(epochs):
        # solve for V, keep only the first k rows
        V = torch.lstsq(X,U)[0][:k,:]
        # solve for U, keep only the first k rows
        U = torch.lstsq(X.T,V.T)[0][:k,:].T
        # record RMSE
        loss_list.append(torch.sqrt((U@V-X).pow(2).mean()).item())
    # return a detached cloned version of U and V and a list of losses
    return U.detach().clone(),V.detach().clone(),loss_list
```

<span style="font-weight:bold;font-size:28px">1.2 ALS for incomplete matrices</span>

For incomplete matrices, we do not have a function to use directly. So we need to use an optimizer to update U and V. The full idea of the implementation is explained in the following figure. By using the boolean mask for train, we can compute the train loss only over the observed ratings using `torch`'s methods only. The boolean mask for test is used when recording the test loss.
<center><img src="https://dingma129.github.io/assets/figures/rec/rec_4_als2.png" width="1000" ></center>
```python
def incompleteALS(X, k=5, weights_train=weights_train, weights_test=weights_test,
                    optimizer=torch.optim.SGD, lr=0.01,epochs=20,reg=0.0,tol=1e-4):
    n_rows,n_cols = X.shape
    # size of train/test, used to compute mean
    n_observed = weights_train.sum()
    n_unobserved = weights_test.sum()
    # record loss of train/test
    train_loss_list = []
    test_loss_list = []
    # random initialization, normal distribution with mean=0 and std=0.1
    U = (torch.randn((n_rows,k),dtype=torch.float32)*0.1).clone().requires_grad_(True)
    V = (torch.randn((k,n_cols),dtype=torch.float32)*0.1).clone().requires_grad_(True)
    # 2 optimizers for U and V
    optimizer_U = optimizer([U], lr=lr)
    optimizer_V = optimizer([V], lr=lr)
    # repeating
    for epoch in range(epochs):
        # using optimizer to find the V minimizing loss, until the improvement < tol
        last_loss = -1
        while True:
            # clean the gradients
            optimizer_V.zero_grad()
            # only learning V, so detach U
            X_pred = U.detach()@V
            # mean MSE + regularization terms
            loss = ((X_pred-X)*weights_train).pow(2).sum()/n_observed + reg * V.pow(2).mean()
            loss.backward()
            optimizer_V.step()
            # if does not improve > tol, quit the loop
            if torch.abs(last_loss-loss)<tol:
                break
            else:
                last_loss = loss.item()
        # do the same for U
        last_loss = -1
        while True:
            optimizer_U.zero_grad()
            X_pred = U@V.detach()
            loss = ((X_pred-X)*weights_train).pow(2).sum()/n_observed + reg * U.pow(2).mean()
            loss.backward()
            optimizer_U.step()
            if torch.abs(last_loss-loss)<tol:
                break
            else:
                last_loss = loss.item()
        # record losses
        train_loss_list.append(torch.sqrt(((X_pred-X)*weights_train).pow(2).sum()/n_observed).item())
        test_loss_list.append(torch.sqrt(((X_pred-X)*weights_test).pow(2).sum()/n_unobserved).item())
    # return a detached cloned version of U and V
    return U.detach().clone(),V.detach().clone(),train_loss_list,test_loss_list
```

---
<span style="font-weight:bold;font-size:32px">2. SVD</span>

In this section, I will only implement SVD with/without baselines for incomplete matrices. The models for complete matrices are just special cases of it. Again, as in the above implementation of ALS, I used two boolean masks for train/test datas.

```python
def incompleteSVD(X, k=5, weights_train=weights_train, weights_test=weights_test,
                    baseline = True,optimizer=torch.optim.SGD, l2=True, reg=0.0, lr=0.1, epochs=200):
    n_rows,n_cols = X.shape
    # used to compute mean
    n_observed = weights_train.sum()
    n_unobserved = weights_test.sum()
    # record loss of train/test
    train_loss_list = []
    test_loss_list = []
    # mu set to be 
    mu = (X*weights_train).sum()/n_observed
    # random initialization
    U = (torch.randn((n_rows,k),dtype=torch.float32)*0.1).clone().requires_grad_(True)
    V = (torch.randn((k,n_cols),dtype=torch.float32)*0.1).clone().requires_grad_(True)
    bu = torch.randn((n_rows,1),dtype=torch.float32,requires_grad=True)
    bv = torch.randn((1,n_cols),dtype=torch.float32,requires_grad=True)
    # specified optimizer with learning rate = lr
    if baseline:
        optimizer = optimizer([U,V,bu,bv], lr=lr)
    else:
        optimizer = optimizer([U,V], lr=lr)
    # training
    for epoch in range(epochs):
        # clear all gradients
        optimizer.zero_grad()
        # with baseline
        if baseline:
            X_pred = mu+bu+bv+U@V
            # compute loss function with regularization
            loss = ((X_pred-X)*weights_train).pow(2).sum()/n_observed + reg * U.pow(2).mean() + reg * V.pow(2).mean() + reg * bu.pow(2).mean() + reg * bv.pow(2).mean()
        # without baseline
        else:
            X_pred = U@V
            loss = ((X_pred-X)*weights_train).pow(2).sum()/n_observed + reg * U.pow(2).mean() + reg * V.pow(2).mean()
        # back propagation
        loss.backward()
        optimizer.step()
        # record loss
        train_loss_list.append(torch.sqrt(((X_pred-X)*weights_train).pow(2).sum()/n_observed).item())
        test_loss_list.append(torch.sqrt(((X_pred-X)*weights_test).pow(2).sum()/n_unobserved).item())
    # return a detached cloned version of U and V
    return U.detach().clone(),V.detach().clone(),train_loss_list,test_loss_list
```

<span style="font-weight:bold;font-size:32px">3. SVD++</span>

The following is the `PyTorch` implementation of SVD++. It includes an extra implicit user factor term `FY`.
```python
def SVDpp(X, k=5, weights_train=weights_train, weights_test=weights_test,
          optimizer=torch.optim.SGD, l2=True, reg=0.0, lr=0.1, epochs=200):
    # un-normalized F
    F0 = torch.where(X>0,torch.ones_like(X),torch.zeros_like(X))
    # norms of F
    Fn =  torch.norm(F0, p=2, dim=1,keepdim=True).detach()
    # normalize
    F = F0/Fn
    n_rows,n_cols = X.shape
    # used to compute mean
    n_observed = weights_train.sum()
    n_unobserved = weights_test.sum()
    # record loss of train/test
    train_rmse_list = []
    test_rmse_list = []
    train_mae_list = []
    test_mae_list = []
    # mu set to be 
    mu = (X*weights_train).sum()/n_observed
    # random initialization
    Y = (torch.randn((n_cols,k),dtype=torch.float32)*0.1).clone().requires_grad_(True)
    U = (torch.randn((n_rows,k),dtype=torch.float32)*0.1).clone().requires_grad_(True)
    V = (torch.randn((k,n_cols),dtype=torch.float32)*0.1).clone().requires_grad_(True)
    bu = torch.randn((n_rows,1),dtype=torch.float32,requires_grad=True)
    bv = torch.randn((1,n_cols),dtype=torch.float32,requires_grad=True)
    # specified optimizer with learning rate = lr
    optimizer = optimizer([Y,U,V,bu,bv], lr=lr)
    # training
    for epoch in range(epochs):
        # clear all gradients
        optimizer.zero_grad()
        X_pred = mu+bu+bv+(U+F@Y)@V
        # compute loss function with regularization
        loss = ((X_pred-X)*weights_train).pow(2).sum()/n_observed + reg * Y.pow(2).mean() + reg * U.pow(2).mean() + reg * V.pow(2).mean() + reg * bu.pow(2).mean() + reg * bv.pow(2).mean()
        # back propagation
        loss.backward()
        optimizer.step()
        # record loss
        train_rmse_list.append(torch.sqrt(((X_pred-X)*weights_train).pow(2).sum()/n_observed).item())
        test_rmse_list.append(torch.sqrt(((X_pred-X)*weights_test).pow(2).sum()/n_unobserved).item())
        train_mae_list.append((((X_pred-X)*weights_train).abs().sum()/n_observed).item())
        test_mae_list.append((((X_pred-X)*weights_test).abs().sum()/n_unobserved).item())
    # return a detached cloned version of U and V
    return U.detach().clone(),V.detach().clone(),train_rmse_list,test_rmse_list,train_mae_list,test_mae_list
```

---
<span style="font-weight:bold;font-size:32px">4. Comparing with `Surprise`'s implementation</span>

<span style="font-weight:bold;font-size:28px">4.1 SVD with baselines</span>

The following are the model performances of SVD (k=2) on MovieLens 100k with 5-fold cross-validation using `Surprise` and the above implementation. Both model runs for 20 epochs.
```python
from surprise import SVD
from surprise import Dataset
from surprise import accuracy
from surprise.model_selection import cross_validate,KFold
# Load the movielens-100k dataset
data = Dataset.load_builtin('ml-100k')
svd = SVD(n_factors=2)
cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=KFold(n_splits=5,random_state=42), 
               return_train_measures=True,  verbose=True);
Evaluating RMSE, MAE of algorithm SVD on 5 split(s).
```
```
Evaluating RMSE, MAE of algorithm SVD on 5 split(s).

                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     
RMSE (testset)    0.9376  0.9378  0.9445  0.9344  0.9395  0.9388  0.0033  
MAE (testset)     0.7397  0.7403  0.7467  0.7383  0.7424  0.7415  0.0029  
RMSE (trainset)   0.9070  0.9113  0.9052  0.9017  0.9092  0.9069  0.0033  
MAE (trainset)    0.7170  0.7203  0.7149  0.7124  0.7188  0.7167  0.0028  
Fit time          1.72    1.76    1.61    1.74    1.73    1.71    0.05    
Test time         0.21    0.17    0.37    0.17    0.35    0.25    0.09    
```
To use the above SVD implementation, I used `pandas` to preprocess the data.
```python
import pandas as pd
# load data
dfRaw = pd.read_csv("data/mlen/u.data",sep='\t',header=None,names=["userId","movieId","rating","time"])
# transform into rating matrix using pivot method
df = dfRaw.pivot("userId","movieId","rating")
# transform from pandas.DataFrame to torch.Tensor
# one with nan
X_ml = torch.tensor(df.to_numpy(),dtype=torch.float32)
# one filling with 0 for nan
X_true_ml = torch.tensor(df.fillna(0).to_numpy(),dtype=torch.float32)
from time import time
# using randint to assign 0-4 to each position uniformly
# each number corresponding to one fold
mask_ml = torch.randint_like(X_ml,low=0,high=5,dtype=torch.float32)
time_list,rmse1_list,rmse2_list,mae1_list,mae2_list = [],[],[],[],[]
# 5 fold cross-validation
for i in range(5):    
    print("Fold {}:".format(i+1))
    # set train and test boolean mask using mask_ml
    weights_train = torch.where((mask_ml!=i) & (~torch.isnan(X_ml)),torch.ones_like(X_ml,dtype=torch.float32),torch.zeros_like(X_ml,dtype=torch.float32))
    weights_test = torch.where((mask_ml==i) & (~torch.isnan(X_ml)),torch.ones_like(X_ml,dtype=torch.float32),torch.zeros_like(X_ml,dtype=torch.float32))
    # record model runtime
    start = time()
    U_pred, V_pred, train_rmse_list, test_rmse_list, train_mae_list, test_mae_list  = incompleteSVD(X_true_ml, 2, weights_train, weights_test, optimizer=torch.optim.RMSprop, lr=0.3,l2=True, reg=0.008, epochs=20)
    end = time()
    time_list.append(round(end-start,2))
    rmse1_list.append(round(train_rmse_list[-1],4)); rmse2_list.append(round(test_rmse_list[-1],4))
    mae1_list.append(round(train_mae_list[-1],4)); mae2_list.append(round(test_mae_list[-1],4))
    print("\tFold Size:\t{}\t\tTime Used:\t{} seconds".format((mask_ml==1).sum().item(),time_list[-1]))
    print("\tRMSE(train):\t{}\t\tRMSE(test):\t{}".format(rmse1_list[-1],rmse2_list[-1]))
    print("\tMAE(train):\t{}\t\tMAE(test):\t{}".format(mae1_list[-1],mae2_list[-1]))
# print average performance
print("Mean:")
print("\t\t\t\t\tTime Used:\t{} seconds".format(round(np.mean(time_list),2)))
print("\tRMSE(train):\t{}\t\tRMSE(test):\t{}".format(round(np.mean(rmse1_list),4),round(np.mean(rmse2_list),4)))
print("\tMAE(train):\t{}\t\tMAE(test):\t{}".format(round(np.mean(mae1_list),4),round(np.mean(mae2_list),4)))
```
```
Fold 1:
	Fold Size:	316856		Time Used:	1.29 seconds
	RMSE(train):0.9087		RMSE(test):	0.9432
	MAE(train):	0.7159		MAE(test):	0.7401
Fold 2:
	Fold Size:	316856		Time Used:	1.24 seconds
	RMSE(train):0.9099		RMSE(test):	0.9398
	MAE(train):	0.7201		MAE(test):	0.7432
Fold 3:
	Fold Size:	316856		Time Used:	1.27 seconds
	RMSE(train):0.9182		RMSE(test):	0.9501
	MAE(train):	0.7168		MAE(test):	0.7438
Fold 4:
	Fold Size:	316856		Time Used:	1.49 seconds
	RMSE(train):0.909		RMSE(test):	0.9462
	MAE(train):	0.7137		MAE(test):	0.7434
Fold 5:
	Fold Size:	316856		Time Used:	1.62 seconds
	RMSE(train):0.9084		RMSE(test):	0.9422
	MAE(train):	0.7167		MAE(test):	0.7443
Mean:
                            Time Used:	1.38 seconds
	RMSE(train):0.9108		RMSE(test):	0.9443
	MAE(train):	0.7166		MAE(test):	0.743
```
We can see that the `PyTorch` implementation (1.38 s) of `SVD` is slightly faster than the `Surprise` implementation (1.71 s).

<span style="font-weight:bold;font-size:28px">4.2 SVD++</span>

Running the same code as above using `SVD++`, we get
```
Evaluating RMSE, MAE of algorithm SVDpp on 5 split(s).

                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     
RMSE (testset)    0.9202  0.9186  0.9295  0.9198  0.9156  0.9207  0.0047  
MAE (testset)     0.7240  0.7217  0.7329  0.7254  0.7195  0.7247  0.0046  
RMSE (trainset)   0.8685  0.8703  0.8726  0.8709  0.8616  0.8688  0.0038  
MAE (trainset)    0.6848  0.6856  0.6881  0.6864  0.6788  0.6847  0.0032  
Fit time          108.29  117.24  108.81  112.99  110.33  111.53  3.29    
Test time         4.76    4.96    4.34    5.29    4.70    4.81    0.31  

Fold 1:
	Fold Size:	317372		Time Used:	1.37 seconds
	RMSE(train):	0.947		RMSE(test):	0.9993
	MAE(train):	0.7524		MAE(test):	0.7918
Fold 2:
	Fold Size:	317372		Time Used:	1.51 seconds
	RMSE(train):	0.9638		RMSE(test):	1.0122
	MAE(train):	0.7576		MAE(test):	0.7939
Fold 3:
	Fold Size:	317372		Time Used:	1.5 seconds
	RMSE(train):	0.9838		RMSE(test):	1.026
	MAE(train):	0.7824		MAE(test):	0.8144
Fold 4:
	Fold Size:	317372		Time Used:	1.35 seconds
	RMSE(train):	1.0248		RMSE(test):	1.0761
	MAE(train):	0.8137		MAE(test):	0.8533
Fold 5:
	Fold Size:	317372		Time Used:	1.33 seconds
	RMSE(train):	1.0348		RMSE(test):	1.0727
	MAE(train):	0.8115		MAE(test):	0.8452
Mean:
                            Time Used:	1.41 seconds
	RMSE(train):	0.9908		RMSE(test):	1.0373
	MAE(train):	0.7835		MAE(test):	0.8197
```
The `PyTorch` implementation of `SVD`(1.41 s) is 80 times faster than the `Surprise` implementation (111.53 s). 

---
<span style="font-weight:bold;font-size:32px">5. TensorBoard</span>

In this section, I will use the `TensorBoard` UI to visualize the following 3 `PyTorch` models. All three models use the best hyperparameters from tuning with epochs=100.
1. ALS with RMSprop + reg=0.01
2. SVD with Adam + reg=0.05
3. SVD with RMSprop + reg=0.01
4. SVD++ with RMSprop + reg=0.01

The following code shows how to use `TensorBoard` with `PyTorch`.
```python
from torch.utils.tensorboard import SummaryWriter
# initiate a SummaryWriter with log_path
writer = SummaryWriter("log/mlens/"+log_dir)
# how to add scalar
writer.add_scalar("RMSE/train", torch.sqrt(((X_pred-X)*weights_train).pow(2).sum()/n_observed).item(), epoch)
# how to add histogram (uses torch.clamp here to crop the values)
writer.add_histogram("X_pred", torch.clamp(X_pred,0,5).clone().cpu().data.numpy(), epoch)
```

You will see two figures about
1. RMSE and MAE losses for train/test data
2. histograms showing the distributions of all prediction ratings of all users over all items

The following one shows losses. We can see that 
* `SVD` with Adam and `SVD++` with Adam achieve the best train/test RMSE and MAE;
* `SVD` and `SVD++` have shorter runtimes than `ALS`;
* `ALS` converges the fastest, and `SVD` converges faster than `SVD++`.
<center><img src="https://dingma129.github.io/assets/figures/rec/rec_4_tb1.png" width="900" ></center>
The following two show distributions of predictions. I have cropped all the values to [0,5] using `torch.clamp`. For `ALS`, there are quite a few values concentrated at 0, indicating a lot of predictions being negative numbers. For `SVD` and `SVD++`, the predictions become quite stable after around 20 epoches, and only a little number of predictions are negative numbers. 
<center><img src="https://dingma129.github.io/assets/figures/rec/rec_4_tb2.png" width="900" ></center>
<center><img src="https://dingma129.github.io/assets/figures/rec/rec_4_tb3.png" width="900" ></center>
---
<span style="font-weight:bold;font-size:32px">6. Summary</span>

* The `PyTorch` implementation of `SVD` and `SVD++` here is much faster than the one in `Surprise`.
* The same thing can also be done in `TensorFlow`, and I will not discuss it here in detail.
* We can use `PyTorch`+`TensorBoard` to visualize the model parameters and losses.
* As always, `Adam` is a better choice for optimizer.
