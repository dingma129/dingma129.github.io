---
title: LightGBM vs XGBoost
author: Ding Ma
layout: post
categories: [blog]
---

<span style="font-weight:bold;font-size:36px">0. introduction</span>

In this blog, I will introduce LightBGM, how to install it on Mac, compare its run time and performance with the state-of-the-art boosting algorithm XGBoost.

LightGBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithm, used for regression, classification, ranking and many other machine learning tasks.

For each decision in the ensemble, LightGBM splits the tree leaf-wise, while other boosting algorithms(for example, XGBoost) split the tree depth-wise. The leaf-wise algorithm can reduce more loss than the depth-wise algorithm and hence results in better accuracy. Also, it is significantly faster than XGBoost (which is already very fast), and that's the reason why it is named "Light"GBM.

---
<span style="font-weight:bold;font-size:36px">1. how to install</span>

In this section I will show you how to install LightGBM on mac os 10.14.5. Notice that the one on PyPI has a wrong `gcc` version, so we cannot use `pip install lightgbm` directly. By the way, XGBoost can be installed directly with `pip install xgboost`.

```python
brew install cmake
# may need "sudo chown -R $(whoami) /usr/local/*" before next line
brew install libomp  # need this after version 2.2.1, OpenMP is supported by Apple Clang
pip uninstall lightgbm  # remove lightgbm if used pip
git clone --recursive https://github.com/Microsoft/LightGBM
cd LightGBM
export CXX=g++-9 CC=gcc-9   # replace "9" with version of gcc installed on your machine
mkdir build ; cd build
cmake ..
make -j4
python -c "import lightgbm; print(lightgbm.__version__)"
> '2.2.3'
```

---
<span style="font-weight:bold;font-size:36px">2. model speed </span>

In this section, I will generate datasets with various sizes (10k,30k,100k,300k,1m) using 
```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
# n_samples = 10k,30k,100k,300k,1m
make_classification(n_samples=n_samples,n_features=20,n_informative=10,n_redundant=10,n_classes=2)
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)
```
Then I will compare the average run time with 10 runs for each case. I created the following helper function to get `run time`, `AUC`, `accuracy` of XGBoost model and LightGBM model. The `n_estimators` I used here is `10`. I set `max_depth=3` for XGBoost and `num_leaves=8` for LightGBM to make sure they have similar model complexity.
```python
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import roc_curve,roc_auc_score,auc,accuracy_score

def get_performance(X_train,X_test,y_train,y_test,n_estimators):
    # train xgb
    start_xgb = time.time()
    xgb = XGBClassifier(n_estimators=n_estimators,max_depth=3,learning_rate=0.1,n_jobs=-1)
    xgb.fit(X_train,y_train)
    end_xgb = time.time()
    # train lgb
    start_lgb = time.time()
    lgb = LGBMClassifier(n_estimators=n_estimators,num_leaves=8,learning_rate=0.1,n_jobs=-1)
    lgb.fit(X_train,y_train)
    end_lgb = time.time()
    # test xgb
    y_pred_prob_xgb = xgb.predict_proba(X_test)[:,1]
    y_pred_xgb = xgb.predict(X_test)
    auc_xgb = roc_auc_score(y_test,y_pred_prob_xgb)
    acc_xgb = accuracy_score(y_test,y_pred_xgb)
    # test lgb
    y_pred_prob_lgb = lgb.predict_proba(X_test)[:,1]
    y_pred_lgb = lgb.predict(X_test)
    auc_lgb = roc_auc_score(y_test,y_pred_prob_lgb)
    acc_lgb = accuracy_score(y_test,y_pred_lgb)
    return end_xgb-start_xgb,end_lgb-start_lgb,auc_xgb,auc_lgb,acc_xgb,acc_lgb
```

<span style="font-weight:bold;font-size:32px">2.1 AUC and accuracy </span>

From the following figure, we can see that XGBoost model and LightBGM model does not significantly differ from each other. Notice that the xlabel is in log10 scale.
<center><img src="https://dingma129.github.io/assets/figures/blog/lightbgm_auc_acc.png" width="1000"></center>

<span style="font-weight:bold;font-size:32px">2.2 Run time </span>

If we list the run time as in the following table, we can see that both model have exponential trend on run times. XGBoost is siginificantly slower than LightGBM.
<center><img src="https://dingma129.github.io/assets/figures/blog/lightbgm_run_time_table.png" width="300"></center>
If we plot the run time of these two models, we can see that the run times follow a linear function in the log-log graph.
<center><img src="https://dingma129.github.io/assets/figures/blog/lightbgm_run_time.png" width="500"></center>
The two linear models fitting those two groups of run times have R2_score
```python
0.9935582830038014   #xgb
0.9739240824890194   #lgb
```

---
<span style="font-weight:bold;font-size:36px">3. model performance</span>

In this section, I will use a dataset generated by
```python
make_classification(n_samples=30000,n_features=20,n_informative=10,n_redundant=10,n_classes=2)
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)
```
By using `callback` from those two libraries, we can record the stepwise AUC of train/test set.
```python
import xgboost
import lightgbm
# 2 dicts to record AUC
xgb_dict = {}
lgb_dict = {}
# train xgb
xgb = XGBClassifier(n_estimators=300,max_depth=5,learning_rate=0.1,n_jobs=-1)
xgb.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_test, y_test)],eval_metric="auc",verbose=False,callbacks = [xgboost.callback.record_evaluation(xgb_dict)]);
# train lgb
light = LGBMClassifier(n_estimators=300,num_leaves=31,learning_rate=0.1,n_jobs=-1)
light.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_test, y_test)],eval_metric="auc",verbose=False, callbacks = [lightgbm.callback.record_evaluation(lgb_dict)]);
```
From the following figure, we can see again that XGBoost model and LightBGM model does not significantly differ from each other in AUC performance. Comparing with XGBoost, LightBGM learns faster and slightly better, but it starts to overfit earlier.
<center><img src="https://dingma129.github.io/assets/figures/blog/lightbgm_auc_plot.png" width="900"></center>

---
<span style="font-weight:bold;font-size:36px">4. conclusions</span>

* The leaf-wise split approach used by LightBGM does achieve a slightly higher AUC than a depth-wise approach used by XGBoost.
* By using histogram based algorithm, LightBGM is significantly faster than XGBoost.
* In one sentence, LightBGM performs equally good if not better than as compare to XGBoost with a significant reduction in training time.