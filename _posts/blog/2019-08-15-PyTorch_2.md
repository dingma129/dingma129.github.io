---
title: PyTorch (part 2)
subtitle: gradient descent and mini-batch gradient descent
author: Ding Ma
layout: post
categories: [blog]
---
<span style="font-weight:bold;font-size:36px">0. Introduction</span>

In this blog, I will introduce how to perform gradient descent and mini-batch gradient descent on a multi-linear regression problem using `PyTorch`. 

---
<span style="font-weight:bold;font-size:36px">1. Data</span>

I will use the following randomly generated data. 
```python
import torch
n_examples = 1000
x = torch.rand((n_examples,2),dtype=torch.float64,requires_grad=False) * 10
# y = 3x1 - 2x2 + 1
w_true = torch.tensor([[3],[-2]],dtype=torch.float64,requires_grad=False)
b_true = torch.tensor([[1]],dtype=torch.float64,requires_grad=False)
error = torch.randn((n_examples,1),dtype=torch.float64,requires_grad=False)*0.2
y_true = torch.matmul(x,w_true) + b_true + error
# x,w_true,b_true,y_true all not trainable, so requires_grad=False
```
---
<span style="font-weight:bold;font-size:36px">2. Gradient Descent Manually</span>

By setting `requires_grad=True` for w and b, we can get `w.grad` and `b.grad` during each back propagation. 
```python
# w and b are trainable, set requires_grad=True in order to get their gradients
w = torch.tensor([[0],[0]],dtype=torch.float64,requires_grad=True)
b = torch.tensor([[0]],dtype=torch.float64,requires_grad=True)
# loss function
def loss_fn(y,y_true):
    return (y-y_true).pow(2).mean()
# learning rate
lr = 1e-2
# train 2000 epochs
for epoch in range(2000):
    y = torch.matmul(x,w) + b
    loss = loss_fn(y,y_true)
    # back propagation
    loss.backward()
    # update w and b using gradient descent
    with torch.no_grad():
        w -= lr * w.grad
        b -= lr * b.grad
        # reset w.grad and b.grad to 0 for next epoch
        w.grad.zero_()
        b.grad.zero_()
    if epoch%500==499:
        print("epoch={} \tw1={:.4f}\tw2={:.4f}\tb={:.4f}\tloss={:.4f}".format(epoch+1,w[0].item(),w[1].item(),b.item(),loss.item()))

# epoch=500 	w1=3.0222	w2=-1.9796	b=0.7714	loss=0.0454
# epoch=1000 	w1=3.0069	w2=-1.9938	b=0.9406	loss=0.0387
# epoch=1500 	w1=3.0030	w2=-1.9975	b=0.9843	loss=0.0383
# epoch=2000 	w1=3.0020	w2=-1.9984	b=0.9956	loss=0.0383
```
---
<span style="font-weight:bold;font-size:36px">3. Gradient Descent Using `torch.optim.SGD`</span>

Instead of updating w and b manually, the predefined optimizer can update parameters automatically by calling `optimizer.step()`.
```python
# w and b are trainable
w = torch.tensor([[0],[0]],dtype=torch.float64,requires_grad=True)
b = torch.tensor([[0]],dtype=torch.float64,requires_grad=True)
# saving w1,w2,b for later plotting
w1_list_gd = []
w2_list_gd = []
b_list_gd = []
loss_list_gd = []
# use predefined loss function
loss_fn = torch.nn.MSELoss()
# use predefined SGD optimizer
optimizer = torch.optim.SGD([w,b], lr=1e-2)
# train 2000 epochs
for epoch in range(2000):
    # clear the gradients of all optimized Tensors
    optimizer.zero_grad()
    y = torch.matmul(x,w) + b
    loss = loss_fn(y,y_true)
    # back propagation
    loss.backward()
    # perform a single optimization step
    optimizer.step()
    # record loss,w1,w2,b
    loss_list_gd.append(epoch_loss)
    w1_list_gd.append(w[0].item())
    w2_list_gd.append(w[1].item())
    b_list_gd.append(b.item())    
    if epoch%500==499:
        print("epoch={} \tw1={:.4f}\tw2={:.4f}\tb={:.4f}\tloss={:.4f}".format(epoch+1,w[0].item(),w[1].item(),b.item(),loss.item()))
        
# epoch=500 	w1=3.0222	w2=-1.9796	b=0.7714	loss=0.0454
# epoch=1000 	w1=3.0069	w2=-1.9938	b=0.9406	loss=0.0387
# epoch=1500 	w1=3.0030	w2=-1.9975	b=0.9843	loss=0.0383
# epoch=2000 	w1=3.0020	w2=-1.9984	b=0.9956	loss=0.0383
```
We can see that the result obtained using `torch.optim.SGD` is exactly the same as our manual version. Since we are feeding the model the whole dataset during each step, `torch.optim.SGD` is actually performing gradient descent.

---
<span style="font-weight:bold;font-size:36px">4. Mini-Batch Gradient Descent Using `torch.optim.SGD`</span>

In order to use mini-batch gradient descent, we first need to transform the dataset.
```python
from torch.utils.data import DataLoader, TensorDataset
# combine x,y_true together to create a DataLoader with batch_size=16
# data is shuffled for each epoch
xyloader = DataLoader(TensorDataset(x,y_true), batch_size=16, shuffle=True,drop_last=True)
```
Then we can use this DataLoader to train a linear regression model with mini-batch gradient descent.
```python
# w and b are trainable
w = torch.tensor([[0],[0]],dtype=torch.float64,requires_grad=True)
b = torch.tensor([[0]],dtype=torch.float64,requires_grad=True)
w1_list_sgd = []
w2_list_sgd = []
b_list_sgd = []
loss_list_sgd = []
# loss and optimizer
loss_fn = torch.nn.MSELoss()
optimizer = torch.optim.SGD([w,b], lr=1e-2)
# train 30 epochs
for epoch in range(30):
    # reset epoch_loss to 0 for every epoch
    epoch_loss = 0.0
    # load data from DataLoader
    # each data has batch_size = 16
    for i, data in enumerate(xyloader):
        # train one batch
        x_batch, y_true_batch = data
        optimizer.zero_grad()
        y_batch = torch.matmul(x_batch,w) + b
        loss = loss_fn(y_batch,y_true_batch)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
        w1_list_sgd.append(w[0].item())
        w2_list_sgd.append(w[1].item())
        b_list_sgd.append(b.item())
    loss_list_sgd.append(epoch_loss)
    if epoch%5==4:
        print("epoch={} \tw1={:.4f}\tw2={:.4f}\tb={:.4f}\tloss={:.4f}".format(epoch+1,w[0].item(),w[1].item(),b.item(),epoch_loss))

# epoch=5 	w1=3.0262	w2=-1.9677	b=0.6409	loss=4.0050
# epoch=10 	w1=3.0220	w2=-1.9808	b=0.8466	loss=2.9378
# epoch=15 	w1=3.0076	w2=-2.0001	b=0.9338	loss=2.5819
# epoch=20 	w1=3.0102	w2=-1.9952	b=0.9734	loss=2.6352
# epoch=25 	w1=3.0103	w2=-1.9912	b=0.9889	loss=2.6834
# epoch=30 	w1=3.0082	w2=-1.9946	b=0.9965	loss=2.5569
```
---
<span style="font-weight:bold;font-size:36px">5. Visualization</span>

The following figure are plottings of w and b for gradient descent and mini-batch gradient descent. It can be clearly seen that the change of w and b in gradient descent is much smoother but slower than mini-batch gradient descent.

<center><img src="https://dingma129.github.io/assets/figures/pytorch/pytorch_2_wb.png" width="900" ></center>