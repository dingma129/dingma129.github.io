---
title: "Julia: Notes-06"
layout: splash
excerpt: "MNIST: CNN"
categories: [Julia]
tags: [Study Note, Classification, MNIST, Flux, CNN]
---

# 0. load package


```julia
using Flux, Images,Plots, Statistics, MLBase
gr()
```




    Plots.GRBackend()



# 1. load data
We load data exactly the same way as in the last blog.


```julia
X = Flux.Data.MNIST.images();
y = Flux.Data.MNIST.labels();
```


```julia
size(X[1])
```




    (28, 28)



# 2. preprocessing data
Here I divides data into minibatchs with batch_size = 100. This can make training faster, but the objective function does change smoothly comparing to the non-minibatched version.

Also by using `Float32` instead of `Float64`, we can save some time and spaces.


```julia
# using Float32 makes computation faster
function create_minibatch(X,y,r)
    # create a 4D empty array of shape (28,28,1,batch_size)
    X_batch = Array{Float32}(undef,28,28,1,length(r))
    for i in 1:length(r)
        X_batch[:,:,1,i] = Float32.(X[r[i]])
    end
    y_batch = Flux.onehotbatch(y[r],0:9) 
    return (X_batch, y_batch)
end
    
batch_size = 100
# create indices for each mini_batch
mb_idxs = Iterators.partition(1:5000, batch_size)
# size = 5000, batch_size = 100
train_set = [create_minibatch(X[1:5000], y[1:5000], i) for i in mb_idxs];
# size = 5000, only one batch
test_set = create_minibatch(X[5001:10000], y[5001:10000], 1:5000);
```


```julia
println(size(train_set[1][1]))  # size of X_batch
println(size(train_set[1][2]))  # size of y_batch
println(size(test_set[1]))      # test set contains only one batch
println(size(test_set[2]))
```

    (28, 28, 1, 100)
    (10, 100)
    (28, 28, 1, 5000)
    (10, 5000)


# 3. CNN with 3 convolutional layers

The structure of our CNN is shown as below..
![png](/assets/figures/julia_notes/06_structure.png)


```julia
model = Chain(
    # (28,28,1,-1) => (28,28,8,-1) => (14,14,8,-1)
    Conv((3, 3), 1=>8, pad=(1,1), relu), MaxPool((2,2)),

    # (14,14,8,-1) => (14,14,16,-1) => (7,7,16,-1)
    Conv((3, 3), 8=>16, pad=(1,1), relu), MaxPool((2,2)),

    # (7,7,16,-1) => (7,7,8,-1) => (3,3,8,-1)
    Conv((3, 3), 16=>8, pad=(1,1), relu), MaxPool((2,2)),

    # reshape (3,3,8,-1) => (3*3*8=72,-1)
    x -> reshape(x, :, size(x, 4)),
    
    # (72,-1) => (10,-1)
    Dense(72, 10),

    softmax
)
```




    Chain(Conv((3, 3), 1=>8, relu), MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2)), Conv((3, 3), 8=>16, relu), MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2)), Conv((3, 3), 16=>8, relu), MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2)), #23, Dense(72, 10), softmax)




```julia
# feed model with 1st batch to see whether it compiles well
model(train_set[1][1])
```




    Tracked 10×100 Array{Float32,2}:
     0.0137187    0.00254569   0.00174597   …  0.00487373   0.00827201
     0.550319     0.838346     0.147642        0.130377     0.397707  
     ...
     0.0353528    0.058523     0.0206755       0.0735759    0.0744027 
     0.00563831   0.000298173  0.0074881       0.00289689   0.0150109 



# 4. train and evaluate model
Here I only trained the model for 5 epochs. We can see that the accuracy is improving rapidly.
* start:   0.1446
* epoch 1: 0.7906
* epoch 2: 0.8930
* epoch 3: 0.9190
* epoch 4: 0.9342
* epoch 5: 0.9380


```julia
# loss function
loss(x, y) = sum(Flux.crossentropy(model(x), y))
# optimizer
opt = ADAM(0.005) 
# accuracy: using onecold to transfer from 10-dim onehot vector => 1-dim vector with values in 1:10
accuracy(x, y) = mean(Flux.onecold(model(x), 1:10) .== Flux.onecold(y, 1:10))

train_loss2 = Float32[]
test_loss2 = Float32[]

# train 5 epochs
for e in 1:5
    println("epoch $(e):")
    Flux.train!(loss, params(model), train_set, opt,cb=() -> @show accuracy(test_set...))
end
```

    epoch 1:
    accuracy(test_set...) = 0.1446
    ...
    accuracy(test_set...) = 0.7906
    epoch 2:
    accuracy(test_set...) = 0.7934
    ...
    accuracy(test_set...) = 0.893
    epoch 3:
    accuracy(test_set...) = 0.8968
    ...
    accuracy(test_set...) = 0.919
    epoch 4:
    accuracy(test_set...) = 0.9216
    ...
    accuracy(test_set...) = 0.9342
    epoch 5:
    accuracy(test_set...) = 0.932
    ...
    accuracy(test_set...) = 0.938


# 5. visualize confusion matrix
We can use confusion matrix to see how does the misclassification distribute.

We can either use the `MLBase` package to create a confusion matrix, or create it manually.


```julia
pred_test_labels = Flux.onecold(model(test_set[1]), 1:10)
true_test_labels = Flux.onecold(test_set[2], 1:10)

cm = MLBase.confusmat(10,true_test_labels,pred_test_labels)
```




    10×10 Array{Int64,2}:
     499    0    3    1    0   10    2    0    6    1
       0  542    5    3    3    3    0    2    6    0
       3    2  474    8    1    0    1    4   10    0
       2    2    4  503    1   13    0   12    1    1
       0    2    0    0  430    0    4    6    2    1
       1    0    0    2    1  419    1    1    4    0
       4    0    1    0    2    4  494    0    8    0
       0    0    7    2    5    2    0  497    3    4
       5    7    4    2    2    5    9    6  441    1
       7    1    0    8   16    9    0   39   12  391



Using `heapmap`, we can easily visualize the confusion matrix.


```julia
heatmap(cm, c=:inferno, title="Confusion Matrix", 
    ylabel="True label", 
    xlabel= "Predicted label", 
    xticks=(1:10, 0:9), 
    yticks=(1:10, 0:9))
```




![svg](/assets/figures/julia_notes/06_output_16_0.svg)



We can see that it's quite simple to use Flux just as `TensorFlow` or `PyTorch`. 
