---
title: "Recommender Systems - 7"
layout: splash
excerpt: "Matrix Factorization in Scala"
categories: [Scala]
tags: [Matrix Factorization, SVD++, ALS, SVD, Truncated SVD, Spark]
---

# 0. Introduction

In this blog, I will introduce the `Breeze` library in `Scala` and then explain the `GraphX` implementation of `SVD++`. I will provide several examples to show you how to use `SVDPlusPlus`.


# 1. SVD using `Breeze`

[`Breeze`](https://github.com/scalanlp/breeze) is a `Scala` library for numerical processing. `Breeze` has as its core concepts matrices and column vectors. Its matrices default to column major ordering, like `Matlab`, but indexing is 0-based, like `Numpy`.

Let's look at how to use `Breeze` to do `SVD`. Let's define a 3 by 3 matrix first.

```scala
import breeze.linalg._
import breeze.numerics._
// define a DenseMatrix
val dmat = DenseMatrix((1.0,2.0,3.0),(4.0,5.0,6.0),(7.0,8.0,9.0))
```

Performing `SVD` or `Truncated SVD` is easy using `Breeze`.
```scala
// SVD decomposition: u,v are DenseMatrix and s is DenseVector
val svd.SVD(u,s,v) = svd(dmat)
// full rank 
(u * diag(s) * v).map{_.toFloat}
// 1.0  2.0  3.0
// 4.0  5.0  6.0
// 7.0  8.0  9.0
// truncated SVD of rank 2
(u(::,0 to 1) * diag (s(0 to 1)) * v(0 to 1,::)).map{_.toFloat}
// 1.0  2.0  3.0
// 4.0  5.0  6.0
// 7.0  8.0  9.0
```

# 2. ALS using `Spark.ML`

For completion, I will also include the `ALS` method in `Spark.ML` here. Instead of working on the rating matrix directly, `ALS` in `Spark.ML` works on DataFrames of triples of `(userId,itemId,rating)`.

Let's first create the same data as above but in a form of DataFrame of triples.

```scala
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
// RDD
val data = sc.parallelize(Seq((1,1,1.0),(1,2,2.0),(1,3,3.0),
                              (2,1,4.0),(2,2,5.0),(2,3,6.0),
                              (3,1,7.0),(3,2,8.0),(3,3,9.0)))
// DataFrame and rename columns
val df = data.toDF()
.withColumnRenamed("_1","user")
.withColumnRenamed("_2","item")
.withColumnRenamed("_3","rating")
```

Using `ALS` is also straightforward.
```scala
import org.apache.spark.ml.recommendation.ALS
// instantiate
val als = new ALS().setRank(2).setMaxIter(20).setRegParam(0.0)
.setUserCol("user").setItemCol("item").setRatingCol("rating")
// fit the data
val alsModel = als.fit(df)
// get U and V: as DataFrames
val u_df = alsModel.userFactors.sort("id")
val v_df = alsModel.itemFactors.sort("id")
// convert U and V to DenseMatrix
val u_als = DenseMatrix(u_df.map(_.getSeq[Float](1).toArray).collect():_*)
val v_als = DenseMatrix(v_df.map(_.getSeq[Float](1).toArray).collect():_*)
// check U*V
(u_als * v_als.t).map{_.toFloat}
// 0.9999995  2.0  2.9999995
// 4.0        5.0  6.0
// 7.0        8.0  9.0
```

# 3. SVD++ using `Spark.GraphX`

Here comes the most interesting part of this blog. In this section, I will introduce how to use `Spark.GraphX` to perform `SVD++`. This implementation uses parallelism, so it's a good choice when having a very large dataset. Unfortunately, the official [document](https://spark.apache.org/docs/2.4.4/graphx-programming-guide.html) does not include any example about how to use it. I will demystify this `SVDPlusPlus` method here after spending some time on reading the source code.

Let's first import some necessary libraries.
```scala
import org.apache.spark.graphx._
import org.apache.spark.graphx.lib.SVDPlusPlus
import org.apache.spark.rdd.RDD
```
`GraphX` exposes RDD views of the vertices and edges stored within the graph. However, because `GraphX` maintains the vertices and edges in optimized data structures and these data structures provide additional functionality, the vertices and edges are returned as `VertexRDD` and `EdgeRDD` respectively.

I will use the same example as above.
```scala
// +100 to distinguish user and item
// user: 1, 2, 3
// item: 101, 102, 103
// store datas as an EdgeRDD
val edgeRDD = df.rdd.map(row=>Edge(row.getInt(0).toLong,(row.getInt(1)+100).toLong,row.getDouble(2)))
// check the first element
// it's an edge from user 1 to item 101 with rating 1.0
edgeRDD.first
// Edge(1,101,1.0)
```

In order to use `SVDPlusPlus`, we don't need to specify the `VertexRDD`. This saves a lot of time. Let's first recell the `SVD++` model dicussed in the [previous blog](/python/Rec_Sys_2/).

<center><img src="/assets/figures/rec/rec_2_svdpp.png" width="1000" ></center>
The source code and documentation does not provide any explaination about the configuration parameters of `SVDPlusPlus`. I will explain it here for you.

```scala
// configuration parameters for SVDPlusPlus
val conf = new SVDPlusPlus.Conf(
    rank=2,       // latent dimension
    maxIters=20,  // max number of iterations
    minVal=1.0,   // min value of ratings
    maxVal=9.0,   // max value of ratings
    gamma1=0.2,   // learning rate of user and item bias
    gamma2=0.2,   // learning rate of explicit/implicit user factor and item factor
    gamma6=0.0,   // learning rate of regularization of user/item bias
    gamma7=0.0)   // learning rate of regularization of user/item factor
// run SVDPlusPlus model
val svdpp = SVDPlusPlus.run(edgeRDD,conf)
```

Now what does the output `svdpp` look like? It's a 2-tuple of `GraphImpl` object (subtype of `Graph`) and the global bias `mu`(DoubleType). 

This first component contains an `EdgeRDDImpl` (subtype of `EdgeRDD`) and a `VertexRDDImpl` (subtype of `VertexRDD`).
1. `EdgeRDDImpl` is nothing but the input training data;
2. `VertexRDDImpl` is the parameters obtained after training.

Each element in the `VertexRDDImpl` is a `(id,4-tuple)` tuple with the 4-tuple containing either user properties or item properties. Using our notations of `SVD++` in the above figure, the element in the 4-tuple corresponds to
* user u: `(U_u,FY_u,O_u,temp)` of type `(Array,Array,Double,Double)`
* item i: `(V_i,Y_i,P_i,temp)` of type `(Array,Array,Double,Double)`

Now let's look at two examples, one for user and one for item.
```scala
// first element in the VertexRDD
svdpp._1.vertices.first
// (102,(Array(-0.4423084461725741, -0.2914873601127262),Array(0.03511458150266821, 0.03893385382706433),0.07571031306347982,0.0013879658032274055))
// it represents the properties of item 2 (id 2+100=102)
// V_2: Array(-0.4423084461725741, -0.2914873601127262)  
// Y_2: Array(0.03511458150266821, 0.03893385382706433)
// P_2: 0.07571031306347982
// temporary term: 0.0013879658032274055

// second element in the VertexRDD
svdpp._1.vertices.take(2).last
// (2,(Array(0.055843773511751724, 0.030356536446545415),Array(0.2283616733392448, 0.1698765956577604),0.05977996412084609,0.5773502691896258))
// it represents the properties of user 2 (id 2)
// U_2: Array(0.055843773511751724, 0.030356536446545415)
// FY_2: Array(0.2283616733392448, 0.1698765956577604)
// O_2: 0.05977996412084609
// temporary term: 0.5773502691896258
```

After having latent factors of user 2 and item 2, we can get the model prediction of user 2's rating on item 2, which is `mu+P_2+O_2+(U_2+FY_2)@V2`.

I have implement the following helper functions to get the prediction of user u on item i. Here I used `breeze.linalg.DenseVector` to compute the dot product.
```scala
// get properties of a given id (either user or item)
def getAttr(id:Long, 
            vRDD1: VertexRDD[(Array[Double], Array[Double], Double, Double)]) = {
    vRDD1.filter(_._1 == id).first._2
}
// get prediction rating
def getPredictionRaw(u:(Array[Double],Array[Double],Double,Double), 
                     i:(Array[Double],Array[Double],Double,Double),
                     vRDD2:Double):Double = {
    vRDD2 + u._3 + i._3 + (DenseVector(i._1) dot (DenseVector(u._1) + DenseVector(u._2)))
}
// combination of above two
def getPrediction(uid:Long, iid:Long, 
                  vRDD: (Graph[(Array[Double], Array[Double], Double, Double),Double],Double)):Double = {
    getPredictionRaw(getAttr(uid,vRDD._1.vertices),getAttr(iid,vRDD._1.vertices),vRDD._2)
}
```
Using this helper function, we can easily get the prediction of ratings as follows.
```scala
// get all predictions
for (uid <- 1 to 3 ; iid <- 101 to 103) {
    println(uid,iid,
            getPrediction(uid,iid,svdpp).toFloat)
}
// (1,101,1.043977)
// (1,102,2.2353306)
// (1,103,3.0610092)
// (2,101,3.9452682)
// (2,102,4.9514184)
// (2,103,6.02663)
// (3,101,6.910042)
// (3,102,7.8011546)
// (3,103,9.030757)
// or in matrix form
DenseMatrix.tabulate(3, 3){case (i, j) => getPrediction(i+1,j+101,svdpp).toFloat}
// 1.043977   2.2353306  3.0610092
// 3.9452682  4.9514184  6.02663
// 6.910042   7.8011546  9.030757
```


# 4. Example of incomplete matrix

This time I will use `SVDPlusPlus` to predict ratings for an incomplete rating matrix from previous [blog](/python/Rec_Sys_4/).

```scala
// incomplete rating matrix
val partial_data = sc.parallelize(Seq(
    (1,1,2.0),(1,2,4.0),(1,3,3.0),(1,4,5.0),(1,5,5.0),(1,6,1.0),
    (2,1,2.0),(2,2,4.0),(2,4,3.0),(2,5,4.0),(2,6,5.0),
    (3,2,3.0),(3,3,4.0),(3,4,1.0),(3,5,2.0),
    (4,1,1.0),(4,2,2.0),(4,3,3.0),(4,4,1.0),(4,5,2.0),(4,6,2.0),
    (5,1,3.0),(5,3,1.0),(5,4,4.0),(5,5,2.0),(5,6,4.0)))
// + 100 to distinguish user and item
// store as an EdgeRDD
val partial_edgeRDD = partial_data.map(row=>Edge(row._1.toLong,(row._2+100).toLong,row._3.toDouble))
// train model with minVal=1,maxVal=5
val partial_conf = new SVDPlusPlus.Conf(rank=2,maxIters=20,minVal=1.0,maxVal=5.0,gamma1=0.05,gamma2=0.05,gamma6=0.01,gamma7=0.01)
val partial_svdpp = SVDPlusPlus.run(partial_edgeRDD,partial_conf)
// get as a matrix
DenseMatrix.tabulate(5, 6){case (i, j) => getPrediction(i+1,j+101,partial_svdpp).toFloat}
// 2.2250257  3.9954145  3.7121954  3.7725086  4.1545587  3.2395053
// 2.5642664  4.1708846  3.7763753  3.9050388  4.162593   3.6482022
// 1.3601636  2.8254614  2.5404568  2.3717334  2.6394012  2.3248584
// 0.7871508  2.3565392  2.0311818  2.0116217  2.294889   1.8045392
// 1.9892883  3.3816135  2.9110572  3.0085254  3.142966   3.103456
```


